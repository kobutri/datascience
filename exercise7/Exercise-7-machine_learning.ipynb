{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Submission Date: 19/12/2022\n",
    "\n",
    "## Points: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> keywords:<font color='black'> \n",
    "    \n",
    "**Tasks:** regression, classification, clustering  \n",
    "**Algorithms:** linear regression, logistic regression, random forest, SVM, k-means  \n",
    "**Evaluation Measures:** RMSE, precision, recall, F1-score  \n",
    "**Concepts:** training and testing\n",
    "**R packages used:** tidyverse, rpart, randomForest, e1071, caret, mice, mltest  \n",
    "  \n",
    "  \n",
    "    \n",
    "The goal of this lab session is to get familar with various machine learning based tasks in R. Many packages in R have similar interface that uses a formula and other parameters.\n",
    "\n",
    "**formula:** is a way to express the form of a model. For example, suppose you have a response variable y and independent variables x1, x2 and x3. To express that y depends linearly on x1, x2 and x3 you would use the formula `y ~ x1 + x2 + x3.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) is the machine learning task of inferring a function from labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) is the processes to estimate the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "We will use the `lm()` function in the `stats` package which is part of base R. No external package needed.\n",
    "\n",
    "    lm_model <- lm(y ∼ x1 + x2, data=mydata)\n",
    "    summary(lm_model)\n",
    "\n",
    "The vector of coefficients for the model is contained in `lm_model$coefficients.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: We will start with building a simple model using the `cars` dataset that comes with R. The dataset contains the speed of cars and the distances taken to stop. In this example, we will build a linear regression model with only a **single feature**, i.e. to compute `dist` from `speed.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>speed</th><th scope=col>dist</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>4</td><td> 2</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4</td><td>10</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>7</td><td> 4</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>7</td><td>22</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>8</td><td>16</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>9</td><td>10</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & speed & dist\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 4 &  2\\\\\n",
       "\t2 & 4 & 10\\\\\n",
       "\t3 & 7 &  4\\\\\n",
       "\t4 & 7 & 22\\\\\n",
       "\t5 & 8 & 16\\\\\n",
       "\t6 & 9 & 10\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | speed &lt;dbl&gt; | dist &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 4 |  2 |\n",
       "| 2 | 4 | 10 |\n",
       "| 3 | 7 |  4 |\n",
       "| 4 | 7 | 22 |\n",
       "| 5 | 8 | 16 |\n",
       "| 6 | 9 | 10 |\n",
       "\n"
      ],
      "text/plain": [
       "  speed dist\n",
       "1 4      2  \n",
       "2 4     10  \n",
       "3 7      4  \n",
       "4 7     22  \n",
       "5 8     16  \n",
       "6 9     10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view some first rows of the dataset\n",
    "head(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.2 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.4.0      \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.5 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.8      \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.10\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.1      \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.5.0 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.3      \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.2 \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[1m\u001b[22m`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAAQlBMVEUAAAAzMzMzZv89PT1N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\nLIZhwgzBYhgmzPw/ygCpJs2YixAAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now, lets visually examine the data\n",
    "\n",
    "# load the required packages for plotting\n",
    "library(tidyverse)\n",
    "# draw the scatter plot between 'speed' and 'dist'\n",
    "ggplot(data = cars, mapping = aes(x = speed, y = dist)) +\n",
    "    geom_point() +\n",
    "    geom_smooth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above plot suggests that 'dist' can be computed from 'speed' through a linear function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear regression model\n",
    "cars_lm <- lm(dist ~ speed, data = cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the fitted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = dist ~ speed, data = cars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-29.069  -9.525  -2.272   9.215  43.201 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \n",
       "speed         3.9324     0.4155   9.464 1.49e-12 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 15.38 on 48 degrees of freedom\n",
       "Multiple R-squared:  0.6511,\tAdjusted R-squared:  0.6438 \n",
       "F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## summary of fitted model\n",
    "summary(cars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the fitted model for prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>1</dt><dd>-9.71427737226271</dd><dt>2</dt><dd>-5.78186861313862</dd><dt>3</dt><dd>-1.84945985401454</dd><dt>4</dt><dd>9.94776642335772</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] -9.71427737226271\n",
       "\\item[2] -5.78186861313862\n",
       "\\item[3] -1.84945985401454\n",
       "\\item[4] 9.94776642335772\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   -9.714277372262712\n",
       ":   -5.781868613138623\n",
       ":   -1.849459854014544\n",
       ":   9.94776642335772\n",
       "\n"
      ],
      "text/plain": [
       "        1         2         3         4 \n",
       "-9.714277 -5.781869 -1.849460  9.947766 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example unseen data\n",
    "df <- data.frame('speed' = c(2,3,4,7))\n",
    "# prediction\n",
    "predict(cars_lm, newdata = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: We will use another dataset that comes with R, `mtcars`, to build a model with **multiple features** to predict the fuel consumption `mpg.` The features describe different aspects of an automobile design and performance. We will also explore **which features to use**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 11</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>mpg</th><th scope=col>cyl</th><th scope=col>disp</th><th scope=col>hp</th><th scope=col>drat</th><th scope=col>wt</th><th scope=col>qsec</th><th scope=col>vs</th><th scope=col>am</th><th scope=col>gear</th><th scope=col>carb</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Mazda RX4</th><td>21.0</td><td>6</td><td>160</td><td>110</td><td>3.90</td><td>2.620</td><td>16.46</td><td>0</td><td>1</td><td>4</td><td>4</td></tr>\n",
       "\t<tr><th scope=row>Mazda RX4 Wag</th><td>21.0</td><td>6</td><td>160</td><td>110</td><td>3.90</td><td>2.875</td><td>17.02</td><td>0</td><td>1</td><td>4</td><td>4</td></tr>\n",
       "\t<tr><th scope=row>Datsun 710</th><td>22.8</td><td>4</td><td>108</td><td> 93</td><td>3.85</td><td>2.320</td><td>18.61</td><td>1</td><td>1</td><td>4</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>Hornet 4 Drive</th><td>21.4</td><td>6</td><td>258</td><td>110</td><td>3.08</td><td>3.215</td><td>19.44</td><td>1</td><td>0</td><td>3</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>Hornet Sportabout</th><td>18.7</td><td>8</td><td>360</td><td>175</td><td>3.15</td><td>3.440</td><td>17.02</td><td>0</td><td>0</td><td>3</td><td>2</td></tr>\n",
       "\t<tr><th scope=row>Valiant</th><td>18.1</td><td>6</td><td>225</td><td>105</td><td>2.76</td><td>3.460</td><td>20.22</td><td>1</td><td>0</td><td>3</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 11\n",
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & mpg & cyl & disp & hp & drat & wt & qsec & vs & am & gear & carb\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tMazda RX4 & 21.0 & 6 & 160 & 110 & 3.90 & 2.620 & 16.46 & 0 & 1 & 4 & 4\\\\\n",
       "\tMazda RX4 Wag & 21.0 & 6 & 160 & 110 & 3.90 & 2.875 & 17.02 & 0 & 1 & 4 & 4\\\\\n",
       "\tDatsun 710 & 22.8 & 4 & 108 &  93 & 3.85 & 2.320 & 18.61 & 1 & 1 & 4 & 1\\\\\n",
       "\tHornet 4 Drive & 21.4 & 6 & 258 & 110 & 3.08 & 3.215 & 19.44 & 1 & 0 & 3 & 1\\\\\n",
       "\tHornet Sportabout & 18.7 & 8 & 360 & 175 & 3.15 & 3.440 & 17.02 & 0 & 0 & 3 & 2\\\\\n",
       "\tValiant & 18.1 & 6 & 225 & 105 & 2.76 & 3.460 & 20.22 & 1 & 0 & 3 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 11\n",
       "\n",
       "| <!--/--> | mpg &lt;dbl&gt; | cyl &lt;dbl&gt; | disp &lt;dbl&gt; | hp &lt;dbl&gt; | drat &lt;dbl&gt; | wt &lt;dbl&gt; | qsec &lt;dbl&gt; | vs &lt;dbl&gt; | am &lt;dbl&gt; | gear &lt;dbl&gt; | carb &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Mazda RX4 | 21.0 | 6 | 160 | 110 | 3.90 | 2.620 | 16.46 | 0 | 1 | 4 | 4 |\n",
       "| Mazda RX4 Wag | 21.0 | 6 | 160 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 |\n",
       "| Datsun 710 | 22.8 | 4 | 108 |  93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 |\n",
       "| Hornet 4 Drive | 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 |\n",
       "| Hornet Sportabout | 18.7 | 8 | 360 | 175 | 3.15 | 3.440 | 17.02 | 0 | 0 | 3 | 2 |\n",
       "| Valiant | 18.1 | 6 | 225 | 105 | 2.76 | 3.460 | 20.22 | 1 | 0 | 3 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "                  mpg  cyl disp hp  drat wt    qsec  vs am gear carb\n",
       "Mazda RX4         21.0 6   160  110 3.90 2.620 16.46 0  1  4    4   \n",
       "Mazda RX4 Wag     21.0 6   160  110 3.90 2.875 17.02 0  1  4    4   \n",
       "Datsun 710        22.8 4   108   93 3.85 2.320 18.61 1  1  4    1   \n",
       "Hornet 4 Drive    21.4 6   258  110 3.08 3.215 19.44 1  0  3    1   \n",
       "Hornet Sportabout 18.7 8   360  175 3.15 3.440 17.02 0  0  3    2   \n",
       "Valiant           18.1 6   225  105 2.76 3.460 20.22 1  0  3    1   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + qsec + am + carb, data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.1184 -1.5414 -0.1392  1.2917  4.3604 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  12.8972     7.4725   1.726 0.095784 .  \n",
       "wt           -3.4343     0.8200  -4.188 0.000269 ***\n",
       "qsec          1.0191     0.3378   3.017 0.005507 ** \n",
       "am            3.5114     1.4875   2.361 0.025721 *  \n",
       "carb         -0.4886     0.4212  -1.160 0.256212    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.444 on 27 degrees of freedom\n",
       "Multiple R-squared:  0.8568,\tAdjusted R-squared:  0.8356 \n",
       "F-statistic: 40.39 on 4 and 27 DF,  p-value: 5.064e-11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the dataset mtcars to create a linear regression model to predict `mpg` using `wt, qsec, am` and `carb`. \n",
    "mtcars_lm <- lm(mpg ~ wt + qsec + am + carb, data = mtcars)\n",
    "summary(mtcars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "[Classification](https://en.wikipedia.org/wiki/Statistical_classification) is the problem of determining label(s) (e.g., categories or clasesses, etc.) of each data observation based on learning from labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widely used (shallow) classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Logistic Regression**](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "Provided in `stats` package, which is automatically loaded when starting R  \n",
    "\n",
    "    glm_model <- glm(y ∼ x1 + x2, family = binomial, data = mydata)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**K-Nearest Neighbor**](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "\n",
    "Install and load the `class` package\n",
    "\n",
    "    knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\n",
    "    \n",
    "`knn_model` is a factor vector of class attributes for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Decision Trees (CART)**](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "\n",
    "Install and load the `rpart` package.\n",
    "\n",
    "    cart_model <- rpart(y ∼ x1 + x2, data=mydata, method=\"class\")\n",
    " \n",
    "You can use `plot.rpart` and `text.rpart` to plot the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Random Forest**](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "Install and load the `randomForest` package\n",
    "\n",
    "\n",
    "    rf_model <- randomForest(y ~ x1 + x2, data=train, importance=TRUE, ntree=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Support Vector Machines (SVM)**](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "Install and load the `e1071` package.\n",
    "\n",
    "    svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we will use the above models to predict `survivors` in the [Titanic dataset](https://www.kaggle.com/c/titanic). The dataset also provided in `titanic.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m1309\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m12\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (5): Name, Sex, Ticket, Cabin, Embarked\n",
      "\u001b[32mdbl\u001b[39m (7): PassengerId, Pclass, Age, SibSp, Parch, Fare, Survived\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 12</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>PassengerId</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th><th scope=col>Survived</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>3</td><td>Braund, Mr. Owen Harris                            </td><td>male  </td><td>22</td><td>1</td><td>0</td><td>A/5 21171       </td><td> 7.2500</td><td>NA  </td><td>S</td><td>0</td></tr>\n",
       "\t<tr><td>2</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599        </td><td>71.2833</td><td>C85 </td><td>C</td><td>1</td></tr>\n",
       "\t<tr><td>3</td><td>3</td><td>Heikkinen, Miss. Laina                             </td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td> 7.9250</td><td>NA  </td><td>S</td><td>1</td></tr>\n",
       "\t<tr><td>4</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)       </td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803          </td><td>53.1000</td><td>C123</td><td>S</td><td>1</td></tr>\n",
       "\t<tr><td>5</td><td>3</td><td>Allen, Mr. William Henry                           </td><td>male  </td><td>35</td><td>0</td><td>0</td><td>373450          </td><td> 8.0500</td><td>NA  </td><td>S</td><td>0</td></tr>\n",
       "\t<tr><td>6</td><td>3</td><td>Moran, Mr. James                                   </td><td>male  </td><td>NA</td><td>0</td><td>0</td><td>330877          </td><td> 8.4583</td><td>NA  </td><td>Q</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 12\n",
       "\\begin{tabular}{llllllllllll}\n",
       " PassengerId & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked & Survived\\\\\n",
       " <dbl> & <dbl> & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <chr> & <dbl> & <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 3 & Braund, Mr. Owen Harris                             & male   & 22 & 1 & 0 & A/5 21171        &  7.2500 & NA   & S & 0\\\\\n",
       "\t 2 & 1 & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female & 38 & 1 & 0 & PC 17599         & 71.2833 & C85  & C & 1\\\\\n",
       "\t 3 & 3 & Heikkinen, Miss. Laina                              & female & 26 & 0 & 0 & STON/O2. 3101282 &  7.9250 & NA   & S & 1\\\\\n",
       "\t 4 & 1 & Futrelle, Mrs. Jacques Heath (Lily May Peel)        & female & 35 & 1 & 0 & 113803           & 53.1000 & C123 & S & 1\\\\\n",
       "\t 5 & 3 & Allen, Mr. William Henry                            & male   & 35 & 0 & 0 & 373450           &  8.0500 & NA   & S & 0\\\\\n",
       "\t 6 & 3 & Moran, Mr. James                                    & male   & NA & 0 & 0 & 330877           &  8.4583 & NA   & Q & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 12\n",
       "\n",
       "| PassengerId &lt;dbl&gt; | Pclass &lt;dbl&gt; | Name &lt;chr&gt; | Sex &lt;chr&gt; | Age &lt;dbl&gt; | SibSp &lt;dbl&gt; | Parch &lt;dbl&gt; | Ticket &lt;chr&gt; | Fare &lt;dbl&gt; | Cabin &lt;chr&gt; | Embarked &lt;chr&gt; | Survived &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 3 | Braund, Mr. Owen Harris                             | male   | 22 | 1 | 0 | A/5 21171        |  7.2500 | NA   | S | 0 |\n",
       "| 2 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599         | 71.2833 | C85  | C | 1 |\n",
       "| 3 | 3 | Heikkinen, Miss. Laina                              | female | 26 | 0 | 0 | STON/O2. 3101282 |  7.9250 | NA   | S | 1 |\n",
       "| 4 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female | 35 | 1 | 0 | 113803           | 53.1000 | C123 | S | 1 |\n",
       "| 5 | 3 | Allen, Mr. William Henry                            | male   | 35 | 0 | 0 | 373450           |  8.0500 | NA   | S | 0 |\n",
       "| 6 | 3 | Moran, Mr. James                                    | male   | NA | 0 | 0 | 330877           |  8.4583 | NA   | Q | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  PassengerId Pclass Name                                                Sex   \n",
       "1 1           3      Braund, Mr. Owen Harris                             male  \n",
       "2 2           1      Cumings, Mrs. John Bradley (Florence Briggs Thayer) female\n",
       "3 3           3      Heikkinen, Miss. Laina                              female\n",
       "4 4           1      Futrelle, Mrs. Jacques Heath (Lily May Peel)        female\n",
       "5 5           3      Allen, Mr. William Henry                            male  \n",
       "6 6           3      Moran, Mr. James                                    male  \n",
       "  Age SibSp Parch Ticket           Fare    Cabin Embarked Survived\n",
       "1 22  1     0     A/5 21171         7.2500 NA    S        0       \n",
       "2 38  1     0     PC 17599         71.2833 C85   C        1       \n",
       "3 26  0     0     STON/O2. 3101282  7.9250 NA    S        1       \n",
       "4 35  1     0     113803           53.1000 C123  S        1       \n",
       "5 35  0     0     373450            8.0500 NA    S        0       \n",
       "6 NA  0     0     330877            8.4583 NA    Q        0       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  PassengerId       Pclass          Name               Sex           \n",
       " Min.   :   1   Min.   :1.000   Length:1309        Length:1309       \n",
       " 1st Qu.: 328   1st Qu.:2.000   Class :character   Class :character  \n",
       " Median : 655   Median :3.000   Mode  :character   Mode  :character  \n",
       " Mean   : 655   Mean   :2.295                                        \n",
       " 3rd Qu.: 982   3rd Qu.:3.000                                        \n",
       " Max.   :1309   Max.   :3.000                                        \n",
       "                                                                     \n",
       "      Age            SibSp            Parch          Ticket         \n",
       " Min.   : 0.17   Min.   :0.0000   Min.   :0.000   Length:1309       \n",
       " 1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   Class :character  \n",
       " Median :28.00   Median :0.0000   Median :0.000   Mode  :character  \n",
       " Mean   :29.88   Mean   :0.4989   Mean   :0.385                     \n",
       " 3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000                     \n",
       " Max.   :80.00   Max.   :8.0000   Max.   :9.000                     \n",
       " NA's   :263                                                        \n",
       "      Fare            Cabin             Embarked            Survived     \n",
       " Min.   :  0.000   Length:1309        Length:1309        Min.   :0.0000  \n",
       " 1st Qu.:  7.896   Class :character   Class :character   1st Qu.:0.0000  \n",
       " Median : 14.454   Mode  :character   Mode  :character   Median :0.0000  \n",
       " Mean   : 33.295                                         Mean   :0.3838  \n",
       " 3rd Qu.: 31.275                                         3rd Qu.:1.0000  \n",
       " Max.   :512.329                                         Max.   :1.0000  \n",
       " NA's   :1                                               NA's   :418     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load, view data examples, and summarize the dataset\n",
    "titanic <- read_csv('titanic.csv', skip=5)\n",
    "head(titanic)\n",
    "summary(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: most models do not work with `character` type, we need to convert strings to factors for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic$Sex <- as.factor(titanic$Sex)\n",
    "titanic$Cabin <- as.factor(titanic$Cabin)\n",
    "titanic$Embarked <- as.factor(titanic$Embarked)\n",
    "titanic$Survived <- as.factor(titanic$Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Testing data**\n",
    "\n",
    "split the titanic data into training and testing sets based on the feature we want to predict `Survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>891</li><li>12</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 891\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 891\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 891  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>418</li><li>12</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 418\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 418\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 418  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_training <- filter(titanic, !is.na(Survived))\n",
    "dim(titanic_training)\n",
    "titanic_testing <- filter(titanic, is.na(Survived))\n",
    "dim(titanic_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  PassengerId        Pclass          Name               Sex     \n",
       " Min.   :  1.0   Min.   :1.000   Length:891         female:314  \n",
       " 1st Qu.:223.5   1st Qu.:2.000   Class :character   male  :577  \n",
       " Median :446.0   Median :3.000   Mode  :character               \n",
       " Mean   :446.0   Mean   :2.309                                  \n",
       " 3rd Qu.:668.5   3rd Qu.:3.000                                  \n",
       " Max.   :891.0   Max.   :3.000                                  \n",
       "                                                                \n",
       "      Age            SibSp           Parch           Ticket         \n",
       " Min.   : 0.42   Min.   :0.000   Min.   :0.0000   Length:891        \n",
       " 1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000   Class :character  \n",
       " Median :28.00   Median :0.000   Median :0.0000   Mode  :character  \n",
       " Mean   :29.70   Mean   :0.523   Mean   :0.3816                     \n",
       " 3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000                     \n",
       " Max.   :80.00   Max.   :8.000   Max.   :6.0000                     \n",
       " NA's   :177                                                        \n",
       "      Fare                Cabin     Embarked   Survived\n",
       " Min.   :  0.00   B96 B98    :  4   C   :168   0:549   \n",
       " 1st Qu.:  7.91   C23 C25 C27:  4   Q   : 77   1:342   \n",
       " Median : 14.45   G6         :  4   S   :644           \n",
       " Mean   : 32.20   C22 C26    :  3   NA's:  2           \n",
       " 3rd Qu.: 31.00   D          :  3                      \n",
       " Max.   :512.33   (Other)    :186                      \n",
       "                  NA's       :687                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if there is missing value in training data\n",
    "summary(titanic_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  PassengerId         Pclass          Name               Sex     \n",
       " Min.   : 892.0   Min.   :1.000   Length:418         female:152  \n",
       " 1st Qu.: 996.2   1st Qu.:1.000   Class :character   male  :266  \n",
       " Median :1100.5   Median :3.000   Mode  :character               \n",
       " Mean   :1100.5   Mean   :2.266                                  \n",
       " 3rd Qu.:1204.8   3rd Qu.:3.000                                  \n",
       " Max.   :1309.0   Max.   :3.000                                  \n",
       "                                                                 \n",
       "      Age            SibSp            Parch           Ticket         \n",
       " Min.   : 0.17   Min.   :0.0000   Min.   :0.0000   Length:418        \n",
       " 1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.0000   Class :character  \n",
       " Median :27.00   Median :0.0000   Median :0.0000   Mode  :character  \n",
       " Mean   :30.27   Mean   :0.4474   Mean   :0.3923                     \n",
       " 3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.0000                     \n",
       " Max.   :76.00   Max.   :8.0000   Max.   :9.0000                     \n",
       " NA's   :86                                                          \n",
       "      Fare                     Cabin     Embarked Survived  \n",
       " Min.   :  0.000   B57 B59 B63 B66:  3   C:102    0   :  0  \n",
       " 1st Qu.:  7.896   A34            :  2   Q: 46    1   :  0  \n",
       " Median : 14.454   B45            :  2   S:270    NA's:418  \n",
       " Mean   : 35.627   C101           :  2                      \n",
       " 3rd Qu.: 31.500   C116           :  2                      \n",
       " Max.   :512.329   (Other)        : 80                      \n",
       " NA's   :1         NA's           :327                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if there is missing value in test data\n",
    "summary(titanic_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting `Survived` based on `Pclass` and `Sex` using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Survived ~ Pclass + Sex, family = binomial, data = titanic_training)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.2030  -0.7036  -0.4519   0.6719   2.1599  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)   3.2946     0.2974  11.077   <2e-16 ***\n",
       "Pclass       -0.9606     0.1061  -9.057   <2e-16 ***\n",
       "Sexmale      -2.6434     0.1838 -14.380   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 1186.7  on 890  degrees of freedom\n",
       "Residual deviance:  827.2  on 888  degrees of freedom\n",
       "AIC: 833.2\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "titanic_glm <- glm(Survived ~ Pclass + Sex, data = titanic_training, family = binomial)\n",
    "\n",
    "# examine the model\n",
    "summary(titanic_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>1</dt><dd>0.0970522051145409</dd><dt>2</dt><dd>0.601802744773117</dd><dt>3</dt><dd>0.219280773210838</dd><dt>4</dt><dd>0.0970522051145409</dd><dt>5</dt><dd>0.601802744773117</dd><dt>6</dt><dd>0.0970522051145409</dd><dt>7</dt><dd>0.601802744773117</dd><dt>8</dt><dd>0.219280773210838</dd><dt>9</dt><dd>0.601802744773117</dd><dt>10</dt><dd>0.0970522051145409</dd><dt>11</dt><dd>0.0970522051145409</dd><dt>12</dt><dd>0.423283291840528</dd><dt>13</dt><dd>0.911661154107878</dd><dt>14</dt><dd>0.219280773210838</dd><dt>15</dt><dd>0.911661154107878</dd><dt>16</dt><dd>0.797950739188506</dd><dt>17</dt><dd>0.219280773210838</dd><dt>18</dt><dd>0.0970522051145409</dd><dt>19</dt><dd>0.601802744773117</dd><dt>20</dt><dd>0.601802744773117</dd><dt>21</dt><dd>0.423283291840528</dd><dt>22</dt><dd>0.0970522051145409</dd><dt>23</dt><dd>0.911661154107878</dd><dt>24</dt><dd>0.423283291840528</dd><dt>25</dt><dd>0.911661154107878</dd><dt>26</dt><dd>0.0970522051145409</dd><dt>27</dt><dd>0.911661154107878</dd><dt>28</dt><dd>0.0970522051145409</dd><dt>29</dt><dd>0.423283291840528</dd><dt>30</dt><dd>0.0970522051145409</dd><dt>31</dt><dd>0.219280773210838</dd><dt>32</dt><dd>0.219280773210838</dd><dt>33</dt><dd>0.601802744773117</dd><dt>34</dt><dd>0.601802744773117</dd><dt>35</dt><dd>0.423283291840528</dd><dt>36</dt><dd>0.0970522051145409</dd><dt>37</dt><dd>0.601802744773117</dd><dt>38</dt><dd>0.601802744773117</dd><dt>39</dt><dd>0.0970522051145409</dd><dt>40</dt><dd>0.0970522051145409</dd><dt>41</dt><dd>0.0970522051145409</dd><dt>42</dt><dd>0.423283291840528</dd><dt>43</dt><dd>0.0970522051145409</dd><dt>44</dt><dd>0.797950739188506</dd><dt>45</dt><dd>0.911661154107878</dd><dt>46</dt><dd>0.0970522051145409</dd><dt>47</dt><dd>0.423283291840528</dd><dt>48</dt><dd>0.0970522051145409</dd><dt>49</dt><dd>0.911661154107878</dd><dt>50</dt><dd>0.601802744773117</dd><dt>51</dt><dd>0.423283291840528</dd><dt>52</dt><dd>0.219280773210838</dd><dt>53</dt><dd>0.797950739188506</dd><dt>54</dt><dd>0.911661154107878</dd><dt>55</dt><dd>0.219280773210838</dd><dt>56</dt><dd>0.0970522051145409</dd><dt>57</dt><dd>0.0970522051145409</dd><dt>58</dt><dd>0.0970522051145409</dd><dt>59</dt><dd>0.0970522051145409</dd><dt>60</dt><dd>0.911661154107878</dd><dt>61</dt><dd>0.0970522051145409</dd><dt>62</dt><dd>0.219280773210838</dd><dt>63</dt><dd>0.0970522051145409</dd><dt>64</dt><dd>0.601802744773117</dd><dt>65</dt><dd>0.423283291840528</dd><dt>66</dt><dd>0.797950739188506</dd><dt>67</dt><dd>0.601802744773117</dd><dt>68</dt><dd>0.423283291840528</dd><dt>69</dt><dd>0.423283291840528</dd><dt>70</dt><dd>0.911661154107878</dd><dt>71</dt><dd>0.601802744773117</dd><dt>72</dt><dd>0.0970522051145409</dd><dt>73</dt><dd>0.601802744773117</dd><dt>74</dt><dd>0.423283291840528</dd><dt>75</dt><dd>0.911661154107878</dd><dt>76</dt><dd>0.423283291840528</dd><dt>77</dt><dd>0.0970522051145409</dd><dt>78</dt><dd>0.911661154107878</dd><dt>79</dt><dd>0.219280773210838</dd><dt>80</dt><dd>0.601802744773117</dd><dt>81</dt><dd>0.0970522051145409</dd><dt>82</dt><dd>0.423283291840528</dd><dt>83</dt><dd>0.423283291840528</dd><dt>84</dt><dd>0.0970522051145409</dd><dt>85</dt><dd>0.219280773210838</dd><dt>86</dt><dd>0.0970522051145409</dd><dt>87</dt><dd>0.601802744773117</dd><dt>88</dt><dd>0.601802744773117</dd><dt>89</dt><dd>0.601802744773117</dd><dt>90</dt><dd>0.219280773210838</dd><dt>91</dt><dd>0.601802744773117</dd><dt>92</dt><dd>0.0970522051145409</dd><dt>93</dt><dd>0.911661154107878</dd><dt>94</dt><dd>0.0970522051145409</dd><dt>95</dt><dd>0.423283291840528</dd><dt>96</dt><dd>0.0970522051145409</dd><dt>97</dt><dd>0.911661154107878</dd><dt>98</dt><dd>0.0970522051145409</dd><dt>99</dt><dd>0.601802744773117</dd><dt>100</dt><dd>0.0970522051145409</dd><dt>101</dt><dd>0.911661154107878</dd><dt>102</dt><dd>0.219280773210838</dd><dt>103</dt><dd>0.0970522051145409</dd><dt>104</dt><dd>0.0970522051145409</dd><dt>105</dt><dd>0.601802744773117</dd><dt>106</dt><dd>0.0970522051145409</dd><dt>107</dt><dd>0.0970522051145409</dd><dt>108</dt><dd>0.0970522051145409</dd><dt>109</dt><dd>0.0970522051145409</dd><dt>110</dt><dd>0.219280773210838</dd><dt>111</dt><dd>0.219280773210838</dd><dt>112</dt><dd>0.601802744773117</dd><dt>113</dt><dd>0.911661154107878</dd><dt>114</dt><dd>0.601802744773117</dd><dt>115</dt><dd>0.911661154107878</dd><dt>116</dt><dd>0.0970522051145409</dd><dt>117</dt><dd>0.0970522051145409</dd><dt>118</dt><dd>0.601802744773117</dd><dt>119</dt><dd>0.423283291840528</dd><dt>120</dt><dd>0.797950739188506</dd><dt>121</dt><dd>0.797950739188506</dd><dt>122</dt><dd>0.0970522051145409</dd><dt>123</dt><dd>0.911661154107878</dd><dt>124</dt><dd>0.0970522051145409</dd><dt>125</dt><dd>0.0970522051145409</dd><dt>126</dt><dd>0.601802744773117</dd><dt>127</dt><dd>0.0970522051145409</dd><dt>128</dt><dd>0.601802744773117</dd><dt>129</dt><dd>0.219280773210838</dd><dt>130</dt><dd>0.0970522051145409</dd><dt>131</dt><dd>0.0970522051145409</dd><dt>132</dt><dd>0.423283291840528</dd><dt>133</dt><dd>0.601802744773117</dd><dt>134</dt><dd>0.0970522051145409</dd><dt>135</dt><dd>0.0970522051145409</dd><dt>136</dt><dd>0.0970522051145409</dd><dt>137</dt><dd>0.0970522051145409</dd><dt>138</dt><dd>0.219280773210838</dd><dt>139</dt><dd>0.601802744773117</dd><dt>140</dt><dd>0.0970522051145409</dd><dt>141</dt><dd>0.601802744773117</dd><dt>142</dt><dd>0.911661154107878</dd><dt>143</dt><dd>0.423283291840528</dd><dt>144</dt><dd>0.219280773210838</dd><dt>145</dt><dd>0.423283291840528</dd><dt>146</dt><dd>0.0970522051145409</dd><dt>147</dt><dd>0.423283291840528</dd><dt>148</dt><dd>0.0970522051145409</dd><dt>149</dt><dd>0.423283291840528</dd><dt>150</dt><dd>0.219280773210838</dd><dt>151</dt><dd>0.911661154107878</dd><dt>152</dt><dd>0.0970522051145409</dd><dt>153</dt><dd>0.0970522051145409</dd><dt>154</dt><dd>0.601802744773117</dd><dt>155</dt><dd>0.0970522051145409</dd><dt>156</dt><dd>0.0970522051145409</dd><dt>157</dt><dd>0.911661154107878</dd><dt>158</dt><dd>0.601802744773117</dd><dt>159</dt><dd>0.423283291840528</dd><dt>160</dt><dd>0.601802744773117</dd><dt>161</dt><dd>0.601802744773117</dd><dt>162</dt><dd>0.0970522051145409</dd><dt>163</dt><dd>0.797950739188506</dd><dt>164</dt><dd>0.0970522051145409</dd><dt>165</dt><dd>0.219280773210838</dd><dt>166</dt><dd>0.601802744773117</dd><dt>167</dt><dd>0.423283291840528</dd><dt>168</dt><dd>0.0970522051145409</dd><dt>169</dt><dd>0.911661154107878</dd><dt>170</dt><dd>0.601802744773117</dd><dt>171</dt><dd>0.0970522051145409</dd><dt>172</dt><dd>0.0970522051145409</dd><dt>173</dt><dd>0.0970522051145409</dd><dt>174</dt><dd>0.0970522051145409</dd><dt>175</dt><dd>0.0970522051145409</dd><dt>176</dt><dd>0.797950739188506</dd><dt>177</dt><dd>0.797950739188506</dd><dt>178</dt><dd>0.423283291840528</dd><dt>179</dt><dd>0.797950739188506</dd><dt>180</dt><dd>0.911661154107878</dd><dt>181</dt><dd>0.219280773210838</dd><dt>182</dt><dd>0.423283291840528</dd><dt>183</dt><dd>0.911661154107878</dd><dt>184</dt><dd>0.0970522051145409</dd><dt>185</dt><dd>0.911661154107878</dd><dt>186</dt><dd>0.219280773210838</dd><dt>187</dt><dd>0.797950739188506</dd><dt>188</dt><dd>0.0970522051145409</dd><dt>189</dt><dd>0.601802744773117</dd><dt>190</dt><dd>0.219280773210838</dd><dt>191</dt><dd>0.219280773210838</dd><dt>192</dt><dd>0.423283291840528</dd><dt>193</dt><dd>0.0970522051145409</dd><dt>194</dt><dd>0.219280773210838</dd><dt>195</dt><dd>0.219280773210838</dd><dt>196</dt><dd>0.0970522051145409</dd><dt>197</dt><dd>0.423283291840528</dd><dt>198</dt><dd>0.601802744773117</dd><dt>199</dt><dd>0.219280773210838</dd><dt>200</dt><dd>0.601802744773117</dd><dt>201</dt><dd>⋯</dd><dt>202</dt><dd>0.911661154107878</dd><dt>203</dt><dd>0.0970522051145409</dd><dt>204</dt><dd>0.797950739188506</dd><dt>205</dt><dd>0.0970522051145409</dd><dt>206</dt><dd>0.797950739188506</dd><dt>207</dt><dd>0.0970522051145409</dd><dt>208</dt><dd>0.911661154107878</dd><dt>209</dt><dd>0.601802744773117</dd><dt>210</dt><dd>0.0970522051145409</dd><dt>211</dt><dd>0.601802744773117</dd><dt>212</dt><dd>0.0970522051145409</dd><dt>213</dt><dd>0.219280773210838</dd><dt>214</dt><dd>0.219280773210838</dd><dt>215</dt><dd>0.911661154107878</dd><dt>216</dt><dd>0.0970522051145409</dd><dt>217</dt><dd>0.0970522051145409</dd><dt>218</dt><dd>0.423283291840528</dd><dt>219</dt><dd>0.0970522051145409</dd><dt>220</dt><dd>0.423283291840528</dd><dt>221</dt><dd>0.0970522051145409</dd><dt>222</dt><dd>0.797950739188506</dd><dt>223</dt><dd>0.911661154107878</dd><dt>224</dt><dd>0.911661154107878</dd><dt>225</dt><dd>0.797950739188506</dd><dt>226</dt><dd>0.423283291840528</dd><dt>227</dt><dd>0.0970522051145409</dd><dt>228</dt><dd>0.0970522051145409</dd><dt>229</dt><dd>0.423283291840528</dd><dt>230</dt><dd>0.797950739188506</dd><dt>231</dt><dd>0.219280773210838</dd><dt>232</dt><dd>0.797950739188506</dd><dt>233</dt><dd>0.601802744773117</dd><dt>234</dt><dd>0.797950739188506</dd><dt>235</dt><dd>0.0970522051145409</dd><dt>236</dt><dd>0.423283291840528</dd><dt>237</dt><dd>0.0970522051145409</dd><dt>238</dt><dd>0.0970522051145409</dd><dt>239</dt><dd>0.0970522051145409</dd><dt>240</dt><dd>0.0970522051145409</dd><dt>241</dt><dd>0.0970522051145409</dd><dt>242</dt><dd>0.797950739188506</dd><dt>243</dt><dd>0.0970522051145409</dd><dt>244</dt><dd>0.0970522051145409</dd><dt>245</dt><dd>0.0970522051145409</dd><dt>246</dt><dd>0.797950739188506</dd><dt>247</dt><dd>0.601802744773117</dd><dt>248</dt><dd>0.219280773210838</dd><dt>249</dt><dd>0.0970522051145409</dd><dt>250</dt><dd>0.423283291840528</dd><dt>251</dt><dd>0.0970522051145409</dd><dt>252</dt><dd>0.601802744773117</dd><dt>253</dt><dd>0.0970522051145409</dd><dt>254</dt><dd>0.423283291840528</dd><dt>255</dt><dd>0.0970522051145409</dd><dt>256</dt><dd>0.911661154107878</dd><dt>257</dt><dd>0.601802744773117</dd><dt>258</dt><dd>0.0970522051145409</dd><dt>259</dt><dd>0.797950739188506</dd><dt>260</dt><dd>0.219280773210838</dd><dt>261</dt><dd>0.219280773210838</dd><dt>262</dt><dd>0.219280773210838</dd><dt>263</dt><dd>0.219280773210838</dd><dt>264</dt><dd>0.601802744773117</dd><dt>265</dt><dd>0.0970522051145409</dd><dt>266</dt><dd>0.601802744773117</dd><dt>267</dt><dd>0.601802744773117</dd><dt>268</dt><dd>0.601802744773117</dd><dt>269</dt><dd>0.0970522051145409</dd><dt>270</dt><dd>0.0970522051145409</dd><dt>271</dt><dd>0.423283291840528</dd><dt>272</dt><dd>0.0970522051145409</dd><dt>273</dt><dd>0.0970522051145409</dd><dt>274</dt><dd>0.423283291840528</dd><dt>275</dt><dd>0.601802744773117</dd><dt>276</dt><dd>0.0970522051145409</dd><dt>277</dt><dd>0.423283291840528</dd><dt>278</dt><dd>0.0970522051145409</dd><dt>279</dt><dd>0.0970522051145409</dd><dt>280</dt><dd>0.797950739188506</dd><dt>281</dt><dd>0.0970522051145409</dd><dt>282</dt><dd>0.423283291840528</dd><dt>283</dt><dd>0.0970522051145409</dd><dt>284</dt><dd>0.0970522051145409</dd><dt>285</dt><dd>0.219280773210838</dd><dt>286</dt><dd>0.219280773210838</dd><dt>287</dt><dd>0.0970522051145409</dd><dt>288</dt><dd>0.601802744773117</dd><dt>289</dt><dd>0.911661154107878</dd><dt>290</dt><dd>0.423283291840528</dd><dt>291</dt><dd>0.0970522051145409</dd><dt>292</dt><dd>0.423283291840528</dd><dt>293</dt><dd>0.601802744773117</dd><dt>294</dt><dd>0.0970522051145409</dd><dt>295</dt><dd>0.0970522051145409</dd><dt>296</dt><dd>0.0970522051145409</dd><dt>297</dt><dd>0.601802744773117</dd><dt>298</dt><dd>0.911661154107878</dd><dt>299</dt><dd>0.601802744773117</dd><dt>300</dt><dd>0.423283291840528</dd><dt>301</dt><dd>0.219280773210838</dd><dt>302</dt><dd>0.0970522051145409</dd><dt>303</dt><dd>0.219280773210838</dd><dt>304</dt><dd>0.0970522051145409</dd><dt>305</dt><dd>0.0970522051145409</dd><dt>306</dt><dd>0.219280773210838</dd><dt>307</dt><dd>0.423283291840528</dd><dt>308</dt><dd>0.911661154107878</dd><dt>309</dt><dd>0.0970522051145409</dd><dt>310</dt><dd>0.797950739188506</dd><dt>311</dt><dd>0.423283291840528</dd><dt>312</dt><dd>0.219280773210838</dd><dt>313</dt><dd>0.219280773210838</dd><dt>314</dt><dd>0.797950739188506</dd><dt>315</dt><dd>0.423283291840528</dd><dt>316</dt><dd>0.0970522051145409</dd><dt>317</dt><dd>0.601802744773117</dd><dt>318</dt><dd>0.0970522051145409</dd><dt>319</dt><dd>0.423283291840528</dd><dt>320</dt><dd>0.219280773210838</dd><dt>321</dt><dd>0.0970522051145409</dd><dt>322</dt><dd>0.219280773210838</dd><dt>323</dt><dd>0.0970522051145409</dd><dt>324</dt><dd>0.219280773210838</dd><dt>325</dt><dd>0.0970522051145409</dd><dt>326</dt><dd>0.0970522051145409</dd><dt>327</dt><dd>0.911661154107878</dd><dt>328</dt><dd>0.0970522051145409</dd><dt>329</dt><dd>0.601802744773117</dd><dt>330</dt><dd>0.219280773210838</dd><dt>331</dt><dd>0.601802744773117</dd><dt>332</dt><dd>0.219280773210838</dd><dt>333</dt><dd>0.797950739188506</dd><dt>334</dt><dd>0.911661154107878</dd><dt>335</dt><dd>0.219280773210838</dd><dt>336</dt><dd>0.219280773210838</dd><dt>337</dt><dd>0.219280773210838</dd><dt>338</dt><dd>0.601802744773117</dd><dt>339</dt><dd>0.423283291840528</dd><dt>340</dt><dd>0.911661154107878</dd><dt>341</dt><dd>0.0970522051145409</dd><dt>342</dt><dd>0.0970522051145409</dd><dt>343</dt><dd>0.601802744773117</dd><dt>344</dt><dd>0.0970522051145409</dd><dt>345</dt><dd>0.797950739188506</dd><dt>346</dt><dd>0.797950739188506</dd><dt>347</dt><dd>0.0970522051145409</dd><dt>348</dt><dd>0.911661154107878</dd><dt>349</dt><dd>0.601802744773117</dd><dt>350</dt><dd>0.0970522051145409</dd><dt>351</dt><dd>0.601802744773117</dd><dt>352</dt><dd>0.911661154107878</dd><dt>353</dt><dd>0.219280773210838</dd><dt>354</dt><dd>0.219280773210838</dd><dt>355</dt><dd>0.911661154107878</dd><dt>356</dt><dd>0.423283291840528</dd><dt>357</dt><dd>0.219280773210838</dd><dt>358</dt><dd>0.911661154107878</dd><dt>359</dt><dd>0.911661154107878</dd><dt>360</dt><dd>0.601802744773117</dd><dt>361</dt><dd>0.219280773210838</dd><dt>362</dt><dd>0.423283291840528</dd><dt>363</dt><dd>0.0970522051145409</dd><dt>364</dt><dd>0.0970522051145409</dd><dt>365</dt><dd>0.0970522051145409</dd><dt>366</dt><dd>0.601802744773117</dd><dt>367</dt><dd>0.601802744773117</dd><dt>368</dt><dd>0.219280773210838</dd><dt>369</dt><dd>0.797950739188506</dd><dt>370</dt><dd>0.0970522051145409</dd><dt>371</dt><dd>0.219280773210838</dd><dt>372</dt><dd>0.0970522051145409</dd><dt>373</dt><dd>0.0970522051145409</dd><dt>374</dt><dd>0.423283291840528</dd><dt>375</dt><dd>0.911661154107878</dd><dt>376</dt><dd>0.0970522051145409</dd><dt>377</dt><dd>0.219280773210838</dd><dt>378</dt><dd>0.0970522051145409</dd><dt>379</dt><dd>0.911661154107878</dd><dt>380</dt><dd>0.0970522051145409</dd><dt>381</dt><dd>0.911661154107878</dd><dt>382</dt><dd>0.0970522051145409</dd><dt>383</dt><dd>0.0970522051145409</dd><dt>384</dt><dd>0.911661154107878</dd><dt>385</dt><dd>0.219280773210838</dd><dt>386</dt><dd>0.911661154107878</dd><dt>387</dt><dd>0.423283291840528</dd><dt>388</dt><dd>0.423283291840528</dd><dt>389</dt><dd>0.219280773210838</dd><dt>390</dt><dd>0.219280773210838</dd><dt>391</dt><dd>0.423283291840528</dd><dt>392</dt><dd>0.601802744773117</dd><dt>393</dt><dd>0.601802744773117</dd><dt>394</dt><dd>0.601802744773117</dd><dt>395</dt><dd>0.911661154107878</dd><dt>396</dt><dd>0.601802744773117</dd><dt>397</dt><dd>0.0970522051145409</dd><dt>398</dt><dd>0.911661154107878</dd><dt>399</dt><dd>0.0970522051145409</dd><dt>400</dt><dd>0.0970522051145409</dd><dt>401</dt><dd>0.0970522051145409</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.0970522051145409\n",
       "\\item[2] 0.601802744773117\n",
       "\\item[3] 0.219280773210838\n",
       "\\item[4] 0.0970522051145409\n",
       "\\item[5] 0.601802744773117\n",
       "\\item[6] 0.0970522051145409\n",
       "\\item[7] 0.601802744773117\n",
       "\\item[8] 0.219280773210838\n",
       "\\item[9] 0.601802744773117\n",
       "\\item[10] 0.0970522051145409\n",
       "\\item[11] 0.0970522051145409\n",
       "\\item[12] 0.423283291840528\n",
       "\\item[13] 0.911661154107878\n",
       "\\item[14] 0.219280773210838\n",
       "\\item[15] 0.911661154107878\n",
       "\\item[16] 0.797950739188506\n",
       "\\item[17] 0.219280773210838\n",
       "\\item[18] 0.0970522051145409\n",
       "\\item[19] 0.601802744773117\n",
       "\\item[20] 0.601802744773117\n",
       "\\item[21] 0.423283291840528\n",
       "\\item[22] 0.0970522051145409\n",
       "\\item[23] 0.911661154107878\n",
       "\\item[24] 0.423283291840528\n",
       "\\item[25] 0.911661154107878\n",
       "\\item[26] 0.0970522051145409\n",
       "\\item[27] 0.911661154107878\n",
       "\\item[28] 0.0970522051145409\n",
       "\\item[29] 0.423283291840528\n",
       "\\item[30] 0.0970522051145409\n",
       "\\item[31] 0.219280773210838\n",
       "\\item[32] 0.219280773210838\n",
       "\\item[33] 0.601802744773117\n",
       "\\item[34] 0.601802744773117\n",
       "\\item[35] 0.423283291840528\n",
       "\\item[36] 0.0970522051145409\n",
       "\\item[37] 0.601802744773117\n",
       "\\item[38] 0.601802744773117\n",
       "\\item[39] 0.0970522051145409\n",
       "\\item[40] 0.0970522051145409\n",
       "\\item[41] 0.0970522051145409\n",
       "\\item[42] 0.423283291840528\n",
       "\\item[43] 0.0970522051145409\n",
       "\\item[44] 0.797950739188506\n",
       "\\item[45] 0.911661154107878\n",
       "\\item[46] 0.0970522051145409\n",
       "\\item[47] 0.423283291840528\n",
       "\\item[48] 0.0970522051145409\n",
       "\\item[49] 0.911661154107878\n",
       "\\item[50] 0.601802744773117\n",
       "\\item[51] 0.423283291840528\n",
       "\\item[52] 0.219280773210838\n",
       "\\item[53] 0.797950739188506\n",
       "\\item[54] 0.911661154107878\n",
       "\\item[55] 0.219280773210838\n",
       "\\item[56] 0.0970522051145409\n",
       "\\item[57] 0.0970522051145409\n",
       "\\item[58] 0.0970522051145409\n",
       "\\item[59] 0.0970522051145409\n",
       "\\item[60] 0.911661154107878\n",
       "\\item[61] 0.0970522051145409\n",
       "\\item[62] 0.219280773210838\n",
       "\\item[63] 0.0970522051145409\n",
       "\\item[64] 0.601802744773117\n",
       "\\item[65] 0.423283291840528\n",
       "\\item[66] 0.797950739188506\n",
       "\\item[67] 0.601802744773117\n",
       "\\item[68] 0.423283291840528\n",
       "\\item[69] 0.423283291840528\n",
       "\\item[70] 0.911661154107878\n",
       "\\item[71] 0.601802744773117\n",
       "\\item[72] 0.0970522051145409\n",
       "\\item[73] 0.601802744773117\n",
       "\\item[74] 0.423283291840528\n",
       "\\item[75] 0.911661154107878\n",
       "\\item[76] 0.423283291840528\n",
       "\\item[77] 0.0970522051145409\n",
       "\\item[78] 0.911661154107878\n",
       "\\item[79] 0.219280773210838\n",
       "\\item[80] 0.601802744773117\n",
       "\\item[81] 0.0970522051145409\n",
       "\\item[82] 0.423283291840528\n",
       "\\item[83] 0.423283291840528\n",
       "\\item[84] 0.0970522051145409\n",
       "\\item[85] 0.219280773210838\n",
       "\\item[86] 0.0970522051145409\n",
       "\\item[87] 0.601802744773117\n",
       "\\item[88] 0.601802744773117\n",
       "\\item[89] 0.601802744773117\n",
       "\\item[90] 0.219280773210838\n",
       "\\item[91] 0.601802744773117\n",
       "\\item[92] 0.0970522051145409\n",
       "\\item[93] 0.911661154107878\n",
       "\\item[94] 0.0970522051145409\n",
       "\\item[95] 0.423283291840528\n",
       "\\item[96] 0.0970522051145409\n",
       "\\item[97] 0.911661154107878\n",
       "\\item[98] 0.0970522051145409\n",
       "\\item[99] 0.601802744773117\n",
       "\\item[100] 0.0970522051145409\n",
       "\\item[101] 0.911661154107878\n",
       "\\item[102] 0.219280773210838\n",
       "\\item[103] 0.0970522051145409\n",
       "\\item[104] 0.0970522051145409\n",
       "\\item[105] 0.601802744773117\n",
       "\\item[106] 0.0970522051145409\n",
       "\\item[107] 0.0970522051145409\n",
       "\\item[108] 0.0970522051145409\n",
       "\\item[109] 0.0970522051145409\n",
       "\\item[110] 0.219280773210838\n",
       "\\item[111] 0.219280773210838\n",
       "\\item[112] 0.601802744773117\n",
       "\\item[113] 0.911661154107878\n",
       "\\item[114] 0.601802744773117\n",
       "\\item[115] 0.911661154107878\n",
       "\\item[116] 0.0970522051145409\n",
       "\\item[117] 0.0970522051145409\n",
       "\\item[118] 0.601802744773117\n",
       "\\item[119] 0.423283291840528\n",
       "\\item[120] 0.797950739188506\n",
       "\\item[121] 0.797950739188506\n",
       "\\item[122] 0.0970522051145409\n",
       "\\item[123] 0.911661154107878\n",
       "\\item[124] 0.0970522051145409\n",
       "\\item[125] 0.0970522051145409\n",
       "\\item[126] 0.601802744773117\n",
       "\\item[127] 0.0970522051145409\n",
       "\\item[128] 0.601802744773117\n",
       "\\item[129] 0.219280773210838\n",
       "\\item[130] 0.0970522051145409\n",
       "\\item[131] 0.0970522051145409\n",
       "\\item[132] 0.423283291840528\n",
       "\\item[133] 0.601802744773117\n",
       "\\item[134] 0.0970522051145409\n",
       "\\item[135] 0.0970522051145409\n",
       "\\item[136] 0.0970522051145409\n",
       "\\item[137] 0.0970522051145409\n",
       "\\item[138] 0.219280773210838\n",
       "\\item[139] 0.601802744773117\n",
       "\\item[140] 0.0970522051145409\n",
       "\\item[141] 0.601802744773117\n",
       "\\item[142] 0.911661154107878\n",
       "\\item[143] 0.423283291840528\n",
       "\\item[144] 0.219280773210838\n",
       "\\item[145] 0.423283291840528\n",
       "\\item[146] 0.0970522051145409\n",
       "\\item[147] 0.423283291840528\n",
       "\\item[148] 0.0970522051145409\n",
       "\\item[149] 0.423283291840528\n",
       "\\item[150] 0.219280773210838\n",
       "\\item[151] 0.911661154107878\n",
       "\\item[152] 0.0970522051145409\n",
       "\\item[153] 0.0970522051145409\n",
       "\\item[154] 0.601802744773117\n",
       "\\item[155] 0.0970522051145409\n",
       "\\item[156] 0.0970522051145409\n",
       "\\item[157] 0.911661154107878\n",
       "\\item[158] 0.601802744773117\n",
       "\\item[159] 0.423283291840528\n",
       "\\item[160] 0.601802744773117\n",
       "\\item[161] 0.601802744773117\n",
       "\\item[162] 0.0970522051145409\n",
       "\\item[163] 0.797950739188506\n",
       "\\item[164] 0.0970522051145409\n",
       "\\item[165] 0.219280773210838\n",
       "\\item[166] 0.601802744773117\n",
       "\\item[167] 0.423283291840528\n",
       "\\item[168] 0.0970522051145409\n",
       "\\item[169] 0.911661154107878\n",
       "\\item[170] 0.601802744773117\n",
       "\\item[171] 0.0970522051145409\n",
       "\\item[172] 0.0970522051145409\n",
       "\\item[173] 0.0970522051145409\n",
       "\\item[174] 0.0970522051145409\n",
       "\\item[175] 0.0970522051145409\n",
       "\\item[176] 0.797950739188506\n",
       "\\item[177] 0.797950739188506\n",
       "\\item[178] 0.423283291840528\n",
       "\\item[179] 0.797950739188506\n",
       "\\item[180] 0.911661154107878\n",
       "\\item[181] 0.219280773210838\n",
       "\\item[182] 0.423283291840528\n",
       "\\item[183] 0.911661154107878\n",
       "\\item[184] 0.0970522051145409\n",
       "\\item[185] 0.911661154107878\n",
       "\\item[186] 0.219280773210838\n",
       "\\item[187] 0.797950739188506\n",
       "\\item[188] 0.0970522051145409\n",
       "\\item[189] 0.601802744773117\n",
       "\\item[190] 0.219280773210838\n",
       "\\item[191] 0.219280773210838\n",
       "\\item[192] 0.423283291840528\n",
       "\\item[193] 0.0970522051145409\n",
       "\\item[194] 0.219280773210838\n",
       "\\item[195] 0.219280773210838\n",
       "\\item[196] 0.0970522051145409\n",
       "\\item[197] 0.423283291840528\n",
       "\\item[198] 0.601802744773117\n",
       "\\item[199] 0.219280773210838\n",
       "\\item[200] 0.601802744773117\n",
       "\\item[201] ⋯\n",
       "\\item[202] 0.911661154107878\n",
       "\\item[203] 0.0970522051145409\n",
       "\\item[204] 0.797950739188506\n",
       "\\item[205] 0.0970522051145409\n",
       "\\item[206] 0.797950739188506\n",
       "\\item[207] 0.0970522051145409\n",
       "\\item[208] 0.911661154107878\n",
       "\\item[209] 0.601802744773117\n",
       "\\item[210] 0.0970522051145409\n",
       "\\item[211] 0.601802744773117\n",
       "\\item[212] 0.0970522051145409\n",
       "\\item[213] 0.219280773210838\n",
       "\\item[214] 0.219280773210838\n",
       "\\item[215] 0.911661154107878\n",
       "\\item[216] 0.0970522051145409\n",
       "\\item[217] 0.0970522051145409\n",
       "\\item[218] 0.423283291840528\n",
       "\\item[219] 0.0970522051145409\n",
       "\\item[220] 0.423283291840528\n",
       "\\item[221] 0.0970522051145409\n",
       "\\item[222] 0.797950739188506\n",
       "\\item[223] 0.911661154107878\n",
       "\\item[224] 0.911661154107878\n",
       "\\item[225] 0.797950739188506\n",
       "\\item[226] 0.423283291840528\n",
       "\\item[227] 0.0970522051145409\n",
       "\\item[228] 0.0970522051145409\n",
       "\\item[229] 0.423283291840528\n",
       "\\item[230] 0.797950739188506\n",
       "\\item[231] 0.219280773210838\n",
       "\\item[232] 0.797950739188506\n",
       "\\item[233] 0.601802744773117\n",
       "\\item[234] 0.797950739188506\n",
       "\\item[235] 0.0970522051145409\n",
       "\\item[236] 0.423283291840528\n",
       "\\item[237] 0.0970522051145409\n",
       "\\item[238] 0.0970522051145409\n",
       "\\item[239] 0.0970522051145409\n",
       "\\item[240] 0.0970522051145409\n",
       "\\item[241] 0.0970522051145409\n",
       "\\item[242] 0.797950739188506\n",
       "\\item[243] 0.0970522051145409\n",
       "\\item[244] 0.0970522051145409\n",
       "\\item[245] 0.0970522051145409\n",
       "\\item[246] 0.797950739188506\n",
       "\\item[247] 0.601802744773117\n",
       "\\item[248] 0.219280773210838\n",
       "\\item[249] 0.0970522051145409\n",
       "\\item[250] 0.423283291840528\n",
       "\\item[251] 0.0970522051145409\n",
       "\\item[252] 0.601802744773117\n",
       "\\item[253] 0.0970522051145409\n",
       "\\item[254] 0.423283291840528\n",
       "\\item[255] 0.0970522051145409\n",
       "\\item[256] 0.911661154107878\n",
       "\\item[257] 0.601802744773117\n",
       "\\item[258] 0.0970522051145409\n",
       "\\item[259] 0.797950739188506\n",
       "\\item[260] 0.219280773210838\n",
       "\\item[261] 0.219280773210838\n",
       "\\item[262] 0.219280773210838\n",
       "\\item[263] 0.219280773210838\n",
       "\\item[264] 0.601802744773117\n",
       "\\item[265] 0.0970522051145409\n",
       "\\item[266] 0.601802744773117\n",
       "\\item[267] 0.601802744773117\n",
       "\\item[268] 0.601802744773117\n",
       "\\item[269] 0.0970522051145409\n",
       "\\item[270] 0.0970522051145409\n",
       "\\item[271] 0.423283291840528\n",
       "\\item[272] 0.0970522051145409\n",
       "\\item[273] 0.0970522051145409\n",
       "\\item[274] 0.423283291840528\n",
       "\\item[275] 0.601802744773117\n",
       "\\item[276] 0.0970522051145409\n",
       "\\item[277] 0.423283291840528\n",
       "\\item[278] 0.0970522051145409\n",
       "\\item[279] 0.0970522051145409\n",
       "\\item[280] 0.797950739188506\n",
       "\\item[281] 0.0970522051145409\n",
       "\\item[282] 0.423283291840528\n",
       "\\item[283] 0.0970522051145409\n",
       "\\item[284] 0.0970522051145409\n",
       "\\item[285] 0.219280773210838\n",
       "\\item[286] 0.219280773210838\n",
       "\\item[287] 0.0970522051145409\n",
       "\\item[288] 0.601802744773117\n",
       "\\item[289] 0.911661154107878\n",
       "\\item[290] 0.423283291840528\n",
       "\\item[291] 0.0970522051145409\n",
       "\\item[292] 0.423283291840528\n",
       "\\item[293] 0.601802744773117\n",
       "\\item[294] 0.0970522051145409\n",
       "\\item[295] 0.0970522051145409\n",
       "\\item[296] 0.0970522051145409\n",
       "\\item[297] 0.601802744773117\n",
       "\\item[298] 0.911661154107878\n",
       "\\item[299] 0.601802744773117\n",
       "\\item[300] 0.423283291840528\n",
       "\\item[301] 0.219280773210838\n",
       "\\item[302] 0.0970522051145409\n",
       "\\item[303] 0.219280773210838\n",
       "\\item[304] 0.0970522051145409\n",
       "\\item[305] 0.0970522051145409\n",
       "\\item[306] 0.219280773210838\n",
       "\\item[307] 0.423283291840528\n",
       "\\item[308] 0.911661154107878\n",
       "\\item[309] 0.0970522051145409\n",
       "\\item[310] 0.797950739188506\n",
       "\\item[311] 0.423283291840528\n",
       "\\item[312] 0.219280773210838\n",
       "\\item[313] 0.219280773210838\n",
       "\\item[314] 0.797950739188506\n",
       "\\item[315] 0.423283291840528\n",
       "\\item[316] 0.0970522051145409\n",
       "\\item[317] 0.601802744773117\n",
       "\\item[318] 0.0970522051145409\n",
       "\\item[319] 0.423283291840528\n",
       "\\item[320] 0.219280773210838\n",
       "\\item[321] 0.0970522051145409\n",
       "\\item[322] 0.219280773210838\n",
       "\\item[323] 0.0970522051145409\n",
       "\\item[324] 0.219280773210838\n",
       "\\item[325] 0.0970522051145409\n",
       "\\item[326] 0.0970522051145409\n",
       "\\item[327] 0.911661154107878\n",
       "\\item[328] 0.0970522051145409\n",
       "\\item[329] 0.601802744773117\n",
       "\\item[330] 0.219280773210838\n",
       "\\item[331] 0.601802744773117\n",
       "\\item[332] 0.219280773210838\n",
       "\\item[333] 0.797950739188506\n",
       "\\item[334] 0.911661154107878\n",
       "\\item[335] 0.219280773210838\n",
       "\\item[336] 0.219280773210838\n",
       "\\item[337] 0.219280773210838\n",
       "\\item[338] 0.601802744773117\n",
       "\\item[339] 0.423283291840528\n",
       "\\item[340] 0.911661154107878\n",
       "\\item[341] 0.0970522051145409\n",
       "\\item[342] 0.0970522051145409\n",
       "\\item[343] 0.601802744773117\n",
       "\\item[344] 0.0970522051145409\n",
       "\\item[345] 0.797950739188506\n",
       "\\item[346] 0.797950739188506\n",
       "\\item[347] 0.0970522051145409\n",
       "\\item[348] 0.911661154107878\n",
       "\\item[349] 0.601802744773117\n",
       "\\item[350] 0.0970522051145409\n",
       "\\item[351] 0.601802744773117\n",
       "\\item[352] 0.911661154107878\n",
       "\\item[353] 0.219280773210838\n",
       "\\item[354] 0.219280773210838\n",
       "\\item[355] 0.911661154107878\n",
       "\\item[356] 0.423283291840528\n",
       "\\item[357] 0.219280773210838\n",
       "\\item[358] 0.911661154107878\n",
       "\\item[359] 0.911661154107878\n",
       "\\item[360] 0.601802744773117\n",
       "\\item[361] 0.219280773210838\n",
       "\\item[362] 0.423283291840528\n",
       "\\item[363] 0.0970522051145409\n",
       "\\item[364] 0.0970522051145409\n",
       "\\item[365] 0.0970522051145409\n",
       "\\item[366] 0.601802744773117\n",
       "\\item[367] 0.601802744773117\n",
       "\\item[368] 0.219280773210838\n",
       "\\item[369] 0.797950739188506\n",
       "\\item[370] 0.0970522051145409\n",
       "\\item[371] 0.219280773210838\n",
       "\\item[372] 0.0970522051145409\n",
       "\\item[373] 0.0970522051145409\n",
       "\\item[374] 0.423283291840528\n",
       "\\item[375] 0.911661154107878\n",
       "\\item[376] 0.0970522051145409\n",
       "\\item[377] 0.219280773210838\n",
       "\\item[378] 0.0970522051145409\n",
       "\\item[379] 0.911661154107878\n",
       "\\item[380] 0.0970522051145409\n",
       "\\item[381] 0.911661154107878\n",
       "\\item[382] 0.0970522051145409\n",
       "\\item[383] 0.0970522051145409\n",
       "\\item[384] 0.911661154107878\n",
       "\\item[385] 0.219280773210838\n",
       "\\item[386] 0.911661154107878\n",
       "\\item[387] 0.423283291840528\n",
       "\\item[388] 0.423283291840528\n",
       "\\item[389] 0.219280773210838\n",
       "\\item[390] 0.219280773210838\n",
       "\\item[391] 0.423283291840528\n",
       "\\item[392] 0.601802744773117\n",
       "\\item[393] 0.601802744773117\n",
       "\\item[394] 0.601802744773117\n",
       "\\item[395] 0.911661154107878\n",
       "\\item[396] 0.601802744773117\n",
       "\\item[397] 0.0970522051145409\n",
       "\\item[398] 0.911661154107878\n",
       "\\item[399] 0.0970522051145409\n",
       "\\item[400] 0.0970522051145409\n",
       "\\item[401] 0.0970522051145409\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.09705220511454092\n",
       ":   0.6018027447731173\n",
       ":   0.2192807732108384\n",
       ":   0.09705220511454095\n",
       ":   0.6018027447731176\n",
       ":   0.09705220511454097\n",
       ":   0.6018027447731178\n",
       ":   0.2192807732108389\n",
       ":   0.60180274477311710\n",
       ":   0.097052205114540911\n",
       ":   0.097052205114540912\n",
       ":   0.42328329184052813\n",
       ":   0.91166115410787814\n",
       ":   0.21928077321083815\n",
       ":   0.91166115410787816\n",
       ":   0.79795073918850617\n",
       ":   0.21928077321083818\n",
       ":   0.097052205114540919\n",
       ":   0.60180274477311720\n",
       ":   0.60180274477311721\n",
       ":   0.42328329184052822\n",
       ":   0.097052205114540923\n",
       ":   0.91166115410787824\n",
       ":   0.42328329184052825\n",
       ":   0.91166115410787826\n",
       ":   0.097052205114540927\n",
       ":   0.91166115410787828\n",
       ":   0.097052205114540929\n",
       ":   0.42328329184052830\n",
       ":   0.097052205114540931\n",
       ":   0.21928077321083832\n",
       ":   0.21928077321083833\n",
       ":   0.60180274477311734\n",
       ":   0.60180274477311735\n",
       ":   0.42328329184052836\n",
       ":   0.097052205114540937\n",
       ":   0.60180274477311738\n",
       ":   0.60180274477311739\n",
       ":   0.097052205114540940\n",
       ":   0.097052205114540941\n",
       ":   0.097052205114540942\n",
       ":   0.42328329184052843\n",
       ":   0.097052205114540944\n",
       ":   0.79795073918850645\n",
       ":   0.91166115410787846\n",
       ":   0.097052205114540947\n",
       ":   0.42328329184052848\n",
       ":   0.097052205114540949\n",
       ":   0.91166115410787850\n",
       ":   0.60180274477311751\n",
       ":   0.42328329184052852\n",
       ":   0.21928077321083853\n",
       ":   0.79795073918850654\n",
       ":   0.91166115410787855\n",
       ":   0.21928077321083856\n",
       ":   0.097052205114540957\n",
       ":   0.097052205114540958\n",
       ":   0.097052205114540959\n",
       ":   0.097052205114540960\n",
       ":   0.91166115410787861\n",
       ":   0.097052205114540962\n",
       ":   0.21928077321083863\n",
       ":   0.097052205114540964\n",
       ":   0.60180274477311765\n",
       ":   0.42328329184052866\n",
       ":   0.79795073918850667\n",
       ":   0.60180274477311768\n",
       ":   0.42328329184052869\n",
       ":   0.42328329184052870\n",
       ":   0.91166115410787871\n",
       ":   0.60180274477311772\n",
       ":   0.097052205114540973\n",
       ":   0.60180274477311774\n",
       ":   0.42328329184052875\n",
       ":   0.91166115410787876\n",
       ":   0.42328329184052877\n",
       ":   0.097052205114540978\n",
       ":   0.91166115410787879\n",
       ":   0.21928077321083880\n",
       ":   0.60180274477311781\n",
       ":   0.097052205114540982\n",
       ":   0.42328329184052883\n",
       ":   0.42328329184052884\n",
       ":   0.097052205114540985\n",
       ":   0.21928077321083886\n",
       ":   0.097052205114540987\n",
       ":   0.60180274477311788\n",
       ":   0.60180274477311789\n",
       ":   0.60180274477311790\n",
       ":   0.21928077321083891\n",
       ":   0.60180274477311792\n",
       ":   0.097052205114540993\n",
       ":   0.91166115410787894\n",
       ":   0.097052205114540995\n",
       ":   0.42328329184052896\n",
       ":   0.097052205114540997\n",
       ":   0.91166115410787898\n",
       ":   0.097052205114540999\n",
       ":   0.601802744773117100\n",
       ":   0.0970522051145409101\n",
       ":   0.911661154107878102\n",
       ":   0.219280773210838103\n",
       ":   0.0970522051145409104\n",
       ":   0.0970522051145409105\n",
       ":   0.601802744773117106\n",
       ":   0.0970522051145409107\n",
       ":   0.0970522051145409108\n",
       ":   0.0970522051145409109\n",
       ":   0.0970522051145409110\n",
       ":   0.219280773210838111\n",
       ":   0.219280773210838112\n",
       ":   0.601802744773117113\n",
       ":   0.911661154107878114\n",
       ":   0.601802744773117115\n",
       ":   0.911661154107878116\n",
       ":   0.0970522051145409117\n",
       ":   0.0970522051145409118\n",
       ":   0.601802744773117119\n",
       ":   0.423283291840528120\n",
       ":   0.797950739188506121\n",
       ":   0.797950739188506122\n",
       ":   0.0970522051145409123\n",
       ":   0.911661154107878124\n",
       ":   0.0970522051145409125\n",
       ":   0.0970522051145409126\n",
       ":   0.601802744773117127\n",
       ":   0.0970522051145409128\n",
       ":   0.601802744773117129\n",
       ":   0.219280773210838130\n",
       ":   0.0970522051145409131\n",
       ":   0.0970522051145409132\n",
       ":   0.423283291840528133\n",
       ":   0.601802744773117134\n",
       ":   0.0970522051145409135\n",
       ":   0.0970522051145409136\n",
       ":   0.0970522051145409137\n",
       ":   0.0970522051145409138\n",
       ":   0.219280773210838139\n",
       ":   0.601802744773117140\n",
       ":   0.0970522051145409141\n",
       ":   0.601802744773117142\n",
       ":   0.911661154107878143\n",
       ":   0.423283291840528144\n",
       ":   0.219280773210838145\n",
       ":   0.423283291840528146\n",
       ":   0.0970522051145409147\n",
       ":   0.423283291840528148\n",
       ":   0.0970522051145409149\n",
       ":   0.423283291840528150\n",
       ":   0.219280773210838151\n",
       ":   0.911661154107878152\n",
       ":   0.0970522051145409153\n",
       ":   0.0970522051145409154\n",
       ":   0.601802744773117155\n",
       ":   0.0970522051145409156\n",
       ":   0.0970522051145409157\n",
       ":   0.911661154107878158\n",
       ":   0.601802744773117159\n",
       ":   0.423283291840528160\n",
       ":   0.601802744773117161\n",
       ":   0.601802744773117162\n",
       ":   0.0970522051145409163\n",
       ":   0.797950739188506164\n",
       ":   0.0970522051145409165\n",
       ":   0.219280773210838166\n",
       ":   0.601802744773117167\n",
       ":   0.423283291840528168\n",
       ":   0.0970522051145409169\n",
       ":   0.911661154107878170\n",
       ":   0.601802744773117171\n",
       ":   0.0970522051145409172\n",
       ":   0.0970522051145409173\n",
       ":   0.0970522051145409174\n",
       ":   0.0970522051145409175\n",
       ":   0.0970522051145409176\n",
       ":   0.797950739188506177\n",
       ":   0.797950739188506178\n",
       ":   0.423283291840528179\n",
       ":   0.797950739188506180\n",
       ":   0.911661154107878181\n",
       ":   0.219280773210838182\n",
       ":   0.423283291840528183\n",
       ":   0.911661154107878184\n",
       ":   0.0970522051145409185\n",
       ":   0.911661154107878186\n",
       ":   0.219280773210838187\n",
       ":   0.797950739188506188\n",
       ":   0.0970522051145409189\n",
       ":   0.601802744773117190\n",
       ":   0.219280773210838191\n",
       ":   0.219280773210838192\n",
       ":   0.423283291840528193\n",
       ":   0.0970522051145409194\n",
       ":   0.219280773210838195\n",
       ":   0.219280773210838196\n",
       ":   0.0970522051145409197\n",
       ":   0.423283291840528198\n",
       ":   0.601802744773117199\n",
       ":   0.219280773210838200\n",
       ":   0.601802744773117201\n",
       ":   ⋯202\n",
       ":   0.911661154107878203\n",
       ":   0.0970522051145409204\n",
       ":   0.797950739188506205\n",
       ":   0.0970522051145409206\n",
       ":   0.797950739188506207\n",
       ":   0.0970522051145409208\n",
       ":   0.911661154107878209\n",
       ":   0.601802744773117210\n",
       ":   0.0970522051145409211\n",
       ":   0.601802744773117212\n",
       ":   0.0970522051145409213\n",
       ":   0.219280773210838214\n",
       ":   0.219280773210838215\n",
       ":   0.911661154107878216\n",
       ":   0.0970522051145409217\n",
       ":   0.0970522051145409218\n",
       ":   0.423283291840528219\n",
       ":   0.0970522051145409220\n",
       ":   0.423283291840528221\n",
       ":   0.0970522051145409222\n",
       ":   0.797950739188506223\n",
       ":   0.911661154107878224\n",
       ":   0.911661154107878225\n",
       ":   0.797950739188506226\n",
       ":   0.423283291840528227\n",
       ":   0.0970522051145409228\n",
       ":   0.0970522051145409229\n",
       ":   0.423283291840528230\n",
       ":   0.797950739188506231\n",
       ":   0.219280773210838232\n",
       ":   0.797950739188506233\n",
       ":   0.601802744773117234\n",
       ":   0.797950739188506235\n",
       ":   0.0970522051145409236\n",
       ":   0.423283291840528237\n",
       ":   0.0970522051145409238\n",
       ":   0.0970522051145409239\n",
       ":   0.0970522051145409240\n",
       ":   0.0970522051145409241\n",
       ":   0.0970522051145409242\n",
       ":   0.797950739188506243\n",
       ":   0.0970522051145409244\n",
       ":   0.0970522051145409245\n",
       ":   0.0970522051145409246\n",
       ":   0.797950739188506247\n",
       ":   0.601802744773117248\n",
       ":   0.219280773210838249\n",
       ":   0.0970522051145409250\n",
       ":   0.423283291840528251\n",
       ":   0.0970522051145409252\n",
       ":   0.601802744773117253\n",
       ":   0.0970522051145409254\n",
       ":   0.423283291840528255\n",
       ":   0.0970522051145409256\n",
       ":   0.911661154107878257\n",
       ":   0.601802744773117258\n",
       ":   0.0970522051145409259\n",
       ":   0.797950739188506260\n",
       ":   0.219280773210838261\n",
       ":   0.219280773210838262\n",
       ":   0.219280773210838263\n",
       ":   0.219280773210838264\n",
       ":   0.601802744773117265\n",
       ":   0.0970522051145409266\n",
       ":   0.601802744773117267\n",
       ":   0.601802744773117268\n",
       ":   0.601802744773117269\n",
       ":   0.0970522051145409270\n",
       ":   0.0970522051145409271\n",
       ":   0.423283291840528272\n",
       ":   0.0970522051145409273\n",
       ":   0.0970522051145409274\n",
       ":   0.423283291840528275\n",
       ":   0.601802744773117276\n",
       ":   0.0970522051145409277\n",
       ":   0.423283291840528278\n",
       ":   0.0970522051145409279\n",
       ":   0.0970522051145409280\n",
       ":   0.797950739188506281\n",
       ":   0.0970522051145409282\n",
       ":   0.423283291840528283\n",
       ":   0.0970522051145409284\n",
       ":   0.0970522051145409285\n",
       ":   0.219280773210838286\n",
       ":   0.219280773210838287\n",
       ":   0.0970522051145409288\n",
       ":   0.601802744773117289\n",
       ":   0.911661154107878290\n",
       ":   0.423283291840528291\n",
       ":   0.0970522051145409292\n",
       ":   0.423283291840528293\n",
       ":   0.601802744773117294\n",
       ":   0.0970522051145409295\n",
       ":   0.0970522051145409296\n",
       ":   0.0970522051145409297\n",
       ":   0.601802744773117298\n",
       ":   0.911661154107878299\n",
       ":   0.601802744773117300\n",
       ":   0.423283291840528301\n",
       ":   0.219280773210838302\n",
       ":   0.0970522051145409303\n",
       ":   0.219280773210838304\n",
       ":   0.0970522051145409305\n",
       ":   0.0970522051145409306\n",
       ":   0.219280773210838307\n",
       ":   0.423283291840528308\n",
       ":   0.911661154107878309\n",
       ":   0.0970522051145409310\n",
       ":   0.797950739188506311\n",
       ":   0.423283291840528312\n",
       ":   0.219280773210838313\n",
       ":   0.219280773210838314\n",
       ":   0.797950739188506315\n",
       ":   0.423283291840528316\n",
       ":   0.0970522051145409317\n",
       ":   0.601802744773117318\n",
       ":   0.0970522051145409319\n",
       ":   0.423283291840528320\n",
       ":   0.219280773210838321\n",
       ":   0.0970522051145409322\n",
       ":   0.219280773210838323\n",
       ":   0.0970522051145409324\n",
       ":   0.219280773210838325\n",
       ":   0.0970522051145409326\n",
       ":   0.0970522051145409327\n",
       ":   0.911661154107878328\n",
       ":   0.0970522051145409329\n",
       ":   0.601802744773117330\n",
       ":   0.219280773210838331\n",
       ":   0.601802744773117332\n",
       ":   0.219280773210838333\n",
       ":   0.797950739188506334\n",
       ":   0.911661154107878335\n",
       ":   0.219280773210838336\n",
       ":   0.219280773210838337\n",
       ":   0.219280773210838338\n",
       ":   0.601802744773117339\n",
       ":   0.423283291840528340\n",
       ":   0.911661154107878341\n",
       ":   0.0970522051145409342\n",
       ":   0.0970522051145409343\n",
       ":   0.601802744773117344\n",
       ":   0.0970522051145409345\n",
       ":   0.797950739188506346\n",
       ":   0.797950739188506347\n",
       ":   0.0970522051145409348\n",
       ":   0.911661154107878349\n",
       ":   0.601802744773117350\n",
       ":   0.0970522051145409351\n",
       ":   0.601802744773117352\n",
       ":   0.911661154107878353\n",
       ":   0.219280773210838354\n",
       ":   0.219280773210838355\n",
       ":   0.911661154107878356\n",
       ":   0.423283291840528357\n",
       ":   0.219280773210838358\n",
       ":   0.911661154107878359\n",
       ":   0.911661154107878360\n",
       ":   0.601802744773117361\n",
       ":   0.219280773210838362\n",
       ":   0.423283291840528363\n",
       ":   0.0970522051145409364\n",
       ":   0.0970522051145409365\n",
       ":   0.0970522051145409366\n",
       ":   0.601802744773117367\n",
       ":   0.601802744773117368\n",
       ":   0.219280773210838369\n",
       ":   0.797950739188506370\n",
       ":   0.0970522051145409371\n",
       ":   0.219280773210838372\n",
       ":   0.0970522051145409373\n",
       ":   0.0970522051145409374\n",
       ":   0.423283291840528375\n",
       ":   0.911661154107878376\n",
       ":   0.0970522051145409377\n",
       ":   0.219280773210838378\n",
       ":   0.0970522051145409379\n",
       ":   0.911661154107878380\n",
       ":   0.0970522051145409381\n",
       ":   0.911661154107878382\n",
       ":   0.0970522051145409383\n",
       ":   0.0970522051145409384\n",
       ":   0.911661154107878385\n",
       ":   0.219280773210838386\n",
       ":   0.911661154107878387\n",
       ":   0.423283291840528388\n",
       ":   0.423283291840528389\n",
       ":   0.219280773210838390\n",
       ":   0.219280773210838391\n",
       ":   0.423283291840528392\n",
       ":   0.601802744773117393\n",
       ":   0.601802744773117394\n",
       ":   0.601802744773117395\n",
       ":   0.911661154107878396\n",
       ":   0.601802744773117397\n",
       ":   0.0970522051145409398\n",
       ":   0.911661154107878399\n",
       ":   0.0970522051145409400\n",
       ":   0.0970522051145409401\n",
       ":   0.0970522051145409\n",
       "\n"
      ],
      "text/plain": [
       "         1          2          3          4          5          6          7 \n",
       "0.09705221 0.60180274 0.21928077 0.09705221 0.60180274 0.09705221 0.60180274 \n",
       "         8          9         10         11         12         13         14 \n",
       "0.21928077 0.60180274 0.09705221 0.09705221 0.42328329 0.91166115 0.21928077 \n",
       "        15         16         17         18         19         20         21 \n",
       "0.91166115 0.79795074 0.21928077 0.09705221 0.60180274 0.60180274 0.42328329 \n",
       "        22         23         24         25         26         27         28 \n",
       "0.09705221 0.91166115 0.42328329 0.91166115 0.09705221 0.91166115 0.09705221 \n",
       "        29         30         31         32         33         34         35 \n",
       "0.42328329 0.09705221 0.21928077 0.21928077 0.60180274 0.60180274 0.42328329 \n",
       "        36         37         38         39         40         41         42 \n",
       "0.09705221 0.60180274 0.60180274 0.09705221 0.09705221 0.09705221 0.42328329 \n",
       "        43         44         45         46         47         48         49 \n",
       "0.09705221 0.79795074 0.91166115 0.09705221 0.42328329 0.09705221 0.91166115 \n",
       "        50         51         52         53         54         55         56 \n",
       "0.60180274 0.42328329 0.21928077 0.79795074 0.91166115 0.21928077 0.09705221 \n",
       "        57         58         59         60         61         62         63 \n",
       "0.09705221 0.09705221 0.09705221 0.91166115 0.09705221 0.21928077 0.09705221 \n",
       "        64         65         66         67         68         69         70 \n",
       "0.60180274 0.42328329 0.79795074 0.60180274 0.42328329 0.42328329 0.91166115 \n",
       "        71         72         73         74         75         76         77 \n",
       "0.60180274 0.09705221 0.60180274 0.42328329 0.91166115 0.42328329 0.09705221 \n",
       "        78         79         80         81         82         83         84 \n",
       "0.91166115 0.21928077 0.60180274 0.09705221 0.42328329 0.42328329 0.09705221 \n",
       "        85         86         87         88         89         90         91 \n",
       "0.21928077 0.09705221 0.60180274 0.60180274 0.60180274 0.21928077 0.60180274 \n",
       "        92         93         94         95         96         97         98 \n",
       "0.09705221 0.91166115 0.09705221 0.42328329 0.09705221 0.91166115 0.09705221 \n",
       "        99        100        101        102        103        104        105 \n",
       "0.60180274 0.09705221 0.91166115 0.21928077 0.09705221 0.09705221 0.60180274 \n",
       "       106        107        108        109        110        111        112 \n",
       "0.09705221 0.09705221 0.09705221 0.09705221 0.21928077 0.21928077 0.60180274 \n",
       "       113        114        115        116        117        118        119 \n",
       "0.91166115 0.60180274 0.91166115 0.09705221 0.09705221 0.60180274 0.42328329 \n",
       "       120        121        122        123        124        125        126 \n",
       "0.79795074 0.79795074 0.09705221 0.91166115 0.09705221 0.09705221 0.60180274 \n",
       "       127        128        129        130        131        132        133 \n",
       "0.09705221 0.60180274 0.21928077 0.09705221 0.09705221 0.42328329 0.60180274 \n",
       "       134        135        136        137        138        139        140 \n",
       "0.09705221 0.09705221 0.09705221 0.09705221 0.21928077 0.60180274 0.09705221 \n",
       "       141        142        143        144        145        146        147 \n",
       "0.60180274 0.91166115 0.42328329 0.21928077 0.42328329 0.09705221 0.42328329 \n",
       "       148        149        150        151        152        153        154 \n",
       "0.09705221 0.42328329 0.21928077 0.91166115 0.09705221 0.09705221 0.60180274 \n",
       "       155        156        157        158        159        160        161 \n",
       "0.09705221 0.09705221 0.91166115 0.60180274 0.42328329 0.60180274 0.60180274 \n",
       "       162        163        164        165        166        167        168 \n",
       "0.09705221 0.79795074 0.09705221 0.21928077 0.60180274 0.42328329 0.09705221 \n",
       "       169        170        171        172        173        174        175 \n",
       "0.91166115 0.60180274 0.09705221 0.09705221 0.09705221 0.09705221 0.09705221 \n",
       "       176        177        178        179        180        181        182 \n",
       "0.79795074 0.79795074 0.42328329 0.79795074 0.91166115 0.21928077 0.42328329 \n",
       "       183        184        185        186        187        188        189 \n",
       "0.91166115 0.09705221 0.91166115 0.21928077 0.79795074 0.09705221 0.60180274 \n",
       "       190        191        192        193        194        195        196 \n",
       "0.21928077 0.21928077 0.42328329 0.09705221 0.21928077 0.21928077 0.09705221 \n",
       "       197        198        199        200        201        202        203 \n",
       "0.42328329 0.60180274 0.21928077 0.60180274 0.60180274 0.09705221 0.42328329 \n",
       "       204        205        206        207        208        209        210 \n",
       "0.79795074 0.21928077 0.42328329 0.60180274 0.21928077 0.91166115 0.09705221 \n",
       "       211        212        213        214        215        216        217 \n",
       "0.09705221 0.09705221 0.21928077 0.79795074 0.60180274 0.42328329 0.60180274 \n",
       "       218        219        220        221        222        223        224 \n",
       "0.42328329 0.91166115 0.09705221 0.79795074 0.09705221 0.79795074 0.09705221 \n",
       "       225        226        227        228        229        230        231 \n",
       "0.91166115 0.60180274 0.09705221 0.60180274 0.09705221 0.21928077 0.21928077 \n",
       "       232        233        234        235        236        237        238 \n",
       "0.91166115 0.09705221 0.09705221 0.42328329 0.09705221 0.42328329 0.09705221 \n",
       "       239        240        241        242        243        244        245 \n",
       "0.79795074 0.91166115 0.91166115 0.79795074 0.42328329 0.09705221 0.09705221 \n",
       "       246        247        248        249        250        251        252 \n",
       "0.42328329 0.79795074 0.21928077 0.79795074 0.60180274 0.79795074 0.09705221 \n",
       "       253        254        255        256        257        258        259 \n",
       "0.42328329 0.09705221 0.09705221 0.09705221 0.09705221 0.09705221 0.79795074 \n",
       "       260        261        262        263        264        265        266 \n",
       "0.09705221 0.09705221 0.09705221 0.79795074 0.60180274 0.21928077 0.09705221 \n",
       "       267        268        269        270        271        272        273 \n",
       "0.42328329 0.09705221 0.60180274 0.09705221 0.42328329 0.09705221 0.91166115 \n",
       "       274        275        276        277        278        279        280 \n",
       "0.60180274 0.09705221 0.79795074 0.21928077 0.21928077 0.21928077 0.21928077 \n",
       "       281        282        283        284        285        286        287 \n",
       "0.60180274 0.09705221 0.60180274 0.60180274 0.60180274 0.09705221 0.09705221 \n",
       "       288        289        290        291        292        293        294 \n",
       "0.42328329 0.09705221 0.09705221 0.42328329 0.60180274 0.09705221 0.42328329 \n",
       "       295        296        297        298        299        300        301 \n",
       "0.09705221 0.09705221 0.79795074 0.09705221 0.42328329 0.09705221 0.09705221 \n",
       "       302        303        304        305        306        307        308 \n",
       "0.21928077 0.21928077 0.09705221 0.60180274 0.91166115 0.42328329 0.09705221 \n",
       "       309        310        311        312        313        314        315 \n",
       "0.42328329 0.60180274 0.09705221 0.09705221 0.09705221 0.60180274 0.91166115 \n",
       "       316        317        318        319        320        321        322 \n",
       "0.60180274 0.42328329 0.21928077 0.09705221 0.21928077 0.09705221 0.09705221 \n",
       "       323        324        325        326        327        328        329 \n",
       "0.21928077 0.42328329 0.91166115 0.09705221 0.79795074 0.42328329 0.21928077 \n",
       "       330        331        332        333        334        335        336 \n",
       "0.21928077 0.79795074 0.42328329 0.09705221 0.60180274 0.09705221 0.42328329 \n",
       "       337        338        339        340        341        342        343 \n",
       "0.21928077 0.09705221 0.21928077 0.09705221 0.21928077 0.09705221 0.09705221 \n",
       "       344        345        346        347        348        349        350 \n",
       "0.91166115 0.09705221 0.60180274 0.21928077 0.60180274 0.21928077 0.79795074 \n",
       "       351        352        353        354        355        356        357 \n",
       "0.91166115 0.21928077 0.21928077 0.21928077 0.60180274 0.42328329 0.91166115 \n",
       "       358        359        360        361        362        363        364 \n",
       "0.09705221 0.09705221 0.60180274 0.09705221 0.79795074 0.79795074 0.09705221 \n",
       "       365        366        367        368        369        370        371 \n",
       "0.91166115 0.60180274 0.09705221 0.60180274 0.91166115 0.21928077 0.21928077 \n",
       "       372        373        374        375        376        377        378 \n",
       "0.91166115 0.42328329 0.21928077 0.91166115 0.91166115 0.60180274 0.21928077 \n",
       "       379        380        381        382        383        384        385 \n",
       "0.42328329 0.09705221 0.09705221 0.09705221 0.60180274 0.60180274 0.21928077 \n",
       "       386        387        388        389        390        391        392 \n",
       "0.79795074 0.09705221 0.21928077 0.09705221 0.09705221 0.42328329 0.91166115 \n",
       "       393        394        395        396        397        398        399 \n",
       "0.09705221 0.21928077 0.09705221 0.91166115 0.09705221 0.91166115 0.09705221 \n",
       "       400        401        402        403        404        405        406 \n",
       "0.09705221 0.91166115 0.21928077 0.91166115 0.42328329 0.42328329 0.21928077 \n",
       "       407        408        409        410        411        412        413 \n",
       "0.21928077 0.42328329 0.60180274 0.60180274 0.60180274 0.91166115 0.60180274 \n",
       "       414        415        416        417        418 \n",
       "0.09705221 0.91166115 0.09705221 0.09705221 0.09705221 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the trained model for predict the survivor in test set\n",
    "titanic_testing$Survived <- predict(titanic_glm, titanic_testing, type=\"response\") # Question: why type=\"response\"\n",
    "titanic_testing$Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 -- 2 points\n",
    "\n",
    "Train a `random forest` model for predicting  `Survived` based only on  `Pclass`, `Age` and `Sex`, then apply the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                Length Class  Mode     \n",
       "call               6   -none- call     \n",
       "type               1   -none- character\n",
       "predicted        891   factor numeric  \n",
       "err.rate         150   -none- numeric  \n",
       "confusion          6   -none- numeric  \n",
       "votes           1782   matrix numeric  \n",
       "oob.times        891   -none- numeric  \n",
       "classes            2   -none- character\n",
       "importance        12   -none- numeric  \n",
       "importanceSD       9   -none- numeric  \n",
       "localImportance    0   -none- NULL     \n",
       "proximity          0   -none- NULL     \n",
       "ntree              1   -none- numeric  \n",
       "mtry               1   -none- numeric  \n",
       "forest            14   -none- list     \n",
       "y                891   factor numeric  \n",
       "test               0   -none- NULL     \n",
       "inbag              0   -none- NULL     \n",
       "terms              3   terms  call     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library('randomForest')\n",
    "rf_model <- randomForest(Survived ~ Pclass + Age + Sex, data=titanic_training, importance=TRUE, ntree=50, na.action=na.roughfix)\n",
    "summary(rf_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "#### Measures of goodness\n",
    "We trained and applied some models to predict labels for un-labeled data, but how can we say if the model is good?\n",
    "\n",
    "The goodness of prediction models are measured w.r.t some dataset with groudtruth -- i.e., we need the labels for observations in test data. Typical measures for the goodness of the classification models are [Precision, Recall](https://en.wikipedia.org/wiki/Precision_and_recall), and [F1 scores](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we will train a model and evaluate its performance using [Iris dataset](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "iris <- read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', col_names = FALSE)\n",
    "\n",
    "# name the columns\n",
    "names(iris) <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some rows\n",
    "head(iris)\n",
    "\n",
    "# view columns's data type\n",
    "str(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert `class` from string to factor\n",
    "iris$class <- as.factor(iris$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the models, often we **divide** the set of labeled data into **training and test sets**. The models will be **trained on the training set**, and **evaluated using the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the iris dataset into: 80% for training set and the remaining 20% for test set \n",
    "training_size <- floor(0.8 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a svm model to predict `class` from `sepal_length` and `sepal_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and load the 'e1071' package if not yet\n",
    "# install.packages('e1071')\n",
    "library(e1071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris_train[features], y=iris_train$class, kernel =\"linear\", cost=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the trained model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and load the 'mltest' package if not yet\n",
    "# install.packages('mltest', repos = 'https://cran.r-project.org/')\n",
    "library(mltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(svm_model, iris_test[features]))\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall classification accuracy\n",
    "classifier_metrics$accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision for classes\n",
    "classifier_metrics$precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall for classes\n",
    "classifier_metrics$recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-measures for classes\n",
    "classifier_metrics$F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 -- 3 points\n",
    "\n",
    "Divide the iris dataset into training and test sets by ratio 9:1.Then train an SVM model to predict `class` using all features, and examine the performance of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# divide the iris dataset into training and test sets by ratio 90:10\n",
    "\n",
    "# set the features\n",
    "\n",
    "# train a svm model\n",
    "\n",
    "\n",
    "# get the prediction\n",
    "\n",
    "\n",
    "# get the groundtruth\n",
    "\n",
    "\n",
    "# measure the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "[Unsupervised machine learning](https://en.wikipedia.org/wiki/Unsupervised_learning) is the machine learning task of uncovering the hidden structure from \"unlabeled\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Clustering**\n",
    "    \n",
    "    kmeans_model <- kmeans(x=X, centers=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise, we will use movies data on the [MovieLens 100K Dataset](http://files.grouplens.org/datasets/movielens/ml-100k/u.item) collected from the [MovieLens web site](http://movielens.org). It is available in the file `movies.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies <- read_delim('movies.txt', delim = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inspecting and preprocessing\n",
    "\n",
    "# check top records\n",
    "head(movies)\n",
    "\n",
    "# remove duplicates\n",
    "movies <- distinct(movies)\n",
    "\n",
    "# Mow many movies are tagged as Comedy\n",
    "filter(movies, Comedy == 1) %>% count()\n",
    "\n",
    "# How many movies are tagged as Romance and Drama?\n",
    "filter(movies, Romance == 1 & Drama == 1) %>% count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a k-means cluster\n",
    "k = 5\n",
    "iters = 1000\n",
    "set.seed(1)\n",
    "\n",
    "movies <- select(movies, -Title)\n",
    "movie_kmeans <- kmeans(movies, centers = k, iter.max=iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Clustering Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view clustering output\n",
    "str(movie_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster vector, i.e., cluster index of each row\n",
    "movie_kmeans$cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid values\n",
    "movie_kmeans$centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of clusters, i.e., number of movies in each cluster\n",
    "movie_kmeans$size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within-cluster sum of squares\n",
    "movie_kmeans$withinss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determining number of clusters**\n",
    "\n",
    "One way to select the number of clusters is by using a **scree plot**. A standard scree plot has the number of clusters on the x-axis, and the sum of the within-cluster sum of squares on the y-axis. The within-cluster sum of squares for a cluster is the sum, across all points in the cluster, of the squared distance between each point and the centroid of the cluster. To determine the best number of clusters using this plot, we want to look for a bend, or elbow, in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(99)\n",
    "\n",
    "# Call kmeans function with centers = 3, centers = 4, etc\n",
    "num_clusters = seq(5, 15,1)\n",
    "\n",
    "# within-cluster sum of squares for all clusters\n",
    "sum_withinss = sapply(num_clusters, function(x) sum(kmeans(movies, centers=x, iter.max=2000)$withinss))\n",
    "\n",
    "# visualize\n",
    "ggplot(mapping = aes(x=num_clusters, y=sum_withinss)) +\n",
    "    geom_line() +\n",
    "    geom_point()\n",
    "    \n",
    "# 12 seems like a good pick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Top 10 algorithms in data mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)\n",
    "\n",
    "[R for Statistical Learning](https://daviddalpiaz.github.io/r4sl/)\n",
    "\n",
    "[The caret Package](http://topepo.github.io/caret/)\n",
    "\n",
    "[Linear Regression: r-statistics.co](http://r-statistics.co/Linear-Regression.html)\n",
    "\n",
    "[Tutorial: SVM in R](http://math.stanford.edu/~yuany/course/2015.fall/SVM_in_R.pdf)\n",
    "\n",
    "[Artificial Neural Networks in R](https://rpubs.com/julianhatwell/annr)\n",
    "\n",
    "[Cluster analysis in R: determine the optimal number of clusters](https://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
