{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Model Evaluation\n",
    "\n",
    "## Submission Date: 09/01/2022\n",
    "\n",
    "## Points: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> keywords:<font color='black'> \n",
    "    \n",
    "**Tasks:** regression, classification, clustering  \n",
    "**Algorithms:** linear regression, logistic regression, random forest, SVM, k-means  \n",
    "**Evaluation Measures:** RMSE, precision, recall, F1-score  \n",
    "**Concepts:** training and testing, overfitting and underfitting, cross-validation  \n",
    "**R packages used:** tidyverse, rpart, randomForest, e1071, caret, mice, mltest  \n",
    "  \n",
    "  \n",
    "    \n",
    "The goal of this lab session is to get familar with various machine learning based tasks in R. Many packages in R have similar interface that uses a formula and other parameters.\n",
    "\n",
    "**formula:** is used to express the form of a model. For example, suppose you have a response variable y and independent variables x1, x2 and x3. To express that y depends linearly on x1, x2 and x3 you would use the formula `y ~ x1 + x2 + x3.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) is the machine learning task of inferring a function from labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) is the processes to estimate the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "We will use the `lm()` function in the `stats` package which is part of base R. No external package needed.\n",
    "\n",
    "    lm_model <- lm(y ∼ x1 + x2, data=mydata)\n",
    "    summary(lm_model)\n",
    "\n",
    "The vector of coefficients for the model is contained in `lm_model$coefficients.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: We will start with building a simple model using the `cars` dataset that comes with R. The dataset contains the speed of cars and the distances taken to stop. In this example, we will build a linear regression model with only a **single feature**, i.e. to compute `dist` from `speed.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>speed</th><th scope=col>dist</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>4</td><td> 2</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4</td><td>10</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>7</td><td> 4</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>7</td><td>22</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>8</td><td>16</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>9</td><td>10</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & speed & dist\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 4 &  2\\\\\n",
       "\t2 & 4 & 10\\\\\n",
       "\t3 & 7 &  4\\\\\n",
       "\t4 & 7 & 22\\\\\n",
       "\t5 & 8 & 16\\\\\n",
       "\t6 & 9 & 10\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | speed &lt;dbl&gt; | dist &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 4 |  2 |\n",
       "| 2 | 4 | 10 |\n",
       "| 3 | 7 |  4 |\n",
       "| 4 | 7 | 22 |\n",
       "| 5 | 8 | 16 |\n",
       "| 6 | 9 | 10 |\n",
       "\n"
      ],
      "text/plain": [
       "  speed dist\n",
       "1 4      2  \n",
       "2 4     10  \n",
       "3 7      4  \n",
       "4 7     22  \n",
       "5 8     16  \n",
       "6 9     10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view some first rows of the dataset\n",
    "head(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.2 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.4.0      \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.5 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.8      \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.10\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.1      \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.5.0 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.3      \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.2 \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[1m\u001b[22m`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAAQlBMVEUAAAAzMzMzZv89PT1N\nTU1oaGh8fHyMjIyampqnp6eysrK9vb3Hx8fKysrQ0NDW1tbZ2dnh4eHp6enr6+vw8PD///9w\n3A53AAAACXBIWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO3di5IbN5JAUbpFyZa9HNmS+v9/\ndZuP5hOoAhKZQGbhZsRqtLtTOgSqeIOvbu7eGYZhgsxu9A1gGIYpHYLFMEyYIVgMw4QZgsUw\nTJghWAzDhBmCxTBMmCFYDMOEGYLFMEyYaQ3Wrw7TBUFGRvYqEyxkZOQwMsFCRkYOIxMsZGTk\nMDLBQkZGDiMTLGRk5DAywUJGRg4jEyxkZOQwMsFCRkYOIxMsZGTkMDLBQkZGDiMTLGRk5DAy\nwUJGRg4jEyxkZOQwMsFCRkYOIxMsZGTkMDLBQkZGDiMTLGRk5DAywUJGRg4jEyxkZOQwMsFC\nRkYOIxMsZGTkMDLBQkZGDiMTLGRk5DAywUJGRg4j1wVrf/7zY+7/k2AhIyN3kauCdenU5Y/b\n/0KwkJGnl3e7nb1cE6z9O8FCRkZOzm5XXaxOTwkJFjIy8uPsdvXFGhWsP45TcDjDMBudS7A6\nggX/nf39X3iEhYyMfBnfj7AIFjIy8v14fg1r//gHwUJGnl529y7hNU/7p2oRLGRk5B6yJFj3\n2SJYyMjI3WRBsPb7y0fc+aQ7MjJyV7kuWEujv5zEre2BICMje5UJFjIychiZYCEjI4eRCRYy\nMnIYmWAhIyOHkQkWMjJyGJlgISMjh5EJFjIychiZYCEjI4eRCRYyMnIYmWAhIyOHkQkWMjJy\nGJlgISMjh5EJFjIychiZYCEjI4eRCRYyMnIYmWAhIyOHkQkWMjJyGJlgISMjh5EJFjIychiZ\nYCEjI4eRCRYyMnIYmWAhIyOHkQkWMjJyGJlgISMjh5EJFjIychiZYCEjI4eRCRYyMnIYmWAh\nIyOHkQkWMjJyGJlgISMjh5EJFjIy8pA5ECxkZOQg8oFgISMjR5EJFjIychT5QLCQkZGDyAeC\nhYyMHEUmWMjIyFHkA8FCRkYOIh8IFjIychSZYCEjI0eRDwQLGRk5iHwgWMjIyFFkgoWMjBxF\nPhAsZGTkIPKBYCEjI0eRCRYyMnIU+UCwkJGRg8gHgoWMjBxEPhCsLoOMjKwwBKvPICMjt8+B\nYPUZZGTk5jkQrE6DjIzcPASr1yAjI7fOgWD1GmRk5MZ57hXBQkZG9iq/9IpgISMje5UJVsdB\nRkZumtdeESxkZGSfcqJXBAsZGdmnTLC6DjIycsOkekWwkJGRPcrJXhEsZGRkh3K6VwQLGRnZ\noUyweg8yMrJ0Mr0iWMjIyO7kXK8IFjIysjc52yuChYyM7E0mWAMGGRlZNPleESxkZGRf8kKv\nCBYyMrIrealXBAsZGdmVTLDGDDIycv0s9opgISMjO5KXe0WwkJGR/cgrvSJYyMjIfmSC1QNB\nRkbWmLVeESxkZGQv8mqvCBYyMrITeb1XBAsZGdmJTLA2eFKRkTcqF/SKYCEjI7uQS3o1NlgM\nwzDn+VE0TQSPsJCRkVWm6PEVTwmRkZE9yATrcmt7IMjIyE1T2CuChYyMPFwu7RXBQkZGHi0X\n94pgISMjD5bLe0WwkJGRB8sE6+7W9kCQkZHFU9ErgoWMjDxUrukVwUJGRh4pV/WKYCEjIw+U\n63pFsJCRkQfKBOvp1vZAkJGRRVPZK4KFjIw8TK7tFcFCRkYeJVf3imAhIyMPkut7RbCQkZHH\nyIJeESxkZOQxMsFK3doeCDIycu1IekWwkJGRR8iiXhEsZGTkAbIoV29vBAsZGbm7LOzV25vg\nxhIsZGTklhH3ikdYyMjInWVxr3gNCxkZubcs7hXBQkZG7izLe0WwkJGR+8oNvSJYyMjIXeWW\nXhEsZGTknvJ9h758+VLXK4KFjIzcUX7qVVGxbr0iWMjIyP3kl14VFOuuVwQLGRm5m/wQosJg\n3feKYCEjI3eTBcF66BXBQkZG7iU/tai+VwQLGRm5k5zq1UqxnnpFsJCRkfvIzzEqCNZzrwgW\nMjJyF/mlRuvBeukVwUJGRu4hJ3pU3yuChYyM3EFOFqm6VwQLGRnZXl7oUk2vCBYyMrK5rNUr\ngoWMjGwtq/WKYCEjIxvLer0iWMjIyLayYq8IFjIysqksy1W6VwQLGRnZUlbtFcFCRkY2lGW9\nyv4/CRYyMrKZrNwrgoWMjGwma/eKYCEjI1vJ6r0iWMjIyEayfq8IFjIyso1s0CuChYyMbCJb\n9IpgISMjW8gmvSJYyMjIBrJNrwgWMjKyvmzUK4KFjIysLotyVdArgoWMjKwtm/WKYCEjIyvL\nol6V/TcJFjIysqps2CuChYyMrCpb9opgISMja8qmvSJYyMjIirIkV+W9IljIyMh6snGvvux2\n9TeWYCEjI6dG0quK//qXj2BVF4tgISMjp8a8V8dg1RaLYCEjIyfGuFcECxkZWU0W5KqqVwQL\nGdlKFrzYoiR3mKRs3itew0JGNpJ3gocCOnKPScmCXlUecuBdQmRkE3m361AsT2uuLY+sV3wO\nCxnZQp4tWL16RbCQkQ3kyYLVrVcECxnZQp7qNSxBroS9IljIyCbyRO8SduwVwUJGRm6SBb2S\n5opgISMjt8i1xWnrFcFCRkaWy517RbCQkZHFsiBXTb0iWMjIyFK5e68IFjIyslAW9KotVwQL\nGRlZJte2RqNXBAsZGVkiD+kVwUJGRhbIY3pFsJCRketlQa40etUrWPuPuf9PgoWMHFf+MapX\nnYK1v/yxv/4vBAt5YQS/B1dJrp8J5UNlsNRyRbCQPcqCX3UQfs2B5ENlsBR7RbCQ/cmSXyYV\nfc1x5GM2qoKl2athwfrjOKWHM3PNJVijbwaTmh+Vc3r5qvag/DTd9uoX3XmEhVwwPMLyK58f\n55Q/wtJ7uX3sIyyChZwfXsNyKn9mozhYyrkiWMguZd4ldClfs1EYLO2HVwQLGRm5dG7ZKAuW\nQa8IFjIycsncZ6MoWBa94pPuyMjIBfOQjZJgWeSKnyVERkZen6dsrAfL5OHVQbRmgoWMPJf8\nHI7VYJn0SrhmgoWMPJP8mo61YFn0SrxmgoWMPJGciMdysOxyRbCQkZGXJpmPxWAZ9KppzQQL\nGXkWOR2QpWCp96p1zQQLGXkSOdOQfLBsc0WwkJFt5PqfLtKS9SZbkWywtHulsWaChYy8Jgt+\nfltJ1pt8R3LBUu6VzpoJFjLyiiz5DTk6stoslSQTLPNcESxkZAs5frAWW5IMlu7DK701Eyxk\n5BU5erBWapIKlmavVNdMsJCR1+TYr2GtBSURrD69IljIyCZy4HcJ14vyEqxOuSJYyMjIj1PQ\nlOdgKfZKf80ECxl5s3JRVR6D9abXK4s1Eyxk5K3KZV15CFbHXBEsZGTk65SW5S5Yeg+vrNZM\nsJCRtyiXt+UWrL65IljIyMjnqajLNVhavbJcM8FCRt6cXNWXH4NyRbCQkZErc/UZLKVeWa+Z\nYCEjb0uubcyPg9qr7fZrJljIyFuS6yvzQ+vhVY81Eyxk5A3Jgs78UHl41WnNBAsZeTOyKDUq\nD696rZlgISNvRJbnasTDK9maCRYy8jZkea/G5IpgISNPK8tao9CrvmsmWMjI8eWWXK19Vb1V\nrggWMvKcckuvWoLVf80ECxk5uNyQq+PTQXGwRqyZYCEjh5aFtbm9eiUN1pA1Eyxk5MByS64u\nr7bLgjVozQQLGTmsLMzV45uDkmClbkz9N3UQLGTkeeSmXN0+zFAfrOStEXwXGsFCRp5Flubq\n5bNXtcFK3xzJt80SLGTkSWS1XlUGK3d7CFbq1vZAkJHdy3q5qgtW/hYRrNSt7YEgIzuXxblK\n/ihORbCWbhSvYSVubQ8EGdm13Jirlx8dLA7Wyu3iXcLXW9sDQUZ2LMtzlftJ59JgjVvz/SEE\nCxk5jNyaq9RvZigL1rg1Px5CsJCR1WWbp0cWuSoLlmQPdNb8fAjBQkbWlm1egLbpVUmwZLug\nseaXQwgWMrKybPIWv1GuCoIl3ofmNScOIVjIyMqyQbDMcrUarIZ9aFxz8hCChYysLOsHy7BX\nK8Fq2Ye2NacPIVjIyNqy8mtYlrlaDlbTLjStOXcIwUJGVpc13yVsyFXRl0wsBKttExrWnD+E\nYCEjO5abc7X6pTjZYA1b89IhBAsZ2a1sn6tssIatefkQgoWM7FRuydVLr758+VITrFFrXjuE\nYCEju5RVc3XsVaZYqWCNWvP6IQQLGdmhrJurc6/SxXoN1qg1lxxCsJCR3clNuUq9elUTrEFr\nLjuEYCEj+5LbapV+sb08WGPWXHwIwUJG9iQ35ir30avS17CGrLniEIKFjOxHVslV8rMMRe8S\nDllz1SEECxnZi2yXq4W5BWvImisPIVjIyD7k1lzJenUL1og1Vx9CsJCRx8u7XeYJm3WursEa\nsGiChYwcUs6/JG6dq0uwhqyaYCEjB5SXPnRg3qtTsIYsm2AhI8eTD4ufkrLO1TFYAxZ9GoKF\njBxMPjWjLVhNufoIVvc1fw7BQkYOJX9Go6FXb429irXbBAsZeZR8lw1pr5pzFWy3CRYy8hD5\nqRylXxif6pU4V4fOa34agoWMHEN+SYckWBq5CrbbBAsZubecikd9sHRyFWy3CRYycl85nY/a\nYLW+eHX7LEOo3SZYyMgd5Ww/6oKll6tgu02wkJF7yUsFqQlWc64ePioaarcJFjJyH3k5IeXB\n0s1VsN0mWMjIHeTViBQHSzlXwXabYCEjm8sFGSkMVnOuXn9wMNRuEyxkZFu5rCNFwTLIVbDd\nJljIyIZycUkKgqX94pXVmouHYCE7lHe73SC5fpTlipasBqs9V5lfIxNqtwkWsrG821UXK/ya\nT1MXk7VgWeUq2G4TLGRbeberL1b0NR+nNifLwWrPVf639IXabb1gMUxqLsEafTP6zg/dOeeq\n6Z8YvSP6wyMsZAt5vkdYsgdAC4+w7J4Nqq1ZNjwlRPYnz/Ualrgp2WBZPhvUWbN4CBayQ3me\ndwlbopIJlnmugu02wUJGVpGbopIJltlHGZTW3DgECxl5hNxSlMskgqWQq5Jv8Aq12wQLGblN\nbgrKdZ6D9dYpV8F2m2AhIzfITT25n8dgqeSq8AtSA+02wUJGlsttOXmc+2D1zFWc3T4fQrCQ\nkQVyW0xe5xYslVpVfP18hN2+HUKwkJGr5MaSZOYzWDq5quiV891+PoRgISOXya0RWZxzsPrn\nyu1uZw4hWMjIWbm5Q8XzQ+ulq8pcedrtkkMIFvL08kM2Bs0PrVzV9irWeSZYyDPLL9lIN+DL\nly/NHVmeYbkKdp4JFvKUcubung7Wly/GxdKqlSBXwc4zwUKeTl64wyeD9eWLbbEuuWoXZNsR\n6jwTLOSp5JW7/IBgXXPVKki3JNR5zgbr83dE7vcEC3kTcsmdvnew3u5y1SbI9yXUeU4Ha7+7\nG4KFHF4uvdv3fQ3r9kp7q9CyN6HOczpY/9z16h+ChRxarrnj93yX8OGNwXG9inWe08F6vz0l\nLB795SRubQ8EeTty9T2/3+ewnt8YbJAbNynUec4Gq3r0l5O4tT0Q5I3Ikvt+p2AlPnUll1v3\nKdR5zgfrn/37+7+7/d8EC7nz1P8S+NcR3vl7BOstkSu53L7dmfOscRZk8uIhuWD98/Gc8Ofx\nxffSYukvJ3FreyDIg2XB1+w8jTwm9sFK50oqa+x3+jy3nwWpvHxILlhfd/9+/M8//+34WANy\n15F8keHdtOXEOFi5WklllQ1PnufGs9AgrxySC9bHA6z/7b5WvPiuv5zEre2BII+VG+4qrT0x\nDtbizwsKZKUN30aw9ruff+3+O76KRbCQe47wrtIYE3k2CmfhwZVMVtvwbQTr749buz8+wPpO\nsJC7TvU9pTElTdkom7VaCWS9/d7Ga1jv33f7/3080CrtFcFC1pqqe0pTSJqzUTQFuaqVNbd7\nG+8SVo/+chK3tgeCHEdu6kh7NkqmqFaV8qDdVh+ChTyR3JIRjWyUTPnv5SuXx+y2xagFa7d7\n54efkR3LDQ1RykbBlD64Ok7xzxKO2G2rIVjIE8hNEVkZvWDV1Kr8tzX0323L4Skh8sbltois\nj1aw6nJV+hu3eu+29RAs5O3KrQ0pGpVgVdbqUBisrrvdZRSfEvIL/JAdyY0FKZ/2YNXX6lAU\nrI673W0IFvL25LZ+VE5jsN5EuTqsv4bVbbe7jupTwj+//Xx///ntz8JeESxkdbmlHbJpCZa4\nVsdZ7FWf3e4/msH6c/f7/H8uLZb+chK3tgeC7EQWl6NhxMFqqtWy3Ge3R4zyb2s4zm+eEiKP\nkIV3+9aRBOvtrblWC3KX3R40msH6tjs/JeQRFnJ/WX6/L5vsE7C6YL09TtNNysg9druDoSZn\ng/Xz8lVf+58EC7nr2P/ez/xL3CXyW3Jab1NS7rLfoa6wbLDef3//utt9/ft3Ya8IFrLGFGej\nYRY+RJCX05nSSFVW7rTloa6wfLBqR385iVvbA0EeKK9lQ2Uqg2Wdqozcbc9DXWEEC9mPvJQN\nxSkKVp9MpeTTdNz1UFcYwUL2ImfvvOqTfw2rd6Zu87Dmrvse6gojWMg+5Oyd12ISvRqXqvPc\nrbnrvge7wggWsgc5e+e1n6GZus11zR13/TyhrjCChTxezt55bSeRqa6pfJiL3G3PbxPqCiNY\nyKPl7J3XbrKPqIYF6/wktdOOP06oK4xgIQ+WE/feLl9nmnzuNypYp7cB+uz3y4S6wggW8lA5\nefc1y8bqy1SDgnXMVYev1UpPqCuMYCEPlDP3X4tslL2iPipYPb5nOTehrjCChTxMzt5/1bNR\n/PbfmGD1+WL43IS6wggW8iB54Q6sm42azyoMCNZpMwb2KtYVRrCQh8iL92HFbFR+sKp7sD73\nY1yvYl1hBAt5gLxyL1bKhuBjoH2D1Wm3VyaUTLCQu8urd2SNbMg+tN4xWL12e3VCyQQLuVJu\nffJScF9uzob4R2y0glX/JYPuzrNPmWAh18mNLw8X3d3bsiGNVbt8HcG3dnk7z05lgoVcJbe9\nAV94f2/IRkut2uS7Wfle1PLd7jKhZIKFXCW3BMs8G421apAfZzFYNbvdZULJBAu5ShYHq+YO\nL8pGe62k8sssBKtut7tMKJlgIdfJsl7V3eEF2dColUxOTaZX9bvdY0LJBAu5Uq7vVfX9vTYb\nSrUSyLlJ9Uq02x0mlEywkI1lwd29Kht6taqVa6bXbgsmlEywkC1l2b27PBsqL1yJ5KrptNuy\nCSUTLGQzWXz/LsyGdq3K5arptNviCSUTLGQjueEuXveF8Q2QSK6aXrvdMKFkgoVsIjfdy1ez\nYVOrErlquu1204SSCRaygdx4R1/OhlmtVuWq6bfbjRNKJljI6nLzfX0hG5a1WparpuNuN08o\nmWAhK8sKd/dsNmxrtSRXTNfdVphQMsFC1pQV7u+HXDbMa5WVy6fvbutMKJlgIevJKs04JLPR\no1ZpuXh677bWhJIJFrKSrBaN12wYv3C1IBdO/91WnFAywUJWke2y0a9Wz/JtEj8aOHS355UJ\nFnK7bJeNrrU65IJ1+eULhnsd4zw7kAkWcqNsl43etTq8Buu0wi5fc+r+PPuQCRZyk2yWjQG1\nOtwH626NBMuPTLCQ5bJZNsbU6nAK1usyCZYfmWAhS2Wzaoyq1Ues0mvu8UXyfs+zK5lgIYtk\ns2YMq9XSmjt8kbzT8+xNJljI9bJdNcbUqmTN1oNcdogsWPvTHx9DsOaTzbpxfXDV8Qvjn16y\n8rfbyI+HiIJ1CtX+Wi6CNY9sFo77p4K1wVr9YvjMAYVr7jLIZYdIgrV/J1hzypUhKZ/HV64q\ng7X2xfCpAzIvSrnabeTEIYJg7d8J1oxyXUUq5uV19rpgrXwxfOKA/McU/Ow2cvoQjWD9cZzi\nw5mI88Nsri9cSecSrNL/+selfp7RW8o0TfH527/zCGs2ueoRT82kP8Rg9wjrtJqFD4K62G3k\nhUOqg3XtFMGaRK6qR81kP3Jl8xrWbWX5D4KO323k5UPqg3UegjWJXNeOiln4hKjBu4SPS8t+\nEHT0biOvHVIdrOvDLIK1fbkyHOWz/Hl27c9hVa150CCXHUKwkDNj9vHN1Z++UZWr1jzjeQ4l\nNwSLT7pvWrb52vayXxujJlever7zHEwWBisx+stJ3NoeCPLlqaBBsAp/bYySLFj4ZOc5nkyw\nkF9GNxvXKf8lV3w7IHLuEIKF/DC62bhO1a/ka5elq5/nPAeVCRby3Shn43OqatUuN2zAJOc5\nrkywkD9HORufU1urRrltD2Y4z6FlgoV8GuVsXKe+Vk1y6zZs/jxHlwkWcu7zoe3BEtVKLivs\nxLbP8wZkgjW7rJ6N63zWqvrX68lkld3Q2u36XwK/3StMVSZYc8vq2fic24Or6l+vJ5G19kNp\ntwVfs7PVK0xZJlgTy+rZ+Jz7p4LVv16vXlbcEp3dlnyR4SavMH2ZYE0ra2fjOo+vXJkHS3VT\nCJZzmWDNKatn43NeXmc3DpbyvhAs5zLBmlDWz8Zlku8KWr6Gpb41vIblXCZYs8kG2ThP9gOi\nZu8SKu/McXiX0LlMsKaSLbJxGsHH2VtlzX25zkbO83ZlgjWPbJKN4+jWqkhW25Sn2cJ53rRM\nsJzL9c8tkqOYjadneOq1ysvX0diR9Ex4hcWSCZZvWfDq7euoZuPhNXSLWmXlz9HY19zMd4UF\nkwmWa1ny/vjTKGfj7lMKRrXKyZfR2tr0THeFRZMJlmu5NVj62bgGy6xWOfk0inubnOmusGgy\nwXIttwTLJhvnXlnWKicf7HM14RUWTSZYvmVpr+yycZcrDaRcVt/b1Mx3hQWTCZZzWdIry2yY\nPhVckA22NjUTXmGxZIK1Ndk0Gz1qlZJNdjY1cc7zpDLB2pJsnI1OuXqRzfb2dWKc54llgrUZ\n2Tgb3Wr1LBcsvfp5c/aAUE/9lSbAtX13CMHahGycjZ61epRLFl/9zkT+gGhvrmiM92v78RCC\nFV82zobdB0TX5LIdqv7sx8IBAT++0jyur+2XQwhWbNk6G/1r9SkX7xDBahu313byEIIVVzbP\nxpBaneSaHSJYbePy2s4eQrBiyvbVGFWrj8dWlbvNa1hN4+7aXjyEYIWTezRjWK0ya14e3iVs\nGU/X9vohBCuW3CMaY2qVX3PHQXYuE6xIcpdwXGvV/lX1ZbO85r6D7FwmWGHkLvG4f2zVI1gr\na+4+yM5lghVC7pCOw8vHQ62DtbLmIYPsXCZY/mXjblzm9YUrw2CtrnnUIDuXCZZzucsTs/TL\n7Fby+qInPM/IhYcQLMeyZTZuk3tT0EQuWvds5xm5/BCC5VQ2zcZtFj7CoC+Xrn2m84xcdwjB\ncijbZuM2yx+40pVr1j/LeUauP4RgOZNts3E3qx8PVZQr92CG84xMsMLLxtm4n4IPs2vJ9fuw\n9fOMLJYJlhtZKRtP3ySfnIdaZQ9QCZZoKzZ9npFbZILlQ87f4yuz8fBN8ul5fGyVP6A5WOLd\n2O55Rm6UCZYDefFeX5eNu2+ST8/zU8GFA9qC1bIhGz3PyO0ywRosr97xNYOVeOHKJliNm7LB\n84ysIxOsgXLRfV8vWMnX2Q2C1b4xGzvPyHoywRokF9/9lV7Dyr4rqPsals7mbOg8I+vKBKu/\nXJcAjXcJFz/DoPcuodoObeM8IxvIBKuzXBuB9vfqVj8gqiNrbdBxNnCekW1kgtVTriyGIBvP\nI65VnayyO7eJfp6RzWSC1U2uL8ZxSj4HmpuWWh2ywXq+RasL1/uKCPsJfYXNIBOsLrIoGJc6\nCIvVWKtDLliPt6hg7YpfwmU/Ya+wWWSCZS5Le3Gtg6BY7bU6ZIJ1f4uKlq/5Naf2E/IKm0km\nWKZyQy5e8lA8KrU6rAWrdAsIFrKiTLCs5LZYPOehfJRqdVgKlml/CBbywiEjg3W8A9TdWgGi\nM3Vycyse+1D+X9er1SH7Gpb5S1K8hoWcP2R0sA41zYqwtRqpuO9Dea9Ua3VIB+tXjzf9eJcQ\nOXuIg2AdipvlfGuVQrGejcRovXC1JFtv8nWcn2fkcbKTYBXeGxxvrVomVrORGINaPcs9Nvk6\njs8z8ljZT7BK7hZOt1azEsvZSE1rrUp+lrDXNl/G6XlGHi87C9bKncPf1soaUTOLwXprf2y1\n/tsaum70afydZ2QnssNgLdxHfG2tNBF1kw+WQq0Kfh9W963+5e08IzuSnQYrc1fxs7XyQtRO\nJlgqtTqsBWvIXns6z8jOZM/BOk7zApXmTm7qQ/2kgqX4IvtCsFzsNjLywyHOg3WcpgUqzUlu\nroNkXoKl/JZgplfjdxsZOXFIgGB93oHGba3tF8Yvz6Ns8AGGRK9+Dd1tZOT8IVGCdbof9d/a\nT9pFsGw+bvU817WHupCR55BDBavny8Cv8qC5yj1q9bAFoS5k5DnkcMHK378U5vRDbEVyvznL\nfR9bnSfUhYw8hxw7WOk7mnSWfjXC0GCNqNWvYBcy8hzyJoKVu8dV3oKlXz41LlidX7i6vzYk\nO6oyyMi5Q7YTrOZxGKxxtZJdTkqDjJw7hGBdx1uwhtZKdjkpDTJy7hCCdRtHr2Fp/eTNymhf\nTkqDjJw7hGDdTf7Xe3YN1kOt7GSDy0lpkJFzhxAsZ/LzYysjueTaEJxSnUFGzh1CsDzJiSeC\nBnLptSE4pTqDjJw7hGC5kdMvW6nLz2ch+5UPwy5kvoQCOXsIwfIhZ19k15VfT0L+S7VGXch8\nzRdy/hCC5UBeektQU06cg4WvLR10IfNFqsgLh6gFSzA/mI+51crWSZ+DSx76nvjF8XeLGI/D\nI6wBcsHHrXTk7DngEdbDhHq0MaNMsMbJZR8O1ZCXTgKvYd1PqDvvjDLBGiQXf5S9WV47C7xL\neDeh7rwzygRrhFzzczeNsuC0XK+NhmPbBhk5dwjB6izX/pRgiyw4J/fXRtvhyMj6MsHqKtf/\nTHP+xxvXRnBCnq6N5n8BGVlZJlj9ZMlvYFj4BRLLIzgdL9eGwr+BjKwqE6xOsuzXxSz9iq6F\nEZyL1LWh888gI+vJBKuDLHlodR5RsARnIn1taP1DyMhaMsGyluW1OkiCpfiZgFAXMvIcMsEy\nlZtqdZy6Xv1S/dRlqAsZeb/I5YMAAAycSURBVA6ZYNnJrbE6TUWvjluq+XMtoS5k5DlkgmUj\nNz+0qpUvW0qwkDctE6wmOfn4502vVnn5ca5bSrCQNy0TrBb59RWmN91aZeX7edhTXsNC3rJM\nsBrkx/fw9FuVl2/zsqm8S4i8YZlgNch3wbKJVVb+HMGml0+oCxl5DplgNcifwTKLVVY+jWDH\nqybUhYw8h0ywWuRjrixjlZftcxXsQkaeQyZYLbLpQ6slWbDZ9RPqQkaeQyZYUtnuVas1WbDT\nogl1ISPPIRMsiWz0hmCB3C9XwS5k5DlkglUp92zVo3wcwSavjr/f6Y6MnD2EYFXIvWN1k08j\n2OL18fetOcjI+UMIVqk8IFYX+TSC/S0Zf99LiIy8cAjBKpoxsTrOac2CzS0cgoUcSSZY6zPo\nodVlfti+zk6wkCPJBGt5Rrxq9TjWlxOvYSEHkglWfrq/IZiYDpcT7xIix5EJVnJeWjXkyaj0\npCoNMrI7mWC9TPKBVf9gNZxUpUFGdicTrIfJPgvsHKy2k6o0yMjuZIL1OcuvWPUMVvNJVRpk\nZHcywTrO+svr3YKlcVKVBhnZnUywyt4L7BMspZOqNMjI7uSpg1XxuYUewVI7qUqDjOxOnjZY\nlR+yMg+W5klVGmRkd/KUwRJ8ItQ4WLonVWmQkd3JswVL+ul1y2Cpn1SlQUZ2J88UrJYftTEL\nlsVJVRpkZHfyyGDtUt/zvjjybLTEqk1+nMdvii46Q/UbqzTIyO7kgcHa7Z6/590mG62tksuv\nc/9N0aVnqHpjtQYZ2Z08Lli7U7DqilWdDZVYieTk3L4quuIM1W6s2iAju5M3HCy1WFXL2Tn3\nKvPbXHJnqHZj1QYZ2Z28yWC9qbaqRl6ZU65yv34qd4ZqN1ZtkJHdyeOCZfIa1tubQayK5LLJ\n/3rP/Bmq3litQUZ2Jw8Mlu67hM+pUv4doSrB+rXw6z3zZ6h+Y5UGGdmdPDJYap/DMk3Volwx\ngv05nyHpgc2DjOxODh8s81Rl5ZoRbM71DDUc2zbIyO7kwMHqlKqEXDmCnbk/Q22HIyNvSQ4Z\nrA5PATNy/Qi25ekMNf8LyMibkYMFy/il9QVZdJRgT17PkMY/goy8DTlQsAal6jz1wRLsR/oM\naf1DyMjx5SDBGpiq81QGS7AZ2TOk+G8hIweX3QdreKrOUxMswU4snSHdfw4ZObLsOFiJp4BD\nvn/5NMWyYBtWzpD6v4iMHFb2GKzXV9Y/H1k5D5ZgC9Yn1OWEjGwr+wpWPlUV2TCZAlmwASUT\n6nJCRraVnQQrUarU61V+gyVYfeGEupyQkW3l4cEqK1VhNuxmWRasvXhCXU7IyLby0GBVpKok\nG5azIAsWXjOhLidkZFvZSbDas2E8OVmw6soJdTkhI9vKw4Olkg37ScqCJddPqMsJGdlWHhos\nnWx0mVdZsF7RhLqckJFtZYIlkgWLlU6oywkZ2VYmWNWyYKEtE+pyQka2lQlWnSxYZeOEupyQ\nkW1lgnU/+a/xOcmCJbZPqMsJGdlWJlh38yX/RYk/xtTqV7DLCRnZViZYt7l9kfzzBDupyMhb\nlQnWbTLBkm6t0iAjI98OIVjXSQWrYWuVBhkZ+XYIwbrNc6/atlZpkJGRb4cQrLu561X71ioN\nMjLy7RCClRqNrVUaZGTk2yEE62WUtlZpkJGRb4cMDNYu+zHN3PQIltrWKg0yMvLtkHHB2u3y\nHyzPjHmwNLdWaZCRkW+HDAvW7hSsumLZBkt5a5UGGRn5dgjBuoz61ioNMjLy7RCCdRyLrVUa\nZGTk2yHDguXoNSybrVUaZGTk2yHjguXkXcLCfapfntIgIyPfDhkYLAefwyrfJ8HydAYZGfl2\nSEOw9h8TOlg1+yRYns4gIyPfDpEHa3/9I2SwKvdJsDydQUZGvh0yabCq90mwPJ1BRka+HTJj\nsAQ3NdRJRUbeqqwSrD+OU3v4x/wYMYLbyTCMt5njEZbgVp7DLj2weZCRkW+HTBUswW383Cf5\noY2DjIx8O2SiYAlu4W2fWg5uGmRk5Nsh0wRLcPvu96ntcGRkZA15lmAJbt3jPrX+A8jIyO1y\nQ7DifNJdcNNe9knh30BGRm6UW4L1OILb2ydYghuW2CeVfwUZGblJ3nywBDcruU9K/w4yMnKD\nvPFgCW5UZp/U/iVkZGSxvOlgCW5Sdp8U/y1kZGShvOFgCW7Qwj6p/mvIyMgEy6pWv4KdVGTk\nrcrbDJbgtqxMqJOKjLxVeYvBEtyS1Ql1UpGRtypvL1iC21EwoU4qMvJW5a0FS3AriibUSUVG\n3qq8rWAJbkPhhDqpyMhblTcULMENKJ9QJxUZeavyZoIl4Gsm1ElFRt6qvJFgCfC6CXVSkZG3\nKm8iWAK6dkKdVGTkrcobCJYArp9QJxUZeaty+GAJWMmEOqnIyFuVYwdLYAon1ElFRt6qHDpY\nAlI6oU4qMvJW5cDBEoDyCXVSkZG3KocNloBrmVAnFRl5q3LQYAmwtgl1UpGRtyqHDJaAap1Q\nJxUZeatywGAJoPYJdVKRkbcqhwuWgNGYUCcVGXmrcrRgCRSVCXVSkZG3KscKVqitRUZG1pYj\nBSvY1iIjI2vLcYIlXKDSICMjO5CjBEu8QKVBRkZ2IMcIVsMClQYZGdmBHCFYTQtUGmRkZAdy\ngGC1LVBpkJGRHcjug9W6QKVBRkZ2IHsPVvMClQYZGdmB7DtYCgtUGmRkZAey52CpLFBpkJGR\nHciOg6WzQKVBRkZ2ILsNltYClQYZGdmB7DRYegtUGmRkZAeyz2ApLlBpkJGRHcgeg6W6QKVB\nRkZ2IDsMlu4ClQYZGdmB7C5Y2gtUGmRkZAeys2DpL1BpkJGRHci+gmWwQKVBRkZ2IHsKVsGt\nFSA6g4yM7ED2E6yiWytAdAYZGdmB7CZYZbdWgOgMMjKyA9lJsEpvrQDRGWRkZAeyj2AV31oB\nojPIyMgOZA/Bqri1AkRnkJGRHcjjg1V1awWIziAjIzuQhwer7tYKEJ1BRkZ2IA8OVu2tFSA6\ng4yM7EAeGqz6W9sDQUZG9ioTLGRk5DAywUJGRg4jEyxkZOQwMsFCRkYOIxMsZGTkMDLBQkZG\nDiMTLGRk5DAywUJGRg4jEyxkZOQwMsFCRkYOIxMsZGTkMDLBQkZGDiMTLGRk5DAywUJGRg4j\nEyxkZOQwMsFCRkYOIxMsZGTkMDLBQkZGDiMTLGRk5DAywUJGRg4jEyxkZOQwMsFCRkYOIxMs\nZGTkMDLBQkZGDiMTLGRk5DAywUJGRg4j6wWLYRim2/AICxkZ2blMsJCRkcPIesHqMX+MvgED\nhjXPMay5cgiWz2HNcwxrrhyC5XNY8xzDmiuHYPkc1jzHsObKiRAshmGY0xAshmHCDMFiGCbM\nECyGYcIMwWIYJswQLIZhwoz/YO2PM/pGdJ3zauda9m3Nsyz6c6kTLflhzcJFBwjW6BvQey7n\n9PrHDHO5fKdZ7+38znSer2ttWC/B8jb79/mCtX8nWBPMFMGa5Fzez3zBmm65l5ksWKfZty3X\nf7Bmeo5/nnmDNdu5njVYDefZf7Cuf0wz8wbr+sccc312xJqLx32wTjPRCX2f8867T/xt8zNr\nsB7/UjcEy98QrDmG8ywY98Ga7IQeZ9oLea41370xypqLJ0Swpjmf55nwzqvyjnes2d/9xyyL\nvlvzZl90n+qTwOeZ+5Pug29Ir9l/vlXGmqvGf7AYhmEuQ7AYhgkzBIthmDBDsBiGCTMEi2GY\nMEOwGIYJMwSLYZgwQ7AYhgkzBIthmDBDsBgXs+NKZAqGy4RxMQSLKRkuE8bFECymZLhMGLP5\ne7/7+s/7KUZ/7r79PP6ffv+12/31+/FvP7/t/iRYTNFwmTBW8313nH+OwfqI025/rNP++H/6\n+n7/t9/Hv/1JsJiS4TJhrGa3+/n+725//Mu33+/fdt8/HnMd//h+jNjtb993395/fyNYTMlw\nmTBWs9/99b/TX3a7/z6e+B0fTn09XW8fzwAf/vbz+P/lSmQKhsuEsZr/fTzX+3p85eoco+Of\nu8s8/u39nRfdmbLhMmHs5r+vu/2/BIvRGy4TxnL+OSfp9KTv2+cTwePc/42nhEzpcJkwVrPf\n/fv+3+VF9+PL6n8fX2D//v7+f8d03f729/klea5EpmC4TBirOX+s4e9TsI4fa3i/fITh9Br8\n09/4WANTNFwmjNl83+/2H706PiX8tvvr9MHRn3991Ovfp7/9yQdHmcLhMmHMhxgxWsOlxJgP\nwWK0hkuJMR+CxWgNlxJjPgSL0RouJYZhwgzBYhgmzBAshmHCDMFiGCbMECyGYcIMwWIYJswQ\nLIZhwgzBYhgmzPw/ygCpJs2YixAAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now, lets visually examine the data\n",
    "\n",
    "# load the required packages for plotting\n",
    "library(tidyverse)\n",
    "\n",
    "# draw the scatter plot between 'speed' and 'dist'\n",
    "ggplot(data = cars, mapping = aes(x = speed, y = dist)) +\n",
    "    geom_point() +\n",
    "    geom_smooth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above plot suggests that 'dist' can be computed from 'speed' through a linear function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear regression model\n",
    "cars_lm <- lm(dist ~ speed, data = cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the fitted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = dist ~ speed, data = cars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-29.069  -9.525  -2.272   9.215  43.201 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \n",
       "speed         3.9324     0.4155   9.464 1.49e-12 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 15.38 on 48 degrees of freedom\n",
       "Multiple R-squared:  0.6511,\tAdjusted R-squared:  0.6438 \n",
       "F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## summary of fitted model\n",
    "summary(cars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to this [blog](https://boostedml.com/2019/06/linear-regression-in-r-interpreting-summarylm.html) for the intepretation of the summary of the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the fitted model for prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>1</dt><dd>-9.71427737226271</dd><dt>2</dt><dd>-5.78186861313862</dd><dt>3</dt><dd>-1.84945985401454</dd><dt>4</dt><dd>9.94776642335772</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] -9.71427737226271\n",
       "\\item[2] -5.78186861313862\n",
       "\\item[3] -1.84945985401454\n",
       "\\item[4] 9.94776642335772\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   -9.714277372262712\n",
       ":   -5.781868613138623\n",
       ":   -1.849459854014544\n",
       ":   9.94776642335772\n",
       "\n"
      ],
      "text/plain": [
       "        1         2         3         4 \n",
       "-9.714277 -5.781869 -1.849460  9.947766 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example unseen data\n",
    "df <- data.frame('speed' = c(2,3,4,7))\n",
    "# prediction\n",
    "predict(cars_lm, newdata = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: We will use another dataset that comes with R, `mtcars`, to build a model with **multiple features** to predict the fuel consumption `mpg.` The features describe different aspects of an automobile design and performance. We will also explore **which features to use**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 11</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>mpg</th><th scope=col>cyl</th><th scope=col>disp</th><th scope=col>hp</th><th scope=col>drat</th><th scope=col>wt</th><th scope=col>qsec</th><th scope=col>vs</th><th scope=col>am</th><th scope=col>gear</th><th scope=col>carb</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Mazda RX4</th><td>21.0</td><td>6</td><td>160</td><td>110</td><td>3.90</td><td>2.620</td><td>16.46</td><td>0</td><td>1</td><td>4</td><td>4</td></tr>\n",
       "\t<tr><th scope=row>Mazda RX4 Wag</th><td>21.0</td><td>6</td><td>160</td><td>110</td><td>3.90</td><td>2.875</td><td>17.02</td><td>0</td><td>1</td><td>4</td><td>4</td></tr>\n",
       "\t<tr><th scope=row>Datsun 710</th><td>22.8</td><td>4</td><td>108</td><td> 93</td><td>3.85</td><td>2.320</td><td>18.61</td><td>1</td><td>1</td><td>4</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>Hornet 4 Drive</th><td>21.4</td><td>6</td><td>258</td><td>110</td><td>3.08</td><td>3.215</td><td>19.44</td><td>1</td><td>0</td><td>3</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>Hornet Sportabout</th><td>18.7</td><td>8</td><td>360</td><td>175</td><td>3.15</td><td>3.440</td><td>17.02</td><td>0</td><td>0</td><td>3</td><td>2</td></tr>\n",
       "\t<tr><th scope=row>Valiant</th><td>18.1</td><td>6</td><td>225</td><td>105</td><td>2.76</td><td>3.460</td><td>20.22</td><td>1</td><td>0</td><td>3</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 11\n",
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & mpg & cyl & disp & hp & drat & wt & qsec & vs & am & gear & carb\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tMazda RX4 & 21.0 & 6 & 160 & 110 & 3.90 & 2.620 & 16.46 & 0 & 1 & 4 & 4\\\\\n",
       "\tMazda RX4 Wag & 21.0 & 6 & 160 & 110 & 3.90 & 2.875 & 17.02 & 0 & 1 & 4 & 4\\\\\n",
       "\tDatsun 710 & 22.8 & 4 & 108 &  93 & 3.85 & 2.320 & 18.61 & 1 & 1 & 4 & 1\\\\\n",
       "\tHornet 4 Drive & 21.4 & 6 & 258 & 110 & 3.08 & 3.215 & 19.44 & 1 & 0 & 3 & 1\\\\\n",
       "\tHornet Sportabout & 18.7 & 8 & 360 & 175 & 3.15 & 3.440 & 17.02 & 0 & 0 & 3 & 2\\\\\n",
       "\tValiant & 18.1 & 6 & 225 & 105 & 2.76 & 3.460 & 20.22 & 1 & 0 & 3 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 11\n",
       "\n",
       "| <!--/--> | mpg &lt;dbl&gt; | cyl &lt;dbl&gt; | disp &lt;dbl&gt; | hp &lt;dbl&gt; | drat &lt;dbl&gt; | wt &lt;dbl&gt; | qsec &lt;dbl&gt; | vs &lt;dbl&gt; | am &lt;dbl&gt; | gear &lt;dbl&gt; | carb &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Mazda RX4 | 21.0 | 6 | 160 | 110 | 3.90 | 2.620 | 16.46 | 0 | 1 | 4 | 4 |\n",
       "| Mazda RX4 Wag | 21.0 | 6 | 160 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 |\n",
       "| Datsun 710 | 22.8 | 4 | 108 |  93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 |\n",
       "| Hornet 4 Drive | 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 |\n",
       "| Hornet Sportabout | 18.7 | 8 | 360 | 175 | 3.15 | 3.440 | 17.02 | 0 | 0 | 3 | 2 |\n",
       "| Valiant | 18.1 | 6 | 225 | 105 | 2.76 | 3.460 | 20.22 | 1 | 0 | 3 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "                  mpg  cyl disp hp  drat wt    qsec  vs am gear carb\n",
       "Mazda RX4         21.0 6   160  110 3.90 2.620 16.46 0  1  4    4   \n",
       "Mazda RX4 Wag     21.0 6   160  110 3.90 2.875 17.02 0  1  4    4   \n",
       "Datsun 710        22.8 4   108   93 3.85 2.320 18.61 1  1  4    1   \n",
       "Hornet 4 Drive    21.4 6   258  110 3.08 3.215 19.44 1  0  3    1   \n",
       "Hornet Sportabout 18.7 8   360  175 3.15 3.440 17.02 0  0  3    2   \n",
       "Valiant           18.1 6   225  105 2.76 3.460 20.22 1  0  3    1   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + qsec + am + carb, data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.1184 -1.5414 -0.1392  1.2917  4.3604 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  12.8972     7.4725   1.726 0.095784 .  \n",
       "wt           -3.4343     0.8200  -4.188 0.000269 ***\n",
       "qsec          1.0191     0.3378   3.017 0.005507 ** \n",
       "am            3.5114     1.4875   2.361 0.025721 *  \n",
       "carb         -0.4886     0.4212  -1.160 0.256212    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.444 on 27 degrees of freedom\n",
       "Multiple R-squared:  0.8568,\tAdjusted R-squared:  0.8356 \n",
       "F-statistic: 40.39 on 4 and 27 DF,  p-value: 5.064e-11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the dataset mtcars to create a linear regression model to predict `mpg` using `wt, qsec, am` and `carb`. \n",
    "mtcars_lm <- lm(mpg ~ wt + qsec + am + carb, data = mtcars)\n",
    "summary(mtcars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "[Classification](https://en.wikipedia.org/wiki/Statistical_classification) is the problem of determining label(s) (e.g., categories or clasesses, etc.) of each data observation based on learning from labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widely used (shallow) classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Logistic Regression**](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "Provided in `stats` package, which is automatically loaded when starting R  \n",
    "\n",
    "    glm_model <- glm(y ∼ x1 + x2, family = binomial, data = mydata)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**K-Nearest Neighbor**](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "\n",
    "Install and load the `class` package\n",
    "\n",
    "    knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\n",
    "    \n",
    "`knn_model` is a factor vector of class attributes for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Decision Trees (CART)**](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "\n",
    "Install and load the `rpart` package.\n",
    "\n",
    "    cart_model <- rpart(y ∼ x1 + x2, data=mydata, method=\"class\")\n",
    " \n",
    "You can use `plot.rpart` and `text.rpart` to plot the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Random Forest**](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "Install and load the `randomForest` package\n",
    "\n",
    "\n",
    "    rf_model <- randomForest(y ~ x1 + x2, data=train, importance=TRUE, ntree=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Support Vector Machines (SVM)**](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "Install and load the `e1071` package.\n",
    "\n",
    "    svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we will use the above models to predict `survivors` in the [Titanic dataset](https://www.kaggle.com/c/titanic). The dataset also provided under `data/titanic.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m1309\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m12\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (5): Name, Sex, Ticket, Cabin, Embarked\n",
      "\u001b[32mdbl\u001b[39m (7): PassengerId, Pclass, Age, SibSp, Parch, Fare, Survived\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 12</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>PassengerId</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th><th scope=col>Survived</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>3</td><td>Braund, Mr. Owen Harris                            </td><td>male  </td><td>22</td><td>1</td><td>0</td><td>A/5 21171       </td><td> 7.2500</td><td>NA  </td><td>S</td><td>0</td></tr>\n",
       "\t<tr><td>2</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599        </td><td>71.2833</td><td>C85 </td><td>C</td><td>1</td></tr>\n",
       "\t<tr><td>3</td><td>3</td><td>Heikkinen, Miss. Laina                             </td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td> 7.9250</td><td>NA  </td><td>S</td><td>1</td></tr>\n",
       "\t<tr><td>4</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)       </td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803          </td><td>53.1000</td><td>C123</td><td>S</td><td>1</td></tr>\n",
       "\t<tr><td>5</td><td>3</td><td>Allen, Mr. William Henry                           </td><td>male  </td><td>35</td><td>0</td><td>0</td><td>373450          </td><td> 8.0500</td><td>NA  </td><td>S</td><td>0</td></tr>\n",
       "\t<tr><td>6</td><td>3</td><td>Moran, Mr. James                                   </td><td>male  </td><td>NA</td><td>0</td><td>0</td><td>330877          </td><td> 8.4583</td><td>NA  </td><td>Q</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 12\n",
       "\\begin{tabular}{llllllllllll}\n",
       " PassengerId & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked & Survived\\\\\n",
       " <dbl> & <dbl> & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <chr> & <dbl> & <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 3 & Braund, Mr. Owen Harris                             & male   & 22 & 1 & 0 & A/5 21171        &  7.2500 & NA   & S & 0\\\\\n",
       "\t 2 & 1 & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female & 38 & 1 & 0 & PC 17599         & 71.2833 & C85  & C & 1\\\\\n",
       "\t 3 & 3 & Heikkinen, Miss. Laina                              & female & 26 & 0 & 0 & STON/O2. 3101282 &  7.9250 & NA   & S & 1\\\\\n",
       "\t 4 & 1 & Futrelle, Mrs. Jacques Heath (Lily May Peel)        & female & 35 & 1 & 0 & 113803           & 53.1000 & C123 & S & 1\\\\\n",
       "\t 5 & 3 & Allen, Mr. William Henry                            & male   & 35 & 0 & 0 & 373450           &  8.0500 & NA   & S & 0\\\\\n",
       "\t 6 & 3 & Moran, Mr. James                                    & male   & NA & 0 & 0 & 330877           &  8.4583 & NA   & Q & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 12\n",
       "\n",
       "| PassengerId &lt;dbl&gt; | Pclass &lt;dbl&gt; | Name &lt;chr&gt; | Sex &lt;chr&gt; | Age &lt;dbl&gt; | SibSp &lt;dbl&gt; | Parch &lt;dbl&gt; | Ticket &lt;chr&gt; | Fare &lt;dbl&gt; | Cabin &lt;chr&gt; | Embarked &lt;chr&gt; | Survived &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 3 | Braund, Mr. Owen Harris                             | male   | 22 | 1 | 0 | A/5 21171        |  7.2500 | NA   | S | 0 |\n",
       "| 2 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599         | 71.2833 | C85  | C | 1 |\n",
       "| 3 | 3 | Heikkinen, Miss. Laina                              | female | 26 | 0 | 0 | STON/O2. 3101282 |  7.9250 | NA   | S | 1 |\n",
       "| 4 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female | 35 | 1 | 0 | 113803           | 53.1000 | C123 | S | 1 |\n",
       "| 5 | 3 | Allen, Mr. William Henry                            | male   | 35 | 0 | 0 | 373450           |  8.0500 | NA   | S | 0 |\n",
       "| 6 | 3 | Moran, Mr. James                                    | male   | NA | 0 | 0 | 330877           |  8.4583 | NA   | Q | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  PassengerId Pclass Name                                                Sex   \n",
       "1 1           3      Braund, Mr. Owen Harris                             male  \n",
       "2 2           1      Cumings, Mrs. John Bradley (Florence Briggs Thayer) female\n",
       "3 3           3      Heikkinen, Miss. Laina                              female\n",
       "4 4           1      Futrelle, Mrs. Jacques Heath (Lily May Peel)        female\n",
       "5 5           3      Allen, Mr. William Henry                            male  \n",
       "6 6           3      Moran, Mr. James                                    male  \n",
       "  Age SibSp Parch Ticket           Fare    Cabin Embarked Survived\n",
       "1 22  1     0     A/5 21171         7.2500 NA    S        0       \n",
       "2 38  1     0     PC 17599         71.2833 C85   C        1       \n",
       "3 26  0     0     STON/O2. 3101282  7.9250 NA    S        1       \n",
       "4 35  1     0     113803           53.1000 C123  S        1       \n",
       "5 35  0     0     373450            8.0500 NA    S        0       \n",
       "6 NA  0     0     330877            8.4583 NA    Q        0       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  PassengerId       Pclass          Name               Sex           \n",
       " Min.   :   1   Min.   :1.000   Length:1309        Length:1309       \n",
       " 1st Qu.: 328   1st Qu.:2.000   Class :character   Class :character  \n",
       " Median : 655   Median :3.000   Mode  :character   Mode  :character  \n",
       " Mean   : 655   Mean   :2.295                                        \n",
       " 3rd Qu.: 982   3rd Qu.:3.000                                        \n",
       " Max.   :1309   Max.   :3.000                                        \n",
       "                                                                     \n",
       "      Age            SibSp            Parch          Ticket         \n",
       " Min.   : 0.17   Min.   :0.0000   Min.   :0.000   Length:1309       \n",
       " 1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   Class :character  \n",
       " Median :28.00   Median :0.0000   Median :0.000   Mode  :character  \n",
       " Mean   :29.88   Mean   :0.4989   Mean   :0.385                     \n",
       " 3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000                     \n",
       " Max.   :80.00   Max.   :8.0000   Max.   :9.000                     \n",
       " NA's   :263                                                        \n",
       "      Fare            Cabin             Embarked            Survived     \n",
       " Min.   :  0.000   Length:1309        Length:1309        Min.   :0.0000  \n",
       " 1st Qu.:  7.896   Class :character   Class :character   1st Qu.:0.0000  \n",
       " Median : 14.454   Mode  :character   Mode  :character   Median :0.0000  \n",
       " Mean   : 33.295                                         Mean   :0.3838  \n",
       " 3rd Qu.: 31.275                                         3rd Qu.:1.0000  \n",
       " Max.   :512.329                                         Max.   :1.0000  \n",
       " NA's   :1                                               NA's   :418     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load, view data examples, and summarize the dataset\n",
    "titanic <- read_csv('titanic.csv', skip=5)\n",
    "head(titanic)\n",
    "summary(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: most models do not work with `character` type, we need to convert strings to factors for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic$Sex <- as.factor(titanic$Sex)\n",
    "titanic$Cabin <- as.factor(titanic$Cabin)\n",
    "titanic$Embarked <- as.factor(titanic$Embarked)\n",
    "titanic$Survived <- as.factor(titanic$Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Testing data**\n",
    "\n",
    "split the titanic data into training and testing sets based on the feature we want to predict `Survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>891</li><li>12</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 891\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 891\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 891  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>418</li><li>12</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 418\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 418\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 418  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_training <- filter(titanic, !is.na(Survived))\n",
    "dim(titanic_training)\n",
    "titanic_testing <- filter(titanic, is.na(Survived))\n",
    "dim(titanic_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  PassengerId        Pclass          Name               Sex     \n",
       " Min.   :  1.0   Min.   :1.000   Length:891         female:314  \n",
       " 1st Qu.:223.5   1st Qu.:2.000   Class :character   male  :577  \n",
       " Median :446.0   Median :3.000   Mode  :character               \n",
       " Mean   :446.0   Mean   :2.309                                  \n",
       " 3rd Qu.:668.5   3rd Qu.:3.000                                  \n",
       " Max.   :891.0   Max.   :3.000                                  \n",
       "                                                                \n",
       "      Age            SibSp           Parch           Ticket         \n",
       " Min.   : 0.42   Min.   :0.000   Min.   :0.0000   Length:891        \n",
       " 1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000   Class :character  \n",
       " Median :28.00   Median :0.000   Median :0.0000   Mode  :character  \n",
       " Mean   :29.70   Mean   :0.523   Mean   :0.3816                     \n",
       " 3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000                     \n",
       " Max.   :80.00   Max.   :8.000   Max.   :6.0000                     \n",
       " NA's   :177                                                        \n",
       "      Fare                Cabin     Embarked   Survived\n",
       " Min.   :  0.00   B96 B98    :  4   C   :168   0:549   \n",
       " 1st Qu.:  7.91   C23 C25 C27:  4   Q   : 77   1:342   \n",
       " Median : 14.45   G6         :  4   S   :644           \n",
       " Mean   : 32.20   C22 C26    :  3   NA's:  2           \n",
       " 3rd Qu.: 31.00   D          :  3                      \n",
       " Max.   :512.33   (Other)    :186                      \n",
       "                  NA's       :687                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if there is missing value in training data\n",
    "summary(titanic_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  PassengerId         Pclass          Name               Sex     \n",
       " Min.   : 892.0   Min.   :1.000   Length:418         female:152  \n",
       " 1st Qu.: 996.2   1st Qu.:1.000   Class :character   male  :266  \n",
       " Median :1100.5   Median :3.000   Mode  :character               \n",
       " Mean   :1100.5   Mean   :2.266                                  \n",
       " 3rd Qu.:1204.8   3rd Qu.:3.000                                  \n",
       " Max.   :1309.0   Max.   :3.000                                  \n",
       "                                                                 \n",
       "      Age            SibSp            Parch           Ticket         \n",
       " Min.   : 0.17   Min.   :0.0000   Min.   :0.0000   Length:418        \n",
       " 1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.0000   Class :character  \n",
       " Median :27.00   Median :0.0000   Median :0.0000   Mode  :character  \n",
       " Mean   :30.27   Mean   :0.4474   Mean   :0.3923                     \n",
       " 3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.0000                     \n",
       " Max.   :76.00   Max.   :8.0000   Max.   :9.0000                     \n",
       " NA's   :86                                                          \n",
       "      Fare                     Cabin     Embarked Survived  \n",
       " Min.   :  0.000   B57 B59 B63 B66:  3   C:102    0   :  0  \n",
       " 1st Qu.:  7.896   A34            :  2   Q: 46    1   :  0  \n",
       " Median : 14.454   B45            :  2   S:270    NA's:418  \n",
       " Mean   : 35.627   C101           :  2                      \n",
       " 3rd Qu.: 31.500   C116           :  2                      \n",
       " Max.   :512.329   (Other)        : 80                      \n",
       " NA's   :1         NA's           :327                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if there is missing value in test data\n",
    "summary(titanic_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting `Survived` based on `Pclass` and `Sex` using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Survived ~ Pclass + Sex, family = binomial, data = titanic_training)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.2030  -0.7036  -0.4519   0.6719   2.1599  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)   3.2946     0.2974  11.077   <2e-16 ***\n",
       "Pclass       -0.9606     0.1061  -9.057   <2e-16 ***\n",
       "Sexmale      -2.6434     0.1838 -14.380   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 1186.7  on 890  degrees of freedom\n",
       "Residual deviance:  827.2  on 888  degrees of freedom\n",
       "AIC: 833.2\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "titanic_glm <- glm(Survived ~ Pclass + Sex, data = titanic_training, family = binomial)\n",
    "\n",
    "# examine the model\n",
    "summary(titanic_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>1</dt><dd>0.0970522051145409</dd><dt>2</dt><dd>0.601802744773117</dd><dt>3</dt><dd>0.219280773210838</dd><dt>4</dt><dd>0.0970522051145409</dd><dt>5</dt><dd>0.601802744773117</dd><dt>6</dt><dd>0.0970522051145409</dd><dt>7</dt><dd>0.601802744773117</dd><dt>8</dt><dd>0.219280773210838</dd><dt>9</dt><dd>0.601802744773117</dd><dt>10</dt><dd>0.0970522051145409</dd><dt>11</dt><dd>0.0970522051145409</dd><dt>12</dt><dd>0.423283291840528</dd><dt>13</dt><dd>0.911661154107878</dd><dt>14</dt><dd>0.219280773210838</dd><dt>15</dt><dd>0.911661154107878</dd><dt>16</dt><dd>0.797950739188506</dd><dt>17</dt><dd>0.219280773210838</dd><dt>18</dt><dd>0.0970522051145409</dd><dt>19</dt><dd>0.601802744773117</dd><dt>20</dt><dd>0.601802744773117</dd><dt>21</dt><dd>0.423283291840528</dd><dt>22</dt><dd>0.0970522051145409</dd><dt>23</dt><dd>0.911661154107878</dd><dt>24</dt><dd>0.423283291840528</dd><dt>25</dt><dd>0.911661154107878</dd><dt>26</dt><dd>0.0970522051145409</dd><dt>27</dt><dd>0.911661154107878</dd><dt>28</dt><dd>0.0970522051145409</dd><dt>29</dt><dd>0.423283291840528</dd><dt>30</dt><dd>0.0970522051145409</dd><dt>31</dt><dd>0.219280773210838</dd><dt>32</dt><dd>0.219280773210838</dd><dt>33</dt><dd>0.601802744773117</dd><dt>34</dt><dd>0.601802744773117</dd><dt>35</dt><dd>0.423283291840528</dd><dt>36</dt><dd>0.0970522051145409</dd><dt>37</dt><dd>0.601802744773117</dd><dt>38</dt><dd>0.601802744773117</dd><dt>39</dt><dd>0.0970522051145409</dd><dt>40</dt><dd>0.0970522051145409</dd><dt>41</dt><dd>0.0970522051145409</dd><dt>42</dt><dd>0.423283291840528</dd><dt>43</dt><dd>0.0970522051145409</dd><dt>44</dt><dd>0.797950739188506</dd><dt>45</dt><dd>0.911661154107878</dd><dt>46</dt><dd>0.0970522051145409</dd><dt>47</dt><dd>0.423283291840528</dd><dt>48</dt><dd>0.0970522051145409</dd><dt>49</dt><dd>0.911661154107878</dd><dt>50</dt><dd>0.601802744773117</dd><dt>51</dt><dd>0.423283291840528</dd><dt>52</dt><dd>0.219280773210838</dd><dt>53</dt><dd>0.797950739188506</dd><dt>54</dt><dd>0.911661154107878</dd><dt>55</dt><dd>0.219280773210838</dd><dt>56</dt><dd>0.0970522051145409</dd><dt>57</dt><dd>0.0970522051145409</dd><dt>58</dt><dd>0.0970522051145409</dd><dt>59</dt><dd>0.0970522051145409</dd><dt>60</dt><dd>0.911661154107878</dd><dt>61</dt><dd>0.0970522051145409</dd><dt>62</dt><dd>0.219280773210838</dd><dt>63</dt><dd>0.0970522051145409</dd><dt>64</dt><dd>0.601802744773117</dd><dt>65</dt><dd>0.423283291840528</dd><dt>66</dt><dd>0.797950739188506</dd><dt>67</dt><dd>0.601802744773117</dd><dt>68</dt><dd>0.423283291840528</dd><dt>69</dt><dd>0.423283291840528</dd><dt>70</dt><dd>0.911661154107878</dd><dt>71</dt><dd>0.601802744773117</dd><dt>72</dt><dd>0.0970522051145409</dd><dt>73</dt><dd>0.601802744773117</dd><dt>74</dt><dd>0.423283291840528</dd><dt>75</dt><dd>0.911661154107878</dd><dt>76</dt><dd>0.423283291840528</dd><dt>77</dt><dd>0.0970522051145409</dd><dt>78</dt><dd>0.911661154107878</dd><dt>79</dt><dd>0.219280773210838</dd><dt>80</dt><dd>0.601802744773117</dd><dt>81</dt><dd>0.0970522051145409</dd><dt>82</dt><dd>0.423283291840528</dd><dt>83</dt><dd>0.423283291840528</dd><dt>84</dt><dd>0.0970522051145409</dd><dt>85</dt><dd>0.219280773210838</dd><dt>86</dt><dd>0.0970522051145409</dd><dt>87</dt><dd>0.601802744773117</dd><dt>88</dt><dd>0.601802744773117</dd><dt>89</dt><dd>0.601802744773117</dd><dt>90</dt><dd>0.219280773210838</dd><dt>91</dt><dd>0.601802744773117</dd><dt>92</dt><dd>0.0970522051145409</dd><dt>93</dt><dd>0.911661154107878</dd><dt>94</dt><dd>0.0970522051145409</dd><dt>95</dt><dd>0.423283291840528</dd><dt>96</dt><dd>0.0970522051145409</dd><dt>97</dt><dd>0.911661154107878</dd><dt>98</dt><dd>0.0970522051145409</dd><dt>99</dt><dd>0.601802744773117</dd><dt>100</dt><dd>0.0970522051145409</dd><dt>101</dt><dd>0.911661154107878</dd><dt>102</dt><dd>0.219280773210838</dd><dt>103</dt><dd>0.0970522051145409</dd><dt>104</dt><dd>0.0970522051145409</dd><dt>105</dt><dd>0.601802744773117</dd><dt>106</dt><dd>0.0970522051145409</dd><dt>107</dt><dd>0.0970522051145409</dd><dt>108</dt><dd>0.0970522051145409</dd><dt>109</dt><dd>0.0970522051145409</dd><dt>110</dt><dd>0.219280773210838</dd><dt>111</dt><dd>0.219280773210838</dd><dt>112</dt><dd>0.601802744773117</dd><dt>113</dt><dd>0.911661154107878</dd><dt>114</dt><dd>0.601802744773117</dd><dt>115</dt><dd>0.911661154107878</dd><dt>116</dt><dd>0.0970522051145409</dd><dt>117</dt><dd>0.0970522051145409</dd><dt>118</dt><dd>0.601802744773117</dd><dt>119</dt><dd>0.423283291840528</dd><dt>120</dt><dd>0.797950739188506</dd><dt>121</dt><dd>0.797950739188506</dd><dt>122</dt><dd>0.0970522051145409</dd><dt>123</dt><dd>0.911661154107878</dd><dt>124</dt><dd>0.0970522051145409</dd><dt>125</dt><dd>0.0970522051145409</dd><dt>126</dt><dd>0.601802744773117</dd><dt>127</dt><dd>0.0970522051145409</dd><dt>128</dt><dd>0.601802744773117</dd><dt>129</dt><dd>0.219280773210838</dd><dt>130</dt><dd>0.0970522051145409</dd><dt>131</dt><dd>0.0970522051145409</dd><dt>132</dt><dd>0.423283291840528</dd><dt>133</dt><dd>0.601802744773117</dd><dt>134</dt><dd>0.0970522051145409</dd><dt>135</dt><dd>0.0970522051145409</dd><dt>136</dt><dd>0.0970522051145409</dd><dt>137</dt><dd>0.0970522051145409</dd><dt>138</dt><dd>0.219280773210838</dd><dt>139</dt><dd>0.601802744773117</dd><dt>140</dt><dd>0.0970522051145409</dd><dt>141</dt><dd>0.601802744773117</dd><dt>142</dt><dd>0.911661154107878</dd><dt>143</dt><dd>0.423283291840528</dd><dt>144</dt><dd>0.219280773210838</dd><dt>145</dt><dd>0.423283291840528</dd><dt>146</dt><dd>0.0970522051145409</dd><dt>147</dt><dd>0.423283291840528</dd><dt>148</dt><dd>0.0970522051145409</dd><dt>149</dt><dd>0.423283291840528</dd><dt>150</dt><dd>0.219280773210838</dd><dt>151</dt><dd>0.911661154107878</dd><dt>152</dt><dd>0.0970522051145409</dd><dt>153</dt><dd>0.0970522051145409</dd><dt>154</dt><dd>0.601802744773117</dd><dt>155</dt><dd>0.0970522051145409</dd><dt>156</dt><dd>0.0970522051145409</dd><dt>157</dt><dd>0.911661154107878</dd><dt>158</dt><dd>0.601802744773117</dd><dt>159</dt><dd>0.423283291840528</dd><dt>160</dt><dd>0.601802744773117</dd><dt>161</dt><dd>0.601802744773117</dd><dt>162</dt><dd>0.0970522051145409</dd><dt>163</dt><dd>0.797950739188506</dd><dt>164</dt><dd>0.0970522051145409</dd><dt>165</dt><dd>0.219280773210838</dd><dt>166</dt><dd>0.601802744773117</dd><dt>167</dt><dd>0.423283291840528</dd><dt>168</dt><dd>0.0970522051145409</dd><dt>169</dt><dd>0.911661154107878</dd><dt>170</dt><dd>0.601802744773117</dd><dt>171</dt><dd>0.0970522051145409</dd><dt>172</dt><dd>0.0970522051145409</dd><dt>173</dt><dd>0.0970522051145409</dd><dt>174</dt><dd>0.0970522051145409</dd><dt>175</dt><dd>0.0970522051145409</dd><dt>176</dt><dd>0.797950739188506</dd><dt>177</dt><dd>0.797950739188506</dd><dt>178</dt><dd>0.423283291840528</dd><dt>179</dt><dd>0.797950739188506</dd><dt>180</dt><dd>0.911661154107878</dd><dt>181</dt><dd>0.219280773210838</dd><dt>182</dt><dd>0.423283291840528</dd><dt>183</dt><dd>0.911661154107878</dd><dt>184</dt><dd>0.0970522051145409</dd><dt>185</dt><dd>0.911661154107878</dd><dt>186</dt><dd>0.219280773210838</dd><dt>187</dt><dd>0.797950739188506</dd><dt>188</dt><dd>0.0970522051145409</dd><dt>189</dt><dd>0.601802744773117</dd><dt>190</dt><dd>0.219280773210838</dd><dt>191</dt><dd>0.219280773210838</dd><dt>192</dt><dd>0.423283291840528</dd><dt>193</dt><dd>0.0970522051145409</dd><dt>194</dt><dd>0.219280773210838</dd><dt>195</dt><dd>0.219280773210838</dd><dt>196</dt><dd>0.0970522051145409</dd><dt>197</dt><dd>0.423283291840528</dd><dt>198</dt><dd>0.601802744773117</dd><dt>199</dt><dd>0.219280773210838</dd><dt>200</dt><dd>0.601802744773117</dd><dt>201</dt><dd>⋯</dd><dt>202</dt><dd>0.911661154107878</dd><dt>203</dt><dd>0.0970522051145409</dd><dt>204</dt><dd>0.797950739188506</dd><dt>205</dt><dd>0.0970522051145409</dd><dt>206</dt><dd>0.797950739188506</dd><dt>207</dt><dd>0.0970522051145409</dd><dt>208</dt><dd>0.911661154107878</dd><dt>209</dt><dd>0.601802744773117</dd><dt>210</dt><dd>0.0970522051145409</dd><dt>211</dt><dd>0.601802744773117</dd><dt>212</dt><dd>0.0970522051145409</dd><dt>213</dt><dd>0.219280773210838</dd><dt>214</dt><dd>0.219280773210838</dd><dt>215</dt><dd>0.911661154107878</dd><dt>216</dt><dd>0.0970522051145409</dd><dt>217</dt><dd>0.0970522051145409</dd><dt>218</dt><dd>0.423283291840528</dd><dt>219</dt><dd>0.0970522051145409</dd><dt>220</dt><dd>0.423283291840528</dd><dt>221</dt><dd>0.0970522051145409</dd><dt>222</dt><dd>0.797950739188506</dd><dt>223</dt><dd>0.911661154107878</dd><dt>224</dt><dd>0.911661154107878</dd><dt>225</dt><dd>0.797950739188506</dd><dt>226</dt><dd>0.423283291840528</dd><dt>227</dt><dd>0.0970522051145409</dd><dt>228</dt><dd>0.0970522051145409</dd><dt>229</dt><dd>0.423283291840528</dd><dt>230</dt><dd>0.797950739188506</dd><dt>231</dt><dd>0.219280773210838</dd><dt>232</dt><dd>0.797950739188506</dd><dt>233</dt><dd>0.601802744773117</dd><dt>234</dt><dd>0.797950739188506</dd><dt>235</dt><dd>0.0970522051145409</dd><dt>236</dt><dd>0.423283291840528</dd><dt>237</dt><dd>0.0970522051145409</dd><dt>238</dt><dd>0.0970522051145409</dd><dt>239</dt><dd>0.0970522051145409</dd><dt>240</dt><dd>0.0970522051145409</dd><dt>241</dt><dd>0.0970522051145409</dd><dt>242</dt><dd>0.797950739188506</dd><dt>243</dt><dd>0.0970522051145409</dd><dt>244</dt><dd>0.0970522051145409</dd><dt>245</dt><dd>0.0970522051145409</dd><dt>246</dt><dd>0.797950739188506</dd><dt>247</dt><dd>0.601802744773117</dd><dt>248</dt><dd>0.219280773210838</dd><dt>249</dt><dd>0.0970522051145409</dd><dt>250</dt><dd>0.423283291840528</dd><dt>251</dt><dd>0.0970522051145409</dd><dt>252</dt><dd>0.601802744773117</dd><dt>253</dt><dd>0.0970522051145409</dd><dt>254</dt><dd>0.423283291840528</dd><dt>255</dt><dd>0.0970522051145409</dd><dt>256</dt><dd>0.911661154107878</dd><dt>257</dt><dd>0.601802744773117</dd><dt>258</dt><dd>0.0970522051145409</dd><dt>259</dt><dd>0.797950739188506</dd><dt>260</dt><dd>0.219280773210838</dd><dt>261</dt><dd>0.219280773210838</dd><dt>262</dt><dd>0.219280773210838</dd><dt>263</dt><dd>0.219280773210838</dd><dt>264</dt><dd>0.601802744773117</dd><dt>265</dt><dd>0.0970522051145409</dd><dt>266</dt><dd>0.601802744773117</dd><dt>267</dt><dd>0.601802744773117</dd><dt>268</dt><dd>0.601802744773117</dd><dt>269</dt><dd>0.0970522051145409</dd><dt>270</dt><dd>0.0970522051145409</dd><dt>271</dt><dd>0.423283291840528</dd><dt>272</dt><dd>0.0970522051145409</dd><dt>273</dt><dd>0.0970522051145409</dd><dt>274</dt><dd>0.423283291840528</dd><dt>275</dt><dd>0.601802744773117</dd><dt>276</dt><dd>0.0970522051145409</dd><dt>277</dt><dd>0.423283291840528</dd><dt>278</dt><dd>0.0970522051145409</dd><dt>279</dt><dd>0.0970522051145409</dd><dt>280</dt><dd>0.797950739188506</dd><dt>281</dt><dd>0.0970522051145409</dd><dt>282</dt><dd>0.423283291840528</dd><dt>283</dt><dd>0.0970522051145409</dd><dt>284</dt><dd>0.0970522051145409</dd><dt>285</dt><dd>0.219280773210838</dd><dt>286</dt><dd>0.219280773210838</dd><dt>287</dt><dd>0.0970522051145409</dd><dt>288</dt><dd>0.601802744773117</dd><dt>289</dt><dd>0.911661154107878</dd><dt>290</dt><dd>0.423283291840528</dd><dt>291</dt><dd>0.0970522051145409</dd><dt>292</dt><dd>0.423283291840528</dd><dt>293</dt><dd>0.601802744773117</dd><dt>294</dt><dd>0.0970522051145409</dd><dt>295</dt><dd>0.0970522051145409</dd><dt>296</dt><dd>0.0970522051145409</dd><dt>297</dt><dd>0.601802744773117</dd><dt>298</dt><dd>0.911661154107878</dd><dt>299</dt><dd>0.601802744773117</dd><dt>300</dt><dd>0.423283291840528</dd><dt>301</dt><dd>0.219280773210838</dd><dt>302</dt><dd>0.0970522051145409</dd><dt>303</dt><dd>0.219280773210838</dd><dt>304</dt><dd>0.0970522051145409</dd><dt>305</dt><dd>0.0970522051145409</dd><dt>306</dt><dd>0.219280773210838</dd><dt>307</dt><dd>0.423283291840528</dd><dt>308</dt><dd>0.911661154107878</dd><dt>309</dt><dd>0.0970522051145409</dd><dt>310</dt><dd>0.797950739188506</dd><dt>311</dt><dd>0.423283291840528</dd><dt>312</dt><dd>0.219280773210838</dd><dt>313</dt><dd>0.219280773210838</dd><dt>314</dt><dd>0.797950739188506</dd><dt>315</dt><dd>0.423283291840528</dd><dt>316</dt><dd>0.0970522051145409</dd><dt>317</dt><dd>0.601802744773117</dd><dt>318</dt><dd>0.0970522051145409</dd><dt>319</dt><dd>0.423283291840528</dd><dt>320</dt><dd>0.219280773210838</dd><dt>321</dt><dd>0.0970522051145409</dd><dt>322</dt><dd>0.219280773210838</dd><dt>323</dt><dd>0.0970522051145409</dd><dt>324</dt><dd>0.219280773210838</dd><dt>325</dt><dd>0.0970522051145409</dd><dt>326</dt><dd>0.0970522051145409</dd><dt>327</dt><dd>0.911661154107878</dd><dt>328</dt><dd>0.0970522051145409</dd><dt>329</dt><dd>0.601802744773117</dd><dt>330</dt><dd>0.219280773210838</dd><dt>331</dt><dd>0.601802744773117</dd><dt>332</dt><dd>0.219280773210838</dd><dt>333</dt><dd>0.797950739188506</dd><dt>334</dt><dd>0.911661154107878</dd><dt>335</dt><dd>0.219280773210838</dd><dt>336</dt><dd>0.219280773210838</dd><dt>337</dt><dd>0.219280773210838</dd><dt>338</dt><dd>0.601802744773117</dd><dt>339</dt><dd>0.423283291840528</dd><dt>340</dt><dd>0.911661154107878</dd><dt>341</dt><dd>0.0970522051145409</dd><dt>342</dt><dd>0.0970522051145409</dd><dt>343</dt><dd>0.601802744773117</dd><dt>344</dt><dd>0.0970522051145409</dd><dt>345</dt><dd>0.797950739188506</dd><dt>346</dt><dd>0.797950739188506</dd><dt>347</dt><dd>0.0970522051145409</dd><dt>348</dt><dd>0.911661154107878</dd><dt>349</dt><dd>0.601802744773117</dd><dt>350</dt><dd>0.0970522051145409</dd><dt>351</dt><dd>0.601802744773117</dd><dt>352</dt><dd>0.911661154107878</dd><dt>353</dt><dd>0.219280773210838</dd><dt>354</dt><dd>0.219280773210838</dd><dt>355</dt><dd>0.911661154107878</dd><dt>356</dt><dd>0.423283291840528</dd><dt>357</dt><dd>0.219280773210838</dd><dt>358</dt><dd>0.911661154107878</dd><dt>359</dt><dd>0.911661154107878</dd><dt>360</dt><dd>0.601802744773117</dd><dt>361</dt><dd>0.219280773210838</dd><dt>362</dt><dd>0.423283291840528</dd><dt>363</dt><dd>0.0970522051145409</dd><dt>364</dt><dd>0.0970522051145409</dd><dt>365</dt><dd>0.0970522051145409</dd><dt>366</dt><dd>0.601802744773117</dd><dt>367</dt><dd>0.601802744773117</dd><dt>368</dt><dd>0.219280773210838</dd><dt>369</dt><dd>0.797950739188506</dd><dt>370</dt><dd>0.0970522051145409</dd><dt>371</dt><dd>0.219280773210838</dd><dt>372</dt><dd>0.0970522051145409</dd><dt>373</dt><dd>0.0970522051145409</dd><dt>374</dt><dd>0.423283291840528</dd><dt>375</dt><dd>0.911661154107878</dd><dt>376</dt><dd>0.0970522051145409</dd><dt>377</dt><dd>0.219280773210838</dd><dt>378</dt><dd>0.0970522051145409</dd><dt>379</dt><dd>0.911661154107878</dd><dt>380</dt><dd>0.0970522051145409</dd><dt>381</dt><dd>0.911661154107878</dd><dt>382</dt><dd>0.0970522051145409</dd><dt>383</dt><dd>0.0970522051145409</dd><dt>384</dt><dd>0.911661154107878</dd><dt>385</dt><dd>0.219280773210838</dd><dt>386</dt><dd>0.911661154107878</dd><dt>387</dt><dd>0.423283291840528</dd><dt>388</dt><dd>0.423283291840528</dd><dt>389</dt><dd>0.219280773210838</dd><dt>390</dt><dd>0.219280773210838</dd><dt>391</dt><dd>0.423283291840528</dd><dt>392</dt><dd>0.601802744773117</dd><dt>393</dt><dd>0.601802744773117</dd><dt>394</dt><dd>0.601802744773117</dd><dt>395</dt><dd>0.911661154107878</dd><dt>396</dt><dd>0.601802744773117</dd><dt>397</dt><dd>0.0970522051145409</dd><dt>398</dt><dd>0.911661154107878</dd><dt>399</dt><dd>0.0970522051145409</dd><dt>400</dt><dd>0.0970522051145409</dd><dt>401</dt><dd>0.0970522051145409</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.0970522051145409\n",
       "\\item[2] 0.601802744773117\n",
       "\\item[3] 0.219280773210838\n",
       "\\item[4] 0.0970522051145409\n",
       "\\item[5] 0.601802744773117\n",
       "\\item[6] 0.0970522051145409\n",
       "\\item[7] 0.601802744773117\n",
       "\\item[8] 0.219280773210838\n",
       "\\item[9] 0.601802744773117\n",
       "\\item[10] 0.0970522051145409\n",
       "\\item[11] 0.0970522051145409\n",
       "\\item[12] 0.423283291840528\n",
       "\\item[13] 0.911661154107878\n",
       "\\item[14] 0.219280773210838\n",
       "\\item[15] 0.911661154107878\n",
       "\\item[16] 0.797950739188506\n",
       "\\item[17] 0.219280773210838\n",
       "\\item[18] 0.0970522051145409\n",
       "\\item[19] 0.601802744773117\n",
       "\\item[20] 0.601802744773117\n",
       "\\item[21] 0.423283291840528\n",
       "\\item[22] 0.0970522051145409\n",
       "\\item[23] 0.911661154107878\n",
       "\\item[24] 0.423283291840528\n",
       "\\item[25] 0.911661154107878\n",
       "\\item[26] 0.0970522051145409\n",
       "\\item[27] 0.911661154107878\n",
       "\\item[28] 0.0970522051145409\n",
       "\\item[29] 0.423283291840528\n",
       "\\item[30] 0.0970522051145409\n",
       "\\item[31] 0.219280773210838\n",
       "\\item[32] 0.219280773210838\n",
       "\\item[33] 0.601802744773117\n",
       "\\item[34] 0.601802744773117\n",
       "\\item[35] 0.423283291840528\n",
       "\\item[36] 0.0970522051145409\n",
       "\\item[37] 0.601802744773117\n",
       "\\item[38] 0.601802744773117\n",
       "\\item[39] 0.0970522051145409\n",
       "\\item[40] 0.0970522051145409\n",
       "\\item[41] 0.0970522051145409\n",
       "\\item[42] 0.423283291840528\n",
       "\\item[43] 0.0970522051145409\n",
       "\\item[44] 0.797950739188506\n",
       "\\item[45] 0.911661154107878\n",
       "\\item[46] 0.0970522051145409\n",
       "\\item[47] 0.423283291840528\n",
       "\\item[48] 0.0970522051145409\n",
       "\\item[49] 0.911661154107878\n",
       "\\item[50] 0.601802744773117\n",
       "\\item[51] 0.423283291840528\n",
       "\\item[52] 0.219280773210838\n",
       "\\item[53] 0.797950739188506\n",
       "\\item[54] 0.911661154107878\n",
       "\\item[55] 0.219280773210838\n",
       "\\item[56] 0.0970522051145409\n",
       "\\item[57] 0.0970522051145409\n",
       "\\item[58] 0.0970522051145409\n",
       "\\item[59] 0.0970522051145409\n",
       "\\item[60] 0.911661154107878\n",
       "\\item[61] 0.0970522051145409\n",
       "\\item[62] 0.219280773210838\n",
       "\\item[63] 0.0970522051145409\n",
       "\\item[64] 0.601802744773117\n",
       "\\item[65] 0.423283291840528\n",
       "\\item[66] 0.797950739188506\n",
       "\\item[67] 0.601802744773117\n",
       "\\item[68] 0.423283291840528\n",
       "\\item[69] 0.423283291840528\n",
       "\\item[70] 0.911661154107878\n",
       "\\item[71] 0.601802744773117\n",
       "\\item[72] 0.0970522051145409\n",
       "\\item[73] 0.601802744773117\n",
       "\\item[74] 0.423283291840528\n",
       "\\item[75] 0.911661154107878\n",
       "\\item[76] 0.423283291840528\n",
       "\\item[77] 0.0970522051145409\n",
       "\\item[78] 0.911661154107878\n",
       "\\item[79] 0.219280773210838\n",
       "\\item[80] 0.601802744773117\n",
       "\\item[81] 0.0970522051145409\n",
       "\\item[82] 0.423283291840528\n",
       "\\item[83] 0.423283291840528\n",
       "\\item[84] 0.0970522051145409\n",
       "\\item[85] 0.219280773210838\n",
       "\\item[86] 0.0970522051145409\n",
       "\\item[87] 0.601802744773117\n",
       "\\item[88] 0.601802744773117\n",
       "\\item[89] 0.601802744773117\n",
       "\\item[90] 0.219280773210838\n",
       "\\item[91] 0.601802744773117\n",
       "\\item[92] 0.0970522051145409\n",
       "\\item[93] 0.911661154107878\n",
       "\\item[94] 0.0970522051145409\n",
       "\\item[95] 0.423283291840528\n",
       "\\item[96] 0.0970522051145409\n",
       "\\item[97] 0.911661154107878\n",
       "\\item[98] 0.0970522051145409\n",
       "\\item[99] 0.601802744773117\n",
       "\\item[100] 0.0970522051145409\n",
       "\\item[101] 0.911661154107878\n",
       "\\item[102] 0.219280773210838\n",
       "\\item[103] 0.0970522051145409\n",
       "\\item[104] 0.0970522051145409\n",
       "\\item[105] 0.601802744773117\n",
       "\\item[106] 0.0970522051145409\n",
       "\\item[107] 0.0970522051145409\n",
       "\\item[108] 0.0970522051145409\n",
       "\\item[109] 0.0970522051145409\n",
       "\\item[110] 0.219280773210838\n",
       "\\item[111] 0.219280773210838\n",
       "\\item[112] 0.601802744773117\n",
       "\\item[113] 0.911661154107878\n",
       "\\item[114] 0.601802744773117\n",
       "\\item[115] 0.911661154107878\n",
       "\\item[116] 0.0970522051145409\n",
       "\\item[117] 0.0970522051145409\n",
       "\\item[118] 0.601802744773117\n",
       "\\item[119] 0.423283291840528\n",
       "\\item[120] 0.797950739188506\n",
       "\\item[121] 0.797950739188506\n",
       "\\item[122] 0.0970522051145409\n",
       "\\item[123] 0.911661154107878\n",
       "\\item[124] 0.0970522051145409\n",
       "\\item[125] 0.0970522051145409\n",
       "\\item[126] 0.601802744773117\n",
       "\\item[127] 0.0970522051145409\n",
       "\\item[128] 0.601802744773117\n",
       "\\item[129] 0.219280773210838\n",
       "\\item[130] 0.0970522051145409\n",
       "\\item[131] 0.0970522051145409\n",
       "\\item[132] 0.423283291840528\n",
       "\\item[133] 0.601802744773117\n",
       "\\item[134] 0.0970522051145409\n",
       "\\item[135] 0.0970522051145409\n",
       "\\item[136] 0.0970522051145409\n",
       "\\item[137] 0.0970522051145409\n",
       "\\item[138] 0.219280773210838\n",
       "\\item[139] 0.601802744773117\n",
       "\\item[140] 0.0970522051145409\n",
       "\\item[141] 0.601802744773117\n",
       "\\item[142] 0.911661154107878\n",
       "\\item[143] 0.423283291840528\n",
       "\\item[144] 0.219280773210838\n",
       "\\item[145] 0.423283291840528\n",
       "\\item[146] 0.0970522051145409\n",
       "\\item[147] 0.423283291840528\n",
       "\\item[148] 0.0970522051145409\n",
       "\\item[149] 0.423283291840528\n",
       "\\item[150] 0.219280773210838\n",
       "\\item[151] 0.911661154107878\n",
       "\\item[152] 0.0970522051145409\n",
       "\\item[153] 0.0970522051145409\n",
       "\\item[154] 0.601802744773117\n",
       "\\item[155] 0.0970522051145409\n",
       "\\item[156] 0.0970522051145409\n",
       "\\item[157] 0.911661154107878\n",
       "\\item[158] 0.601802744773117\n",
       "\\item[159] 0.423283291840528\n",
       "\\item[160] 0.601802744773117\n",
       "\\item[161] 0.601802744773117\n",
       "\\item[162] 0.0970522051145409\n",
       "\\item[163] 0.797950739188506\n",
       "\\item[164] 0.0970522051145409\n",
       "\\item[165] 0.219280773210838\n",
       "\\item[166] 0.601802744773117\n",
       "\\item[167] 0.423283291840528\n",
       "\\item[168] 0.0970522051145409\n",
       "\\item[169] 0.911661154107878\n",
       "\\item[170] 0.601802744773117\n",
       "\\item[171] 0.0970522051145409\n",
       "\\item[172] 0.0970522051145409\n",
       "\\item[173] 0.0970522051145409\n",
       "\\item[174] 0.0970522051145409\n",
       "\\item[175] 0.0970522051145409\n",
       "\\item[176] 0.797950739188506\n",
       "\\item[177] 0.797950739188506\n",
       "\\item[178] 0.423283291840528\n",
       "\\item[179] 0.797950739188506\n",
       "\\item[180] 0.911661154107878\n",
       "\\item[181] 0.219280773210838\n",
       "\\item[182] 0.423283291840528\n",
       "\\item[183] 0.911661154107878\n",
       "\\item[184] 0.0970522051145409\n",
       "\\item[185] 0.911661154107878\n",
       "\\item[186] 0.219280773210838\n",
       "\\item[187] 0.797950739188506\n",
       "\\item[188] 0.0970522051145409\n",
       "\\item[189] 0.601802744773117\n",
       "\\item[190] 0.219280773210838\n",
       "\\item[191] 0.219280773210838\n",
       "\\item[192] 0.423283291840528\n",
       "\\item[193] 0.0970522051145409\n",
       "\\item[194] 0.219280773210838\n",
       "\\item[195] 0.219280773210838\n",
       "\\item[196] 0.0970522051145409\n",
       "\\item[197] 0.423283291840528\n",
       "\\item[198] 0.601802744773117\n",
       "\\item[199] 0.219280773210838\n",
       "\\item[200] 0.601802744773117\n",
       "\\item[201] ⋯\n",
       "\\item[202] 0.911661154107878\n",
       "\\item[203] 0.0970522051145409\n",
       "\\item[204] 0.797950739188506\n",
       "\\item[205] 0.0970522051145409\n",
       "\\item[206] 0.797950739188506\n",
       "\\item[207] 0.0970522051145409\n",
       "\\item[208] 0.911661154107878\n",
       "\\item[209] 0.601802744773117\n",
       "\\item[210] 0.0970522051145409\n",
       "\\item[211] 0.601802744773117\n",
       "\\item[212] 0.0970522051145409\n",
       "\\item[213] 0.219280773210838\n",
       "\\item[214] 0.219280773210838\n",
       "\\item[215] 0.911661154107878\n",
       "\\item[216] 0.0970522051145409\n",
       "\\item[217] 0.0970522051145409\n",
       "\\item[218] 0.423283291840528\n",
       "\\item[219] 0.0970522051145409\n",
       "\\item[220] 0.423283291840528\n",
       "\\item[221] 0.0970522051145409\n",
       "\\item[222] 0.797950739188506\n",
       "\\item[223] 0.911661154107878\n",
       "\\item[224] 0.911661154107878\n",
       "\\item[225] 0.797950739188506\n",
       "\\item[226] 0.423283291840528\n",
       "\\item[227] 0.0970522051145409\n",
       "\\item[228] 0.0970522051145409\n",
       "\\item[229] 0.423283291840528\n",
       "\\item[230] 0.797950739188506\n",
       "\\item[231] 0.219280773210838\n",
       "\\item[232] 0.797950739188506\n",
       "\\item[233] 0.601802744773117\n",
       "\\item[234] 0.797950739188506\n",
       "\\item[235] 0.0970522051145409\n",
       "\\item[236] 0.423283291840528\n",
       "\\item[237] 0.0970522051145409\n",
       "\\item[238] 0.0970522051145409\n",
       "\\item[239] 0.0970522051145409\n",
       "\\item[240] 0.0970522051145409\n",
       "\\item[241] 0.0970522051145409\n",
       "\\item[242] 0.797950739188506\n",
       "\\item[243] 0.0970522051145409\n",
       "\\item[244] 0.0970522051145409\n",
       "\\item[245] 0.0970522051145409\n",
       "\\item[246] 0.797950739188506\n",
       "\\item[247] 0.601802744773117\n",
       "\\item[248] 0.219280773210838\n",
       "\\item[249] 0.0970522051145409\n",
       "\\item[250] 0.423283291840528\n",
       "\\item[251] 0.0970522051145409\n",
       "\\item[252] 0.601802744773117\n",
       "\\item[253] 0.0970522051145409\n",
       "\\item[254] 0.423283291840528\n",
       "\\item[255] 0.0970522051145409\n",
       "\\item[256] 0.911661154107878\n",
       "\\item[257] 0.601802744773117\n",
       "\\item[258] 0.0970522051145409\n",
       "\\item[259] 0.797950739188506\n",
       "\\item[260] 0.219280773210838\n",
       "\\item[261] 0.219280773210838\n",
       "\\item[262] 0.219280773210838\n",
       "\\item[263] 0.219280773210838\n",
       "\\item[264] 0.601802744773117\n",
       "\\item[265] 0.0970522051145409\n",
       "\\item[266] 0.601802744773117\n",
       "\\item[267] 0.601802744773117\n",
       "\\item[268] 0.601802744773117\n",
       "\\item[269] 0.0970522051145409\n",
       "\\item[270] 0.0970522051145409\n",
       "\\item[271] 0.423283291840528\n",
       "\\item[272] 0.0970522051145409\n",
       "\\item[273] 0.0970522051145409\n",
       "\\item[274] 0.423283291840528\n",
       "\\item[275] 0.601802744773117\n",
       "\\item[276] 0.0970522051145409\n",
       "\\item[277] 0.423283291840528\n",
       "\\item[278] 0.0970522051145409\n",
       "\\item[279] 0.0970522051145409\n",
       "\\item[280] 0.797950739188506\n",
       "\\item[281] 0.0970522051145409\n",
       "\\item[282] 0.423283291840528\n",
       "\\item[283] 0.0970522051145409\n",
       "\\item[284] 0.0970522051145409\n",
       "\\item[285] 0.219280773210838\n",
       "\\item[286] 0.219280773210838\n",
       "\\item[287] 0.0970522051145409\n",
       "\\item[288] 0.601802744773117\n",
       "\\item[289] 0.911661154107878\n",
       "\\item[290] 0.423283291840528\n",
       "\\item[291] 0.0970522051145409\n",
       "\\item[292] 0.423283291840528\n",
       "\\item[293] 0.601802744773117\n",
       "\\item[294] 0.0970522051145409\n",
       "\\item[295] 0.0970522051145409\n",
       "\\item[296] 0.0970522051145409\n",
       "\\item[297] 0.601802744773117\n",
       "\\item[298] 0.911661154107878\n",
       "\\item[299] 0.601802744773117\n",
       "\\item[300] 0.423283291840528\n",
       "\\item[301] 0.219280773210838\n",
       "\\item[302] 0.0970522051145409\n",
       "\\item[303] 0.219280773210838\n",
       "\\item[304] 0.0970522051145409\n",
       "\\item[305] 0.0970522051145409\n",
       "\\item[306] 0.219280773210838\n",
       "\\item[307] 0.423283291840528\n",
       "\\item[308] 0.911661154107878\n",
       "\\item[309] 0.0970522051145409\n",
       "\\item[310] 0.797950739188506\n",
       "\\item[311] 0.423283291840528\n",
       "\\item[312] 0.219280773210838\n",
       "\\item[313] 0.219280773210838\n",
       "\\item[314] 0.797950739188506\n",
       "\\item[315] 0.423283291840528\n",
       "\\item[316] 0.0970522051145409\n",
       "\\item[317] 0.601802744773117\n",
       "\\item[318] 0.0970522051145409\n",
       "\\item[319] 0.423283291840528\n",
       "\\item[320] 0.219280773210838\n",
       "\\item[321] 0.0970522051145409\n",
       "\\item[322] 0.219280773210838\n",
       "\\item[323] 0.0970522051145409\n",
       "\\item[324] 0.219280773210838\n",
       "\\item[325] 0.0970522051145409\n",
       "\\item[326] 0.0970522051145409\n",
       "\\item[327] 0.911661154107878\n",
       "\\item[328] 0.0970522051145409\n",
       "\\item[329] 0.601802744773117\n",
       "\\item[330] 0.219280773210838\n",
       "\\item[331] 0.601802744773117\n",
       "\\item[332] 0.219280773210838\n",
       "\\item[333] 0.797950739188506\n",
       "\\item[334] 0.911661154107878\n",
       "\\item[335] 0.219280773210838\n",
       "\\item[336] 0.219280773210838\n",
       "\\item[337] 0.219280773210838\n",
       "\\item[338] 0.601802744773117\n",
       "\\item[339] 0.423283291840528\n",
       "\\item[340] 0.911661154107878\n",
       "\\item[341] 0.0970522051145409\n",
       "\\item[342] 0.0970522051145409\n",
       "\\item[343] 0.601802744773117\n",
       "\\item[344] 0.0970522051145409\n",
       "\\item[345] 0.797950739188506\n",
       "\\item[346] 0.797950739188506\n",
       "\\item[347] 0.0970522051145409\n",
       "\\item[348] 0.911661154107878\n",
       "\\item[349] 0.601802744773117\n",
       "\\item[350] 0.0970522051145409\n",
       "\\item[351] 0.601802744773117\n",
       "\\item[352] 0.911661154107878\n",
       "\\item[353] 0.219280773210838\n",
       "\\item[354] 0.219280773210838\n",
       "\\item[355] 0.911661154107878\n",
       "\\item[356] 0.423283291840528\n",
       "\\item[357] 0.219280773210838\n",
       "\\item[358] 0.911661154107878\n",
       "\\item[359] 0.911661154107878\n",
       "\\item[360] 0.601802744773117\n",
       "\\item[361] 0.219280773210838\n",
       "\\item[362] 0.423283291840528\n",
       "\\item[363] 0.0970522051145409\n",
       "\\item[364] 0.0970522051145409\n",
       "\\item[365] 0.0970522051145409\n",
       "\\item[366] 0.601802744773117\n",
       "\\item[367] 0.601802744773117\n",
       "\\item[368] 0.219280773210838\n",
       "\\item[369] 0.797950739188506\n",
       "\\item[370] 0.0970522051145409\n",
       "\\item[371] 0.219280773210838\n",
       "\\item[372] 0.0970522051145409\n",
       "\\item[373] 0.0970522051145409\n",
       "\\item[374] 0.423283291840528\n",
       "\\item[375] 0.911661154107878\n",
       "\\item[376] 0.0970522051145409\n",
       "\\item[377] 0.219280773210838\n",
       "\\item[378] 0.0970522051145409\n",
       "\\item[379] 0.911661154107878\n",
       "\\item[380] 0.0970522051145409\n",
       "\\item[381] 0.911661154107878\n",
       "\\item[382] 0.0970522051145409\n",
       "\\item[383] 0.0970522051145409\n",
       "\\item[384] 0.911661154107878\n",
       "\\item[385] 0.219280773210838\n",
       "\\item[386] 0.911661154107878\n",
       "\\item[387] 0.423283291840528\n",
       "\\item[388] 0.423283291840528\n",
       "\\item[389] 0.219280773210838\n",
       "\\item[390] 0.219280773210838\n",
       "\\item[391] 0.423283291840528\n",
       "\\item[392] 0.601802744773117\n",
       "\\item[393] 0.601802744773117\n",
       "\\item[394] 0.601802744773117\n",
       "\\item[395] 0.911661154107878\n",
       "\\item[396] 0.601802744773117\n",
       "\\item[397] 0.0970522051145409\n",
       "\\item[398] 0.911661154107878\n",
       "\\item[399] 0.0970522051145409\n",
       "\\item[400] 0.0970522051145409\n",
       "\\item[401] 0.0970522051145409\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.09705220511454092\n",
       ":   0.6018027447731173\n",
       ":   0.2192807732108384\n",
       ":   0.09705220511454095\n",
       ":   0.6018027447731176\n",
       ":   0.09705220511454097\n",
       ":   0.6018027447731178\n",
       ":   0.2192807732108389\n",
       ":   0.60180274477311710\n",
       ":   0.097052205114540911\n",
       ":   0.097052205114540912\n",
       ":   0.42328329184052813\n",
       ":   0.91166115410787814\n",
       ":   0.21928077321083815\n",
       ":   0.91166115410787816\n",
       ":   0.79795073918850617\n",
       ":   0.21928077321083818\n",
       ":   0.097052205114540919\n",
       ":   0.60180274477311720\n",
       ":   0.60180274477311721\n",
       ":   0.42328329184052822\n",
       ":   0.097052205114540923\n",
       ":   0.91166115410787824\n",
       ":   0.42328329184052825\n",
       ":   0.91166115410787826\n",
       ":   0.097052205114540927\n",
       ":   0.91166115410787828\n",
       ":   0.097052205114540929\n",
       ":   0.42328329184052830\n",
       ":   0.097052205114540931\n",
       ":   0.21928077321083832\n",
       ":   0.21928077321083833\n",
       ":   0.60180274477311734\n",
       ":   0.60180274477311735\n",
       ":   0.42328329184052836\n",
       ":   0.097052205114540937\n",
       ":   0.60180274477311738\n",
       ":   0.60180274477311739\n",
       ":   0.097052205114540940\n",
       ":   0.097052205114540941\n",
       ":   0.097052205114540942\n",
       ":   0.42328329184052843\n",
       ":   0.097052205114540944\n",
       ":   0.79795073918850645\n",
       ":   0.91166115410787846\n",
       ":   0.097052205114540947\n",
       ":   0.42328329184052848\n",
       ":   0.097052205114540949\n",
       ":   0.91166115410787850\n",
       ":   0.60180274477311751\n",
       ":   0.42328329184052852\n",
       ":   0.21928077321083853\n",
       ":   0.79795073918850654\n",
       ":   0.91166115410787855\n",
       ":   0.21928077321083856\n",
       ":   0.097052205114540957\n",
       ":   0.097052205114540958\n",
       ":   0.097052205114540959\n",
       ":   0.097052205114540960\n",
       ":   0.91166115410787861\n",
       ":   0.097052205114540962\n",
       ":   0.21928077321083863\n",
       ":   0.097052205114540964\n",
       ":   0.60180274477311765\n",
       ":   0.42328329184052866\n",
       ":   0.79795073918850667\n",
       ":   0.60180274477311768\n",
       ":   0.42328329184052869\n",
       ":   0.42328329184052870\n",
       ":   0.91166115410787871\n",
       ":   0.60180274477311772\n",
       ":   0.097052205114540973\n",
       ":   0.60180274477311774\n",
       ":   0.42328329184052875\n",
       ":   0.91166115410787876\n",
       ":   0.42328329184052877\n",
       ":   0.097052205114540978\n",
       ":   0.91166115410787879\n",
       ":   0.21928077321083880\n",
       ":   0.60180274477311781\n",
       ":   0.097052205114540982\n",
       ":   0.42328329184052883\n",
       ":   0.42328329184052884\n",
       ":   0.097052205114540985\n",
       ":   0.21928077321083886\n",
       ":   0.097052205114540987\n",
       ":   0.60180274477311788\n",
       ":   0.60180274477311789\n",
       ":   0.60180274477311790\n",
       ":   0.21928077321083891\n",
       ":   0.60180274477311792\n",
       ":   0.097052205114540993\n",
       ":   0.91166115410787894\n",
       ":   0.097052205114540995\n",
       ":   0.42328329184052896\n",
       ":   0.097052205114540997\n",
       ":   0.91166115410787898\n",
       ":   0.097052205114540999\n",
       ":   0.601802744773117100\n",
       ":   0.0970522051145409101\n",
       ":   0.911661154107878102\n",
       ":   0.219280773210838103\n",
       ":   0.0970522051145409104\n",
       ":   0.0970522051145409105\n",
       ":   0.601802744773117106\n",
       ":   0.0970522051145409107\n",
       ":   0.0970522051145409108\n",
       ":   0.0970522051145409109\n",
       ":   0.0970522051145409110\n",
       ":   0.219280773210838111\n",
       ":   0.219280773210838112\n",
       ":   0.601802744773117113\n",
       ":   0.911661154107878114\n",
       ":   0.601802744773117115\n",
       ":   0.911661154107878116\n",
       ":   0.0970522051145409117\n",
       ":   0.0970522051145409118\n",
       ":   0.601802744773117119\n",
       ":   0.423283291840528120\n",
       ":   0.797950739188506121\n",
       ":   0.797950739188506122\n",
       ":   0.0970522051145409123\n",
       ":   0.911661154107878124\n",
       ":   0.0970522051145409125\n",
       ":   0.0970522051145409126\n",
       ":   0.601802744773117127\n",
       ":   0.0970522051145409128\n",
       ":   0.601802744773117129\n",
       ":   0.219280773210838130\n",
       ":   0.0970522051145409131\n",
       ":   0.0970522051145409132\n",
       ":   0.423283291840528133\n",
       ":   0.601802744773117134\n",
       ":   0.0970522051145409135\n",
       ":   0.0970522051145409136\n",
       ":   0.0970522051145409137\n",
       ":   0.0970522051145409138\n",
       ":   0.219280773210838139\n",
       ":   0.601802744773117140\n",
       ":   0.0970522051145409141\n",
       ":   0.601802744773117142\n",
       ":   0.911661154107878143\n",
       ":   0.423283291840528144\n",
       ":   0.219280773210838145\n",
       ":   0.423283291840528146\n",
       ":   0.0970522051145409147\n",
       ":   0.423283291840528148\n",
       ":   0.0970522051145409149\n",
       ":   0.423283291840528150\n",
       ":   0.219280773210838151\n",
       ":   0.911661154107878152\n",
       ":   0.0970522051145409153\n",
       ":   0.0970522051145409154\n",
       ":   0.601802744773117155\n",
       ":   0.0970522051145409156\n",
       ":   0.0970522051145409157\n",
       ":   0.911661154107878158\n",
       ":   0.601802744773117159\n",
       ":   0.423283291840528160\n",
       ":   0.601802744773117161\n",
       ":   0.601802744773117162\n",
       ":   0.0970522051145409163\n",
       ":   0.797950739188506164\n",
       ":   0.0970522051145409165\n",
       ":   0.219280773210838166\n",
       ":   0.601802744773117167\n",
       ":   0.423283291840528168\n",
       ":   0.0970522051145409169\n",
       ":   0.911661154107878170\n",
       ":   0.601802744773117171\n",
       ":   0.0970522051145409172\n",
       ":   0.0970522051145409173\n",
       ":   0.0970522051145409174\n",
       ":   0.0970522051145409175\n",
       ":   0.0970522051145409176\n",
       ":   0.797950739188506177\n",
       ":   0.797950739188506178\n",
       ":   0.423283291840528179\n",
       ":   0.797950739188506180\n",
       ":   0.911661154107878181\n",
       ":   0.219280773210838182\n",
       ":   0.423283291840528183\n",
       ":   0.911661154107878184\n",
       ":   0.0970522051145409185\n",
       ":   0.911661154107878186\n",
       ":   0.219280773210838187\n",
       ":   0.797950739188506188\n",
       ":   0.0970522051145409189\n",
       ":   0.601802744773117190\n",
       ":   0.219280773210838191\n",
       ":   0.219280773210838192\n",
       ":   0.423283291840528193\n",
       ":   0.0970522051145409194\n",
       ":   0.219280773210838195\n",
       ":   0.219280773210838196\n",
       ":   0.0970522051145409197\n",
       ":   0.423283291840528198\n",
       ":   0.601802744773117199\n",
       ":   0.219280773210838200\n",
       ":   0.601802744773117201\n",
       ":   ⋯202\n",
       ":   0.911661154107878203\n",
       ":   0.0970522051145409204\n",
       ":   0.797950739188506205\n",
       ":   0.0970522051145409206\n",
       ":   0.797950739188506207\n",
       ":   0.0970522051145409208\n",
       ":   0.911661154107878209\n",
       ":   0.601802744773117210\n",
       ":   0.0970522051145409211\n",
       ":   0.601802744773117212\n",
       ":   0.0970522051145409213\n",
       ":   0.219280773210838214\n",
       ":   0.219280773210838215\n",
       ":   0.911661154107878216\n",
       ":   0.0970522051145409217\n",
       ":   0.0970522051145409218\n",
       ":   0.423283291840528219\n",
       ":   0.0970522051145409220\n",
       ":   0.423283291840528221\n",
       ":   0.0970522051145409222\n",
       ":   0.797950739188506223\n",
       ":   0.911661154107878224\n",
       ":   0.911661154107878225\n",
       ":   0.797950739188506226\n",
       ":   0.423283291840528227\n",
       ":   0.0970522051145409228\n",
       ":   0.0970522051145409229\n",
       ":   0.423283291840528230\n",
       ":   0.797950739188506231\n",
       ":   0.219280773210838232\n",
       ":   0.797950739188506233\n",
       ":   0.601802744773117234\n",
       ":   0.797950739188506235\n",
       ":   0.0970522051145409236\n",
       ":   0.423283291840528237\n",
       ":   0.0970522051145409238\n",
       ":   0.0970522051145409239\n",
       ":   0.0970522051145409240\n",
       ":   0.0970522051145409241\n",
       ":   0.0970522051145409242\n",
       ":   0.797950739188506243\n",
       ":   0.0970522051145409244\n",
       ":   0.0970522051145409245\n",
       ":   0.0970522051145409246\n",
       ":   0.797950739188506247\n",
       ":   0.601802744773117248\n",
       ":   0.219280773210838249\n",
       ":   0.0970522051145409250\n",
       ":   0.423283291840528251\n",
       ":   0.0970522051145409252\n",
       ":   0.601802744773117253\n",
       ":   0.0970522051145409254\n",
       ":   0.423283291840528255\n",
       ":   0.0970522051145409256\n",
       ":   0.911661154107878257\n",
       ":   0.601802744773117258\n",
       ":   0.0970522051145409259\n",
       ":   0.797950739188506260\n",
       ":   0.219280773210838261\n",
       ":   0.219280773210838262\n",
       ":   0.219280773210838263\n",
       ":   0.219280773210838264\n",
       ":   0.601802744773117265\n",
       ":   0.0970522051145409266\n",
       ":   0.601802744773117267\n",
       ":   0.601802744773117268\n",
       ":   0.601802744773117269\n",
       ":   0.0970522051145409270\n",
       ":   0.0970522051145409271\n",
       ":   0.423283291840528272\n",
       ":   0.0970522051145409273\n",
       ":   0.0970522051145409274\n",
       ":   0.423283291840528275\n",
       ":   0.601802744773117276\n",
       ":   0.0970522051145409277\n",
       ":   0.423283291840528278\n",
       ":   0.0970522051145409279\n",
       ":   0.0970522051145409280\n",
       ":   0.797950739188506281\n",
       ":   0.0970522051145409282\n",
       ":   0.423283291840528283\n",
       ":   0.0970522051145409284\n",
       ":   0.0970522051145409285\n",
       ":   0.219280773210838286\n",
       ":   0.219280773210838287\n",
       ":   0.0970522051145409288\n",
       ":   0.601802744773117289\n",
       ":   0.911661154107878290\n",
       ":   0.423283291840528291\n",
       ":   0.0970522051145409292\n",
       ":   0.423283291840528293\n",
       ":   0.601802744773117294\n",
       ":   0.0970522051145409295\n",
       ":   0.0970522051145409296\n",
       ":   0.0970522051145409297\n",
       ":   0.601802744773117298\n",
       ":   0.911661154107878299\n",
       ":   0.601802744773117300\n",
       ":   0.423283291840528301\n",
       ":   0.219280773210838302\n",
       ":   0.0970522051145409303\n",
       ":   0.219280773210838304\n",
       ":   0.0970522051145409305\n",
       ":   0.0970522051145409306\n",
       ":   0.219280773210838307\n",
       ":   0.423283291840528308\n",
       ":   0.911661154107878309\n",
       ":   0.0970522051145409310\n",
       ":   0.797950739188506311\n",
       ":   0.423283291840528312\n",
       ":   0.219280773210838313\n",
       ":   0.219280773210838314\n",
       ":   0.797950739188506315\n",
       ":   0.423283291840528316\n",
       ":   0.0970522051145409317\n",
       ":   0.601802744773117318\n",
       ":   0.0970522051145409319\n",
       ":   0.423283291840528320\n",
       ":   0.219280773210838321\n",
       ":   0.0970522051145409322\n",
       ":   0.219280773210838323\n",
       ":   0.0970522051145409324\n",
       ":   0.219280773210838325\n",
       ":   0.0970522051145409326\n",
       ":   0.0970522051145409327\n",
       ":   0.911661154107878328\n",
       ":   0.0970522051145409329\n",
       ":   0.601802744773117330\n",
       ":   0.219280773210838331\n",
       ":   0.601802744773117332\n",
       ":   0.219280773210838333\n",
       ":   0.797950739188506334\n",
       ":   0.911661154107878335\n",
       ":   0.219280773210838336\n",
       ":   0.219280773210838337\n",
       ":   0.219280773210838338\n",
       ":   0.601802744773117339\n",
       ":   0.423283291840528340\n",
       ":   0.911661154107878341\n",
       ":   0.0970522051145409342\n",
       ":   0.0970522051145409343\n",
       ":   0.601802744773117344\n",
       ":   0.0970522051145409345\n",
       ":   0.797950739188506346\n",
       ":   0.797950739188506347\n",
       ":   0.0970522051145409348\n",
       ":   0.911661154107878349\n",
       ":   0.601802744773117350\n",
       ":   0.0970522051145409351\n",
       ":   0.601802744773117352\n",
       ":   0.911661154107878353\n",
       ":   0.219280773210838354\n",
       ":   0.219280773210838355\n",
       ":   0.911661154107878356\n",
       ":   0.423283291840528357\n",
       ":   0.219280773210838358\n",
       ":   0.911661154107878359\n",
       ":   0.911661154107878360\n",
       ":   0.601802744773117361\n",
       ":   0.219280773210838362\n",
       ":   0.423283291840528363\n",
       ":   0.0970522051145409364\n",
       ":   0.0970522051145409365\n",
       ":   0.0970522051145409366\n",
       ":   0.601802744773117367\n",
       ":   0.601802744773117368\n",
       ":   0.219280773210838369\n",
       ":   0.797950739188506370\n",
       ":   0.0970522051145409371\n",
       ":   0.219280773210838372\n",
       ":   0.0970522051145409373\n",
       ":   0.0970522051145409374\n",
       ":   0.423283291840528375\n",
       ":   0.911661154107878376\n",
       ":   0.0970522051145409377\n",
       ":   0.219280773210838378\n",
       ":   0.0970522051145409379\n",
       ":   0.911661154107878380\n",
       ":   0.0970522051145409381\n",
       ":   0.911661154107878382\n",
       ":   0.0970522051145409383\n",
       ":   0.0970522051145409384\n",
       ":   0.911661154107878385\n",
       ":   0.219280773210838386\n",
       ":   0.911661154107878387\n",
       ":   0.423283291840528388\n",
       ":   0.423283291840528389\n",
       ":   0.219280773210838390\n",
       ":   0.219280773210838391\n",
       ":   0.423283291840528392\n",
       ":   0.601802744773117393\n",
       ":   0.601802744773117394\n",
       ":   0.601802744773117395\n",
       ":   0.911661154107878396\n",
       ":   0.601802744773117397\n",
       ":   0.0970522051145409398\n",
       ":   0.911661154107878399\n",
       ":   0.0970522051145409400\n",
       ":   0.0970522051145409401\n",
       ":   0.0970522051145409\n",
       "\n"
      ],
      "text/plain": [
       "         1          2          3          4          5          6          7 \n",
       "0.09705221 0.60180274 0.21928077 0.09705221 0.60180274 0.09705221 0.60180274 \n",
       "         8          9         10         11         12         13         14 \n",
       "0.21928077 0.60180274 0.09705221 0.09705221 0.42328329 0.91166115 0.21928077 \n",
       "        15         16         17         18         19         20         21 \n",
       "0.91166115 0.79795074 0.21928077 0.09705221 0.60180274 0.60180274 0.42328329 \n",
       "        22         23         24         25         26         27         28 \n",
       "0.09705221 0.91166115 0.42328329 0.91166115 0.09705221 0.91166115 0.09705221 \n",
       "        29         30         31         32         33         34         35 \n",
       "0.42328329 0.09705221 0.21928077 0.21928077 0.60180274 0.60180274 0.42328329 \n",
       "        36         37         38         39         40         41         42 \n",
       "0.09705221 0.60180274 0.60180274 0.09705221 0.09705221 0.09705221 0.42328329 \n",
       "        43         44         45         46         47         48         49 \n",
       "0.09705221 0.79795074 0.91166115 0.09705221 0.42328329 0.09705221 0.91166115 \n",
       "        50         51         52         53         54         55         56 \n",
       "0.60180274 0.42328329 0.21928077 0.79795074 0.91166115 0.21928077 0.09705221 \n",
       "        57         58         59         60         61         62         63 \n",
       "0.09705221 0.09705221 0.09705221 0.91166115 0.09705221 0.21928077 0.09705221 \n",
       "        64         65         66         67         68         69         70 \n",
       "0.60180274 0.42328329 0.79795074 0.60180274 0.42328329 0.42328329 0.91166115 \n",
       "        71         72         73         74         75         76         77 \n",
       "0.60180274 0.09705221 0.60180274 0.42328329 0.91166115 0.42328329 0.09705221 \n",
       "        78         79         80         81         82         83         84 \n",
       "0.91166115 0.21928077 0.60180274 0.09705221 0.42328329 0.42328329 0.09705221 \n",
       "        85         86         87         88         89         90         91 \n",
       "0.21928077 0.09705221 0.60180274 0.60180274 0.60180274 0.21928077 0.60180274 \n",
       "        92         93         94         95         96         97         98 \n",
       "0.09705221 0.91166115 0.09705221 0.42328329 0.09705221 0.91166115 0.09705221 \n",
       "        99        100        101        102        103        104        105 \n",
       "0.60180274 0.09705221 0.91166115 0.21928077 0.09705221 0.09705221 0.60180274 \n",
       "       106        107        108        109        110        111        112 \n",
       "0.09705221 0.09705221 0.09705221 0.09705221 0.21928077 0.21928077 0.60180274 \n",
       "       113        114        115        116        117        118        119 \n",
       "0.91166115 0.60180274 0.91166115 0.09705221 0.09705221 0.60180274 0.42328329 \n",
       "       120        121        122        123        124        125        126 \n",
       "0.79795074 0.79795074 0.09705221 0.91166115 0.09705221 0.09705221 0.60180274 \n",
       "       127        128        129        130        131        132        133 \n",
       "0.09705221 0.60180274 0.21928077 0.09705221 0.09705221 0.42328329 0.60180274 \n",
       "       134        135        136        137        138        139        140 \n",
       "0.09705221 0.09705221 0.09705221 0.09705221 0.21928077 0.60180274 0.09705221 \n",
       "       141        142        143        144        145        146        147 \n",
       "0.60180274 0.91166115 0.42328329 0.21928077 0.42328329 0.09705221 0.42328329 \n",
       "       148        149        150        151        152        153        154 \n",
       "0.09705221 0.42328329 0.21928077 0.91166115 0.09705221 0.09705221 0.60180274 \n",
       "       155        156        157        158        159        160        161 \n",
       "0.09705221 0.09705221 0.91166115 0.60180274 0.42328329 0.60180274 0.60180274 \n",
       "       162        163        164        165        166        167        168 \n",
       "0.09705221 0.79795074 0.09705221 0.21928077 0.60180274 0.42328329 0.09705221 \n",
       "       169        170        171        172        173        174        175 \n",
       "0.91166115 0.60180274 0.09705221 0.09705221 0.09705221 0.09705221 0.09705221 \n",
       "       176        177        178        179        180        181        182 \n",
       "0.79795074 0.79795074 0.42328329 0.79795074 0.91166115 0.21928077 0.42328329 \n",
       "       183        184        185        186        187        188        189 \n",
       "0.91166115 0.09705221 0.91166115 0.21928077 0.79795074 0.09705221 0.60180274 \n",
       "       190        191        192        193        194        195        196 \n",
       "0.21928077 0.21928077 0.42328329 0.09705221 0.21928077 0.21928077 0.09705221 \n",
       "       197        198        199        200        201        202        203 \n",
       "0.42328329 0.60180274 0.21928077 0.60180274 0.60180274 0.09705221 0.42328329 \n",
       "       204        205        206        207        208        209        210 \n",
       "0.79795074 0.21928077 0.42328329 0.60180274 0.21928077 0.91166115 0.09705221 \n",
       "       211        212        213        214        215        216        217 \n",
       "0.09705221 0.09705221 0.21928077 0.79795074 0.60180274 0.42328329 0.60180274 \n",
       "       218        219        220        221        222        223        224 \n",
       "0.42328329 0.91166115 0.09705221 0.79795074 0.09705221 0.79795074 0.09705221 \n",
       "       225        226        227        228        229        230        231 \n",
       "0.91166115 0.60180274 0.09705221 0.60180274 0.09705221 0.21928077 0.21928077 \n",
       "       232        233        234        235        236        237        238 \n",
       "0.91166115 0.09705221 0.09705221 0.42328329 0.09705221 0.42328329 0.09705221 \n",
       "       239        240        241        242        243        244        245 \n",
       "0.79795074 0.91166115 0.91166115 0.79795074 0.42328329 0.09705221 0.09705221 \n",
       "       246        247        248        249        250        251        252 \n",
       "0.42328329 0.79795074 0.21928077 0.79795074 0.60180274 0.79795074 0.09705221 \n",
       "       253        254        255        256        257        258        259 \n",
       "0.42328329 0.09705221 0.09705221 0.09705221 0.09705221 0.09705221 0.79795074 \n",
       "       260        261        262        263        264        265        266 \n",
       "0.09705221 0.09705221 0.09705221 0.79795074 0.60180274 0.21928077 0.09705221 \n",
       "       267        268        269        270        271        272        273 \n",
       "0.42328329 0.09705221 0.60180274 0.09705221 0.42328329 0.09705221 0.91166115 \n",
       "       274        275        276        277        278        279        280 \n",
       "0.60180274 0.09705221 0.79795074 0.21928077 0.21928077 0.21928077 0.21928077 \n",
       "       281        282        283        284        285        286        287 \n",
       "0.60180274 0.09705221 0.60180274 0.60180274 0.60180274 0.09705221 0.09705221 \n",
       "       288        289        290        291        292        293        294 \n",
       "0.42328329 0.09705221 0.09705221 0.42328329 0.60180274 0.09705221 0.42328329 \n",
       "       295        296        297        298        299        300        301 \n",
       "0.09705221 0.09705221 0.79795074 0.09705221 0.42328329 0.09705221 0.09705221 \n",
       "       302        303        304        305        306        307        308 \n",
       "0.21928077 0.21928077 0.09705221 0.60180274 0.91166115 0.42328329 0.09705221 \n",
       "       309        310        311        312        313        314        315 \n",
       "0.42328329 0.60180274 0.09705221 0.09705221 0.09705221 0.60180274 0.91166115 \n",
       "       316        317        318        319        320        321        322 \n",
       "0.60180274 0.42328329 0.21928077 0.09705221 0.21928077 0.09705221 0.09705221 \n",
       "       323        324        325        326        327        328        329 \n",
       "0.21928077 0.42328329 0.91166115 0.09705221 0.79795074 0.42328329 0.21928077 \n",
       "       330        331        332        333        334        335        336 \n",
       "0.21928077 0.79795074 0.42328329 0.09705221 0.60180274 0.09705221 0.42328329 \n",
       "       337        338        339        340        341        342        343 \n",
       "0.21928077 0.09705221 0.21928077 0.09705221 0.21928077 0.09705221 0.09705221 \n",
       "       344        345        346        347        348        349        350 \n",
       "0.91166115 0.09705221 0.60180274 0.21928077 0.60180274 0.21928077 0.79795074 \n",
       "       351        352        353        354        355        356        357 \n",
       "0.91166115 0.21928077 0.21928077 0.21928077 0.60180274 0.42328329 0.91166115 \n",
       "       358        359        360        361        362        363        364 \n",
       "0.09705221 0.09705221 0.60180274 0.09705221 0.79795074 0.79795074 0.09705221 \n",
       "       365        366        367        368        369        370        371 \n",
       "0.91166115 0.60180274 0.09705221 0.60180274 0.91166115 0.21928077 0.21928077 \n",
       "       372        373        374        375        376        377        378 \n",
       "0.91166115 0.42328329 0.21928077 0.91166115 0.91166115 0.60180274 0.21928077 \n",
       "       379        380        381        382        383        384        385 \n",
       "0.42328329 0.09705221 0.09705221 0.09705221 0.60180274 0.60180274 0.21928077 \n",
       "       386        387        388        389        390        391        392 \n",
       "0.79795074 0.09705221 0.21928077 0.09705221 0.09705221 0.42328329 0.91166115 \n",
       "       393        394        395        396        397        398        399 \n",
       "0.09705221 0.21928077 0.09705221 0.91166115 0.09705221 0.91166115 0.09705221 \n",
       "       400        401        402        403        404        405        406 \n",
       "0.09705221 0.91166115 0.21928077 0.91166115 0.42328329 0.42328329 0.21928077 \n",
       "       407        408        409        410        411        412        413 \n",
       "0.21928077 0.42328329 0.60180274 0.60180274 0.60180274 0.91166115 0.60180274 \n",
       "       414        415        416        417        418 \n",
       "0.09705221 0.91166115 0.09705221 0.09705221 0.09705221 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the trained model for predict the survivor in test set\n",
    "titanic_testing$Survived <- predict(titanic_glm, titanic_testing, type=\"response\") # Question: why type=\"response\"\n",
    "titanic_testing$Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.7-1.1\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(randomForest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**: train a `random forest` for predicting  `Survived` based on `Pclass` and `Sex`, then apply the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                Length Class  Mode     \n",
       "call               5   -none- call     \n",
       "type               1   -none- character\n",
       "predicted        891   factor numeric  \n",
       "err.rate          30   -none- numeric  \n",
       "confusion          6   -none- numeric  \n",
       "votes           1782   matrix numeric  \n",
       "oob.times        891   -none- numeric  \n",
       "classes            2   -none- character\n",
       "importance         8   -none- numeric  \n",
       "importanceSD       6   -none- numeric  \n",
       "localImportance    0   -none- NULL     \n",
       "proximity          0   -none- NULL     \n",
       "ntree              1   -none- numeric  \n",
       "mtry               1   -none- numeric  \n",
       "forest            14   -none- list     \n",
       "y                891   factor numeric  \n",
       "test               0   -none- NULL     \n",
       "inbag              0   -none- NULL     \n",
       "terms              3   terms  call     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# install.packages('randomForest')\n",
    "\n",
    "# train the model\n",
    "titanic_rf <- randomForest(Survived ~ Pclass + Sex, data = titanic_training, importance=TRUE, ntree=10)\n",
    "\n",
    "# examine the model\n",
    "summary(titanic_rf)\n",
    "\n",
    "# Apply the trained model for predict the survivor in test set\n",
    "\n",
    "titanic_testing$Survived <- predict(titanic_rf, titanic_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance and Evaluation\n",
    "\n",
    "#### Measures of goodness\n",
    "We trained and applied some models to predict labels for un-labeled data, but how can we say if the model is good?\n",
    "\n",
    "The goodness of prediction models are measured w.r.t some dataset with groudtruth -- i.e., we need the labels for observations in test data. Typical measures for the goodness of the classification models are [Precision, Recall](https://en.wikipedia.org/wiki/Precision_and_recall), and [F1 scores](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we will train a model and evaluate its performance using [Iris dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m150\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m5\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (1): X5\n",
      "\u001b[32mdbl\u001b[39m (4): X1, X2, X3, X4\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "iris <- read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', col_names = FALSE)\n",
    "\n",
    "# name the columns\n",
    "names(iris) <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>sepal_length</th><th scope=col>sepal_width</th><th scope=col>petal_length</th><th scope=col>petal_width</th><th scope=col>class</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>Iris-setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " sepal\\_length & sepal\\_width & petal\\_length & petal\\_width & class\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa\\\\\n",
       "\t 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa\\\\\n",
       "\t 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa\\\\\n",
       "\t 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa\\\\\n",
       "\t 5.0 & 3.6 & 1.4 & 0.2 & Iris-setosa\\\\\n",
       "\t 5.4 & 3.9 & 1.7 & 0.4 & Iris-setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 5\n",
       "\n",
       "| sepal_length &lt;dbl&gt; | sepal_width &lt;dbl&gt; | petal_length &lt;dbl&gt; | petal_width &lt;dbl&gt; | class &lt;chr&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa |\n",
       "| 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa |\n",
       "| 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 5.4 | 3.9 | 1.7 | 0.4 | Iris-setosa |\n",
       "\n"
      ],
      "text/plain": [
       "  sepal_length sepal_width petal_length petal_width class      \n",
       "1 5.1          3.5         1.4          0.2         Iris-setosa\n",
       "2 4.9          3.0         1.4          0.2         Iris-setosa\n",
       "3 4.7          3.2         1.3          0.2         Iris-setosa\n",
       "4 4.6          3.1         1.5          0.2         Iris-setosa\n",
       "5 5.0          3.6         1.4          0.2         Iris-setosa\n",
       "6 5.4          3.9         1.7          0.4         Iris-setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spc_tbl_ [150 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n",
      " $ sepal_length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n",
      " $ sepal_width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n",
      " $ petal_length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n",
      " $ petal_width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n",
      " $ class       : chr [1:150] \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n",
      " - attr(*, \"spec\")=\n",
      "  .. cols(\n",
      "  ..   X1 = \u001b[32mcol_double()\u001b[39m,\n",
      "  ..   X2 = \u001b[32mcol_double()\u001b[39m,\n",
      "  ..   X3 = \u001b[32mcol_double()\u001b[39m,\n",
      "  ..   X4 = \u001b[32mcol_double()\u001b[39m,\n",
      "  ..   X5 = \u001b[31mcol_character()\u001b[39m\n",
      "  .. )\n",
      " - attr(*, \"problems\")=<externalptr> \n"
     ]
    }
   ],
   "source": [
    "# view some rows\n",
    "head(iris)\n",
    "\n",
    "# view columns's data type\n",
    "str(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert `class` from string to factor\n",
    "iris$class <- as.factor(iris$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the models, often we **divide** the set of labeled data into **training and test sets**. The models will be **trained on the training set**, and **evaluated using the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the iris dataset into: 80% for training set and the remaining 20% for test set \n",
    "training_size <- floor(0.8 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a svm model to predict `class` from `sepal_length` and `sepal_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and load the 'e1071' package if not yet\n",
    "# install.packages('e1071')\n",
    "library(e1071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris_train[features], y=iris_train$class, kernel =\"linear\", cost=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the trained model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and load the 'mltest' package if not yet\n",
    "# install.packages('mltest', repos = 'https://cran.r-project.org/')\n",
    "library(mltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(svm_model, iris_test[features]))\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.766666666666667"
      ],
      "text/latex": [
       "0.766666666666667"
      ],
      "text/markdown": [
       "0.766666666666667"
      ],
      "text/plain": [
       "[1] 0.7666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# overall classification accuracy\n",
    "classifier_metrics$accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.6</dd><dt>Iris-virginica</dt><dd>0.7</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.6\n",
       "\\item[Iris-virginica] 0.7\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.6Iris-virginica\n",
       ":   0.7\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "            1.0             0.6             0.7 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# precision for classes\n",
    "classifier_metrics$precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.666666666666667</dd><dt>Iris-virginica</dt><dd>0.636363636363636</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.666666666666667\n",
       "\\item[Iris-virginica] 0.636363636363636\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.666666666666667Iris-virginica\n",
       ":   0.636363636363636\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.6666667       0.6363636 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recall for classes\n",
    "classifier_metrics$recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.631578947368421</dd><dt>Iris-virginica</dt><dd>0.666666666666667</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.631578947368421\n",
       "\\item[Iris-virginica] 0.666666666666667\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.631578947368421Iris-virginica\n",
       ":   0.666666666666667\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.6315789       0.6666667 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F1-measures for classes\n",
    "classifier_metrics$F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Divide the iris dataset into training and test sets by ratio 5:5, train a svm model to predict `class` using all feature, then examine the performance of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.96</dd><dt>Iris-virginica</dt><dd>0.925925925925926</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.96\n",
       "\\item[Iris-virginica] 0.925925925925926\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.96Iris-virginica\n",
       ":   0.925925925925926\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9600000       0.9259259 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.923076923076923</dd><dt>Iris-virginica</dt><dd>0.961538461538462</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.923076923076923\n",
       "\\item[Iris-virginica] 0.961538461538462\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.923076923076923Iris-virginica\n",
       ":   0.961538461538462\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9230769       0.9615385 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.941176470588235</dd><dt>Iris-virginica</dt><dd>0.943396226415094</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.941176470588235\n",
       "\\item[Iris-virginica] 0.943396226415094\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.941176470588235Iris-virginica\n",
       ":   0.943396226415094\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9411765       0.9433962 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "#divide the iris dataset into training and test sets by ratio 50:50\n",
    "training_size <- floor(0.5 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]\n",
    "\n",
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris_train[features], y=iris_train$class, kernel =\"linear\", cost=1)\n",
    "\n",
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(svm_model, iris_test[features]))\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)\n",
    "\n",
    "\n",
    "# precision for classes\n",
    "classifier_metrics$precision\n",
    "# recall for classes\n",
    "classifier_metrics$recall\n",
    "# F1-measures for classes\n",
    "classifier_metrics$F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting\n",
    "\n",
    "Underfitting happens when the trained model does not fit well with the training data, and thus often lead to poor performance on test data\n",
    "\n",
    "[Overfitting](https://en.wikipedia.org/wiki/Overfitting) happens when the trained model fit too well with the training data while perform (predict) poorly on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 -- 5 points \n",
    "\n",
    "Follow the above example and divide the iris dataset into training set and test set by ratio 5:95 and 20:80, and change the number of tree (`ntree`) in the model to 1 and 5 and 10. Examine the performance of the trained models on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=1\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>NaN</dd><dt>Iris-versicolor</dt><dd>0.6</dd><dt>Iris-virginica</dt><dd>0.927835051546392</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] NaN\n",
       "\\item[Iris-versicolor] 0.6\n",
       "\\item[Iris-virginica] 0.927835051546392\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   NaNIris-versicolor\n",
       ":   0.6Iris-virginica\n",
       ":   0.927835051546392\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "            NaN       0.6000000       0.9278351 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=5\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>0.98989898989899</dd><dt>Iris-versicolor</dt><dd>0.703296703296703</dd><dt>Iris-virginica</dt><dd>0.729166666666667</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 0.98989898989899\n",
       "\\item[Iris-versicolor] 0.703296703296703\n",
       "\\item[Iris-virginica] 0.729166666666667\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   0.98989898989899Iris-versicolor\n",
       ":   0.703296703296703Iris-virginica\n",
       ":   0.729166666666667\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      0.9898990       0.7032967       0.7291667 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=10\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.905263157894737</dd><dt>Iris-virginica</dt><dd>0.903225806451613</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.905263157894737\n",
       "\\item[Iris-virginica] 0.903225806451613\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.905263157894737Iris-virginica\n",
       ":   0.903225806451613\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9052632       0.9032258 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "training_size <- floor(0.05 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]\n",
    "\n",
    "# train a random forest model\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "iris_rf1 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=1)\n",
    "iris_rf5 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=5)\n",
    "iris_rf10 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# get the prediction\n",
    "predicted_labels1 <- as.factor(predict(iris_rf1, iris_test[features]))\n",
    "predicted_labels5 <- as.factor(predict(iris_rf5, iris_test[features]))\n",
    "predicted_labels10 <- as.factor(predict(iris_rf10, iris_test[features]))\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics1 <- ml_test(predicted_labels1, true_labels, output.as.table = FALSE)\n",
    "classifier_metrics5 <- ml_test(predicted_labels5, true_labels, output.as.table = FALSE)\n",
    "classifier_metrics10 <- ml_test(predicted_labels10, true_labels, output.as.table = FALSE)\n",
    "\n",
    "# F1-measures for classes\n",
    "print('ncol=1')\n",
    "classifier_metrics1$F1\n",
    "print('ncol=5')\n",
    "classifier_metrics5$F1\n",
    "print('ncol=10')\n",
    "classifier_metrics10$F1\n",
    "# get the groundtruth\n",
    "# get the prediction\n",
    "\n",
    "# measure the performance\n",
    "# F1-measures for classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=1\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>0.794520547945206</dd><dt>Iris-versicolor</dt><dd>0.708333333333333</dd><dt>Iris-virginica</dt><dd>0.788732394366197</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 0.794520547945206\n",
       "\\item[Iris-versicolor] 0.708333333333333\n",
       "\\item[Iris-virginica] 0.788732394366197\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   0.794520547945206Iris-versicolor\n",
       ":   0.708333333333333Iris-virginica\n",
       ":   0.788732394366197\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      0.7945205       0.7083333       0.7887324 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=5\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.866666666666667</dd><dt>Iris-virginica</dt><dd>0.837837837837838</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.866666666666667\n",
       "\\item[Iris-virginica] 0.837837837837838\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.866666666666667Iris-virginica\n",
       ":   0.837837837837838\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.8666667       0.8378378 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=10\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.926829268292683</dd><dt>Iris-virginica</dt><dd>0.926829268292683</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.926829268292683\n",
       "\\item[Iris-virginica] 0.926829268292683\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.926829268292683Iris-virginica\n",
       ":   0.926829268292683\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9268293       0.9268293 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "training_size <- floor(0.2 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]\n",
    "\n",
    "# train a random forest model\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "iris_rf1 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=1)\n",
    "iris_rf5 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=5)\n",
    "iris_rf10 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# get the prediction\n",
    "predicted_labels1 <- as.factor(predict(iris_rf1, iris_test[features]))\n",
    "predicted_labels5 <- as.factor(predict(iris_rf5, iris_test[features]))\n",
    "predicted_labels10 <- as.factor(predict(iris_rf10, iris_test[features]))\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics1 <- ml_test(predicted_labels1, true_labels, output.as.table = FALSE)\n",
    "classifier_metrics5 <- ml_test(predicted_labels5, true_labels, output.as.table = FALSE)\n",
    "classifier_metrics10 <- ml_test(predicted_labels10, true_labels, output.as.table = FALSE)\n",
    "\n",
    "# F1-measures for classes\n",
    "print('ncol=1')\n",
    "classifier_metrics1$F1\n",
    "print('ncol=5')\n",
    "classifier_metrics5$F1\n",
    "print('ncol=10')\n",
    "classifier_metrics10$F1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "As we see in the previous section, the performance of models are highly dependent of the datasets. Hence, their performance can be high or low by chance, e.g., by the way we divide the original labeled dataset into training and test set. To reduce the effect of the chance, or to get more robust, reliable measures for the performance, we need to run K-fold [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "Basically, K-fold cross validation includes:\n",
    "\n",
    "- Devide the labeled dataset in to K equal folds\n",
    "- In turn, take one fold as test set, the union of other K-1 folds is used as train set\n",
    "- Train a model for each division of training & test sets, i.e., K models in total\n",
    "- Evaluate each model on corresponding test set\n",
    "- Aggregate the peformance of K models to form the final performance\n",
    "\n",
    "The prediction is for a new data observation is then formed by aggreating (e.g., voting base) the predictions of the above K models\n",
    "\n",
    "Refer to `trainControl` object and `train` function in `caret` package for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "One important aspect of the shallow classification models is the importance score of features in (trained) models. These scores inform us how important the features are in predicting the label of data observations.\n",
    "\n",
    "**Example** we will examine the importance of features in `random forest` and `svm` (with `linear kernel`) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 4 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Feature</th><th scope=col>Importance</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>sepal_length</td><td>2.519192</td></tr>\n",
       "\t<tr><td>sepal_width </td><td>2.085638</td></tr>\n",
       "\t<tr><td>petal_length</td><td>8.107142</td></tr>\n",
       "\t<tr><td>petal_width </td><td>5.793173</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 4 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Feature & Importance\\\\\n",
       " <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t sepal\\_length & 2.519192\\\\\n",
       "\t sepal\\_width  & 2.085638\\\\\n",
       "\t petal\\_length & 8.107142\\\\\n",
       "\t petal\\_width  & 5.793173\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 4 × 2\n",
       "\n",
       "| Feature &lt;chr&gt; | Importance &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| sepal_length | 2.519192 |\n",
       "| sepal_width  | 2.085638 |\n",
       "| petal_length | 8.107142 |\n",
       "| petal_width  | 5.793173 |\n",
       "\n"
      ],
      "text/plain": [
       "  Feature      Importance\n",
       "1 sepal_length 2.519192  \n",
       "2 sepal_width  2.085638  \n",
       "3 petal_length 8.107142  \n",
       "4 petal_width  5.793173  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAAOVBMVEUAAAAAAIszMzNNTU1o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\nhsICYvx/hHcnrgr/5fwAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finding the importance scores of features in `random forest` model is quite simple\n",
    "\n",
    "# train a random forest model\n",
    "iris_rf <- randomForest(class ~ ., data = iris, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the features importance\n",
    "imp <- importance(iris_rf, type=1)\n",
    "featureImportance <- tibble(Feature=row.names(imp), Importance=imp[,1])\n",
    "\n",
    "# show the importance scores\n",
    "featureImportance\n",
    "\n",
    "# visualizing the importance scores\n",
    "ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +\n",
    "    geom_bar(stat=\"identity\", fill='darkblue') +\n",
    "    coord_flip() +\n",
    "    xlab(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petal_length  petal_width  sepal_width sepal_length \n",
      "   3.0676386    3.0183477    0.8116943    0.1407988 \n"
     ]
    }
   ],
   "source": [
    "# It is a bit more complicated with SVM-linear model\n",
    "\n",
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris[features], y=iris$class, kernel =\"linear\", cost=1)\n",
    "w <- t(svm_model$coefs) %*% svm_model$SV        # weight vectors\n",
    "w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight\n",
    "w <- sort(w, decreasing = T)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 -- 5 points\n",
    "\n",
    "Using [US's income dataset](https://archive.ics.uci.edu/ml/datasets/Adult), train a random forest model for predicting `income` from all other features, examine the importance of the features for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m32561\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m15\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (9): X2, X4, X6, X7, X8, X9, X10, X14, X15\n",
      "\u001b[32mdbl\u001b[39m (6): X1, X3, X5, X11, X12, X13\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 14 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Feature</th><th scope=col>Importance</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>age          </td><td>15.2880148</td></tr>\n",
       "\t<tr><td>workclass    </td><td> 6.6108971</td></tr>\n",
       "\t<tr><td>fnlwgt       </td><td>-0.5487413</td></tr>\n",
       "\t<tr><td>education    </td><td> 6.5973165</td></tr>\n",
       "\t<tr><td>educationNum </td><td> 7.6720009</td></tr>\n",
       "\t<tr><td>maritalStatus</td><td> 6.0464113</td></tr>\n",
       "\t<tr><td>occupation   </td><td>22.7421476</td></tr>\n",
       "\t<tr><td>relationship </td><td> 7.6791192</td></tr>\n",
       "\t<tr><td>race         </td><td> 0.6069371</td></tr>\n",
       "\t<tr><td>sex          </td><td> 4.1743796</td></tr>\n",
       "\t<tr><td>capitalGain  </td><td>21.4010329</td></tr>\n",
       "\t<tr><td>capitalLoss  </td><td>16.2537750</td></tr>\n",
       "\t<tr><td>hoursPerWeek </td><td> 9.4345375</td></tr>\n",
       "\t<tr><td>nativeCountry</td><td> 2.8678038</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 14 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Feature & Importance\\\\\n",
       " <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t age           & 15.2880148\\\\\n",
       "\t workclass     &  6.6108971\\\\\n",
       "\t fnlwgt        & -0.5487413\\\\\n",
       "\t education     &  6.5973165\\\\\n",
       "\t educationNum  &  7.6720009\\\\\n",
       "\t maritalStatus &  6.0464113\\\\\n",
       "\t occupation    & 22.7421476\\\\\n",
       "\t relationship  &  7.6791192\\\\\n",
       "\t race          &  0.6069371\\\\\n",
       "\t sex           &  4.1743796\\\\\n",
       "\t capitalGain   & 21.4010329\\\\\n",
       "\t capitalLoss   & 16.2537750\\\\\n",
       "\t hoursPerWeek  &  9.4345375\\\\\n",
       "\t nativeCountry &  2.8678038\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 14 × 2\n",
       "\n",
       "| Feature &lt;chr&gt; | Importance &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| age           | 15.2880148 |\n",
       "| workclass     |  6.6108971 |\n",
       "| fnlwgt        | -0.5487413 |\n",
       "| education     |  6.5973165 |\n",
       "| educationNum  |  7.6720009 |\n",
       "| maritalStatus |  6.0464113 |\n",
       "| occupation    | 22.7421476 |\n",
       "| relationship  |  7.6791192 |\n",
       "| race          |  0.6069371 |\n",
       "| sex           |  4.1743796 |\n",
       "| capitalGain   | 21.4010329 |\n",
       "| capitalLoss   | 16.2537750 |\n",
       "| hoursPerWeek  |  9.4345375 |\n",
       "| nativeCountry |  2.8678038 |\n",
       "\n"
      ],
      "text/plain": [
       "   Feature       Importance\n",
       "1  age           15.2880148\n",
       "2  workclass      6.6108971\n",
       "3  fnlwgt        -0.5487413\n",
       "4  education      6.5973165\n",
       "5  educationNum   7.6720009\n",
       "6  maritalStatus  6.0464113\n",
       "7  occupation    22.7421476\n",
       "8  relationship   7.6791192\n",
       "9  race           0.6069371\n",
       "10 sex            4.1743796\n",
       "11 capitalGain   21.4010329\n",
       "12 capitalLoss   16.2537750\n",
       "13 hoursPerWeek   9.4345375\n",
       "14 nativeCountry  2.8678038"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAAOVBMVEUAAAAAAIszMzNNTU1o\naGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enr6+vw8PD///8b9ATfAAAACXBI\nWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO2ci3YiR7YFNV1u2zOecV/z/x97haDYBaoUVVlH\nmTsPEWtdIR5SdL7CgLjzdgIAGIS33v8AAICtECwAGAaCBQDDQLAAYBgIFgAMA8ECgGEgWAAw\nDAQLAIbBJ1j/F0LQr0EzqgVNRo0yQbDQECw05hplgmChIVhozDXKBMFCQ7DQmGuUCYKFhmCh\nMdcoEwQLDcFCY65RJggWGoKFxlyjTGQL1r8AwB2CRbAAhoFgESyAYSBYBAtgGAgWwQIYBoJF\nsACGgWARLIBhIFgEC2AYCBbBAhgGgkWwAIaBYBEsgGEgWAQLYBgIFsECGAaCRbAAhoFgESyA\nYSBYBAtgGMYJ1lS8h2ABvAhjBKscqzMEC+BFIFgEC2AYOgVremflcrrWaTrd3f7xdVo+7Ho3\nwQJ4KfoES2EqXE6fb1+5+3T6cebAP2RB75UAgKfUH/CwYJ0eLhdlOi2ur3TtCs+wAF4Eg2dY\nXwbr+pqQYAGAfbAWpSJYAC+Pd7BK72ERLICXxCBYj2F6eNOdl4QAcKVPsEofazhdP7+w/FjD\n5c+CDx9rIFgAr0inYD3r2a5HEyyAF4FgESyAYSBYBAtgGDyDtQ+CBfAiECyCBTAMBItgAQwD\nwSJYAMNAsAgWwDAQLIIFMAwEi2ABDAPBIlgAw0CwCBbAMBAsggUwDASLYAEMA8G6jSzm16AZ\n1YImo0aZIFhoCBYac40yQbDQECw05hplgmChIVhozDXKBMFCQ7DQmGuUiWzB6v3nD4BXYmd3\nao+1MkGwAKCWnd2pPdbKBMECgFp2dqf2WCsTBAsAatnZndpjrUwQLACoZWd3ao+1MkGwAKCW\nnd2pPdbKBMECgFp2dqf2WCsTBAsAatnZndpjrUwQLACoZWd3ao+1MkGwAKCWnd2pPdbKBMEC\ngFp2dqf2WCsTBAsAatnZndpjrUwQLACoZWd3ao+1MkGwAKCWnd2pPdbKBMECgFp2dqf2WCsT\nBAsAatnZndpjrUwQLACoZWd3ao+1MvENwZpWrk5nvngQwQIYkJ3dqT3W3xqshxxNt+ufI7Wk\ndiz39F5AgFdiZ3dqj7Uy0SRY08rNn6gdyz29FxDgldjZndpjrUwcCdb8Mu96OZ0Wl9Pi5vt+\nLW69e5lYO5Z7ei8gwCuxszu1xzokWNNpkaOPQN2uT/d3T6s/NN0e9eNM/T9kSe8FBHglYk7t\nDo4Ga3FlrVCLq4u33ae7x1+pje89vRcQ4JXY+USp9lgHB+v6EvAxWLeb797Dur+VYAGMy87u\n1B7r2GAt36l6aNHnYE2fH0SwAMZkZ3dqj3VosD69l1V8D6tQNYIFMCY7u1N7rOOCNddp/U33\naRmvx1sJFsDg7OxO7bEOCdbyYw3LjylcPtGwuPm0/KS7biVYAIOzszu1xzomWI/9OvbjtWO5\np/cCArwSO7tTe6yVCYIFALXs7E7tsVYmCBYA1LKzO7XHWpngf14GAGrZ2Z3aY61MECwAqGVn\nd2qPtTJBsACglp3dqT3WygTBAoBadnan9lgrEwQLAGrZ2Z3aY61MECwAqGVnd2qPtTJBsACg\nlp3dqT3WygTBAoBadnan9lgrEwQLAGrZ2Z3aY61MECwAqGVnd2qPtTKRLVhBvwbNqBY0GTXK\nBMFCQ7DQmGuUCYKFhmChMdcoEwQLDcFCY65RJggWGoKFxlyjTGQLVu+/mgAUqTmpMcdidI0y\nQbAAGlFzUmOOxegaZYJgATSi5qTGHIvRNcoEwQJoRM1JjTkWo2uUCYIF0IiakxpzLEbXKBME\nC6ARNSc15liMrlEmCBZAI2pOasyxGF2jTBAsgEbUnNSYYzG6RpkgWACNqDmpMcdidI0yQbAA\nGlFzUmOOxegaZYJgATSi5qTGHIvRNcoEwQJoRM1JjTkWo2uUCYIF0IiakxpzLEbXKBMEC6AR\nNSc15liMrlEmCBZAI2pOasyxGF2jTBAsgEbUnNSYYzG6RpkgWACNqDmpMcdidI0yER6s6dPV\naf2Bj8RMSe89CVCk5qTGHIvRNcrEdzzDmu6/J1gAZ2pOasyxGF2jTBAsgEbUnNSYYzG6Rpmo\nD9b0zuJyOi0up8XNCtbDT9wuCRa8BDUnNeZYjK4JCNZ0WuToI1C369P93VPpJ243/jhT+w+5\np/eeBCgSs8Vfm2PBWlxZKdRXwXr8DTzDgvTUPLWIORaja0KDdX0J+Fio282PwTrNLwSnu2TF\nTEnvPQlQpOakxhyL0TWRwVq+UzXdfVt4hlVIVsyU9N6TAEVqTmrMsRhdExislXemnryH9fnH\nCRa8AjUnNeZYjK6JCtZcp/U33QsvCT+/6U6w4BWoOakxx2J0TUCwlh9OuORHn2O4fqxh7tcF\nPtYAL07NSY05FqNrIoL12K+jvyBmSnrvSYAiNSc15liMrlEmCBZAI2pOasyxGF2jTBAsgEbU\nnNSYYzG6Rpngf14GoBE1JzXmWIyuUSYIFkAjak5qzLEYXaNMECyARtSc1JhjMbpGmSBYAI2o\nOakxx2J0jTJBsAAaUXNSY47F6BplgmABNKLmpMYci9E1ygTBAmhEzUmNORaja5QJggXQiJqT\nGnMsRtcoEwQLoBE1JzXmWIyuUSYIFkAjak5qzLEYXaNMECyARtSc1JhjMbpGmcgWLPeZf2FN\nqsGgaapRJggWGoKFxlyjTBAsNAQLjblGmSBYaAgWGnONMpEtWL3fV4UXJGbrruJekkYaZYJg\nARwkZuuu4l6SRhplgmABHCRm667iXpJGGmWCYAEcJGbrruJekkYaZYJgARwkZuuu4l6SRhpl\ngmABHCRm667iXpJGGmWCYAEcJGbrruJekkYaZYJgARwkZuuu4l6SRhplgmABHCRm667iXpJG\nGmWCYAEcJGbrruJekkYaZYJgARwkZuuu4l6SRhplgmABHCRm667iXpJGGmWCYAEcJGbrruJe\nkkYaZYJgARwkZuuu4l6SRhplgmABHCRm667iXpJGGmWCYAEcJGbrruJekkYaZYJgARwkZuuu\n4l6SRhplgmABHCRm667iXpJGGmWCYAEcJGbrruJekkYaZYJgARwkZuuu4l6SRhplomWwpneu\nl6dpcZ1gwdDEbN1V3EvSSNMlWNP1y6VVun46/TgTI+m9d+EFidm6sIXWLwmn+3Cd9BQrpuG9\n9y68IDFbdxX3pz6NNJ2CdXkNeAvWdPeaMGZKeu9deEFitu4q7iVppOkTrOn0+RmWiJmS3nsX\nXpCYrbuKe0kaaboE6/GlIMGCHMRs3VXcS9JI0y1Y0+qb7gQLRiZm667iXpJGmi7BWn6cgY81\nQB5itu4q7iVppOkTrCXTp1tipqT33oUXJGbrruJekkYaZaJDsB5eCs7ETEnvvQsvSMzWXcW9\nJI00ykSPZ1j3LwVnYqak996FFyRm667iXpJGGmWC/19CgIPEbN1V3EvSSKNMECyAg8Rs3VXc\nS9JIo0wQLICDxGzdVdxL0kijTBAsgIPEbN1V3EvSSKNMECyAg8Rs3VXcS9JIo0wQLICDxGzd\nVdxL0kijTBAsgIPEbN1V3EvSSKNMECyAg8Rs3VXcS9JIo0wQLICDxGzdVdxL0kijTBAsgIPE\nbN1V3EvSSKNMECyAg8Rs3VXcS9JIo0xkC5b7zL+wJtVg0DTVKBMECw3BQmOuUSYIFhqChcZc\no0wQLDQEC425RpnIFqze779C8T1o90OBxlejTBAsiKa06WJW+BloEmqUCYIF0ZQ2XcwKPwNN\nQo0yQbAgmtKmi1nhZ6BJqFEmCBZEU9p0MSv8DDQJNcoEwYJoSpsuZoWfgSahRpkgWBBNadPF\nrPAz0CTUKBMEC6IpbbqYFX4GmoQaZYJgQTSlTRezws9Ak1CjTBAsiKa06WJW+BloEmqUCYIF\n0ZQ2XcwKPwNNQo0yQbAgmtKmi1nhZ6BJqFEmCBZEU9p0MSv8DDQJNcoEwYJoSpsuZoWfgSah\nRpkgWBBNadPFrPAz0CTUKBMEC6IpbbqYFX4GmoQaZYJgQTSlTRezws9Ak1CjTDwN1rQlNtOF\n9R+9vyj+6pgp6X1YgWChCdeEB2v1sdP167R2J8FKS2nTxazwM9Ak1LQJ1uX6NF8QrBehtOli\nVvgZaBJqdgXr+lrv+ppvrs/59vsbLxe3W6bpU7Cm9V9FsHJR2nQxK/wMNAk1e4I1zYF6uLyv\n16l09+L/Pv2K+Tf8OPPsH7KN3ocV/vWvmJUEWGHbS8JCbW5Pj/Sm+6e7vw7W4lVhTMN7H1bg\nGRaacE1UsE7T4xvqc7luty9idbvv9pjlu1gxU9L7sALBQhOuCQvW8qXhXboe/0I4fbrvdHuX\ni2DlorTpYlb4GWgSagKD9fnK9mDdpS5mSnofViBYaMI1B4M1La6vBevubfi7m9ffdCdYqSht\nupgVfgaahJqKYC0/k7B8D+vhJeHKBx10yccaXoLSpotZ4WegSajZEaxmxExJ78MKBAtNuEaZ\nIFgQTWnTxazwM9Ak1CgTBAuiKW26mBV+BpqEGmWCYEE0pU0Xs8LPQJNQo0wQLIimtOliVvgZ\naBJqlAmCBdGUNl3MCj8DTUKNMkGwIJrSpotZ4WegSahRJggWRFPadDEr/Aw0CTXKBMGCaEqb\nLmaFn4EmoUaZIFgQTWnTxazwM9Ak1CgTBAuiKW26mBV+BpqEGmWCYEE0pU0Xs8LPQJNQo0wQ\nLIimtOliVvgZaBJqlAmCBdGUNl3MCj8DTUKNMpEtWO4z/8KaVINB01SjTBAsNAQLjblGmSBY\naAgWGnONMkGw0BAsNOYaZYJgoSFYaMw1ykS2YPX+C9nIxKxAEfdDgcZXo0wQLJiJWYEi7ocC\nja9GmSBYMBOzAkXcDwUaX40yQbBgJmYFirgfCjS+GmWCYMFMzAoUcT8UaHw1ygTBgpmYFSji\nfijQ+GqUCYIFMzErUMT9UKDx1SgTBAtmYlagiPuhQOOrUSYIFszErEAR90OBxlejTBAsmIlZ\ngSLuhwKNr0aZIFgwE7MCRdwPBRpfjTJBsGAmZgWKuB8KNL4aZYJgwUzMChRxPxRofDXKBMGC\nmZgVKOJ+KND4apQJggUzMStQxP1QoPHVKBMEC2ZiVqCI+6FA46tRJmKCNX15de2uzw+JmZLe\nh35kYlagiPuhQOOrUSbCg/VFrL6+O2ZKeh/6kYlZgSLuhwKNr0aZIFgwE7MCRdwPBRpfjTJR\nHazpNL3HZ5rOXz8ydPn246uufjzsI1IP16fFXQTLg5gVKOJ+KND4aiKCNYfq0p7lt3dXy9f1\nG36cqf2H3NP70I9MzAoAfCMHnmGd7iu1+PahXw+PeEzclZiG9z70IxOzAkXc/yuOxlcTFqzp\n+kJwvlIM1un66o9g+RKzAkXcDwUaX01YsBZXFqVaC1b5RSTBMiFmBYq4Hwo0vppvCFbpPazy\nwwiWGTErUMT9UKDx1YQGS/n54iXhyqWecREsC2JWoIj7oUDjq4kK1sPHGi4ZevhYg+7+9JKQ\njzVYEbMCRdwPBRpfTUCwjjPdX42Zkt6HfmRiVqCI+6FA46tRJggWzMSsQBH3Q4HGV6NMECyY\niVmBIu6HAo2vRpngf14GZmJWoIj7oUDjq1EmCBbMxKxAEfdDgcZXo0wQLJiJWYEi7ocCja9G\nmSBYMBOzAkXcDwUaX40yQbBgJmYFirgfCjS+GmWCYMFMzAoUcT8UaHw1ygTBgpmYFSjifijQ\n+GqUCYIFMzErUMT9UKDx1SgTBAtmYlagiPuhQOOrUSYIFszErEAR90OBxlejTBAsmIlZgSLu\nhwKNr0aZyBYs95l/YU2qwaBpqlEmCBYagoXGXKNMECw0BAuNuUaZIFhoCBYac40yQbDQECw0\n5hplIluwev+lbWRiVqCI+6FA46tRJggWzMSsQBH3Q4HGV6NMECyYiVmBIu6HAo2vRpkgWDAT\nswJF3A8FGl+NMkGwYCZmBYq4Hwo0vhplgmDBTMwKFHE/FGh8NcoEwYKZmBUo4n4o0PhqlAmC\nBTMxK1DE/VCg8dUoEwQLZmJWoIj7oUDjq1EmCBbMxKxAEfdDgcZXo0wQLJiJWYEi7ocCja9G\nmSBYMBOzAkXcDwUaX40yQbBgJmYFirgfCjS+GmWCYMFMzAoUcT8UaHw1ygTBgpmYFSjifijQ\n+GqUCYIFMzErUMT9UKDx1SgTBAtmYlagiPuhQOOrUSb2B2vaffe06QdjpqT3oR+ZmBUo4n4o\n0PhqlInQYBXumqZnP3gmZkp6H/qRiVmBIu6HAo2vRploEqzLHQTLnJgVKOJ+KND4apSJbcGa\npsuzpPPFdFJ/FrdP16+Lm6fT7fL6A7qcf4Bg+RCzAkXcDwUaX83OYC17M02nlf4s7l9c1fXV\ne6b5V/84s6mcT+l96EcmZgUAvpHdwfp0uXjQY5Z0Wbhn+StiGt770I9MzAoUcf+vOBpfzd5g\nTdfXfMVg6f4vn3gRLGdiVqCI+6FA46vZG6zl5VqwFqUqv2IkWObErEAR90OBxlcTHKzSe1gE\nayhiVqCI+6FA46upCdbDm+6PLxGfvCRc/0GC5UTMChRxPxRofDU7g/X4sYbTw+cXbh93ePxY\nwzJYyx8kWI7ErEAR90OBxlezN1gtiJmS3od+ZGJWoIj7oUDjq1EmCBbMxKxAEfdDgcZXo0wQ\nLJiJWYEi7ocCja9GmSBYMBOzAkXcDwUaX40yQbBgJmYFirgfCjS+GmWCYMFMzAoUcT8UaHw1\nygTBgpmYFSjifijQ+GqUCYIFMzErUMT9UKDx1SgTBAtmYlagiPuhQOOrUSYIFszErEAR90OB\nxlejTBAsmIlZgSLuhwKNr0aZIFgwE7MCRdwPBRpfjTJBsGAmZgWKuB8KNL4aZSJbsNxn/oU1\nqQaDpqlGmSBYaAgWGnONMkGw0BAsNOYaZYJgoSFYaMw1ygTBQkOw0JhrlIlswer9lzY3tm2H\nmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEm\nCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3\nsKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZu\ntm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJ\nqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmvi1Y05dXV4iZkt6BcGPbdoiZewsL\nmowaZYJg5WbbdoiZewsLmowaZYJg5WbbdoiZewsLmowaZeJosKbll+mdj+/eLz7ddvv20yXB\n+j62bYeYubewoMmoiQ3WtPh6adTtDt129+3d5en048zBf8iV3oFwI2ZWAUw4/JJwuqaqUKRJ\nDzst717cdCWm4b0D4ca2/37FzL2FBU1GTZNg6RnUfDm/PLzeON0lK2ZKegfCjW3bIWbuLSxo\nMmpCg3V7PXgfrOnTMyxdX0tWzJT0DoQb27ZDzNxbWNBk1LQI1uL7h5vXLgnW97BtO8TMvYUF\nTUZNYLDm50ynL4KlV4zT6pvuBOvb2LYdYubewoImo+Z7grX8CEPhYw2rH28gWN/Htu0QM/cW\nFjQZNZHBiiJmSnoHwo1t2yFm7i0saDJqlAmClZtt2yFm7i0saDJqlAmClZtt2yFm7i0saDJq\nlAmClZtt2yFm7i0saDJqlAmClZtt2yFm7i0saDJqlAmClZtt2yFm7i0saDJqlAmClZtt2yFm\n7i0saDJqlAmClZtt2yFm7i0saDJqlAmClZtt2yFm7i0saDJqlAmClZtt2yFm7i0saDJqlAmC\nlZtt2yFm7i0saDJqlAmClZtt2yFm7i0saDJqlAmClZtt2yFm7i0saDJqlAmClZtt2yFm7i0s\naDJqlIlswXKf+RfWpBoMmqYaZYJgoSFYaMw1ygTBQkOw0JhrlAmChYZgoTHXKBPZgtX7TW43\ntm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJ\nqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2H\nmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEm\nCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3\nsKDJqFEmCFZutm2HmLm3sKDJqFEmCFZutm2HmLm3sKDJqFEmwoM11d4dMyW9A+HGtu0QM/cW\nFjQZNcpEy2B93bKYKekdCDe2bYeYubewoMmoUSYIVm62bYeYubewoMmoUSZCgjW9c708V2ma\n67S4fbp+Xdw8na53E6zvY9t2iJl7CwuajJrYYM2BusTo/vrj5fLqNP/sjzMB/5B3egfCjZhZ\nBTAhOlifLhcPegzW7Wc/iGl470C4se2/XzFzb2FBk1ETHKzp+pqvGCzdT7Dasm07xMy9hQVN\nRk1wsJaXa8FalIpgtWXbdoiZewsLmoyatsEqvYdFsL6fbdshZu4tLGgyar4hWA9vuj++ROQl\nYR+2bYeYubewoMmoiQ3W48caTg+fX7h93OHxYw0E69vZth1i5t7CgiajJjhYIcRMSe9AuLFt\nO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaN\nMkGwcrNtO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaNMkGwcrNtO8TM\nvYUFTUaNMkGwcrNtO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaNMkGw\ncrNtO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaNMkGwcrNtO8TMvYUFTUaNMpEtWO4z/8KaVINB\n01SjTBAsNAQLjblGmSBYaAgWGnONMkGw0BAsNOYaZYJgoSFYaMw1ykS2YPX+q1w3jkwawULj\nrVEmCFYSjkwawULjrVEmCFYSjkwawULjrVEmCFYSjkwawULjrVEmCFYSjkwawULjrVEmCFYS\njkwawULjrVEmCFYSjkwawULjrVEmCFYSjkwawULjrVEmCFYSjkwawULjrVEmCFYSjkwawULj\nrVEmCFYSjkwawULjrVEmCFYSjkwawULjrVEmCFYSjkwawULjrVEmCFYSjkwawULjrVEmCFYS\njkwawULjrVEmCFYSjkwawULjrVEmgoI1rVyd3tnxK2KmpHc3unFk0ggWGm+NMhH3DGt6+H66\n+2blQQQrkiOTRrDQeGsaBGt6/I5gfStHJo1gofHWVAZrOn28yru+1Lu+5DvfeLlnWtymNC3v\n+Piqp153rxljpqR3N7pxZNIIFhpvTW2wptsXveRbXr/dtqzRwx1331++/Diz5x9Spnc3uhEz\nfQDe7HyGdffldB+u08Nt81Oxxzs+/cCFmIb37kY3jkwaz7DQeGtignV9mXfXn8Vtp8Idi5eE\nBCuMI5NGsNB4a0KC9fjS8PRw26lwx+IpGu9hhXFk0ggWGm9NRLDWLpfvYd0Fa+1Bp+UlwTrI\nkUkjWGi8NVHBenxJuLhtenwmdpo+P4hghXFk0ggWGm9NRLBO148o6K+F08Nt+viD7lh8z8ca\nIjkyaQQLjbemMljfSsyU9O5GN45MGsFC461RJghWEo5MGsFC461RJghWEo5MGsFC461RJghW\nEo5MGsFC461RJghWEo5MGsFC461RJghWEo5MGsFC461RJghWEo5MGsFC461RJghWEo5MGsFC\n461RJghWEo5MGsFC461RJghWEo5MGsFC461RJghWEo5MGsFC461RJghWEo5MGsFC461RJghW\nEo5MGsFC461RJghWEo5MGsFC461RJrIFy33mX1iTajBommqUCYKFhmChMdcoEwQLDcFCY65R\nJggWGoKFxlyjTBAsNAQLjblGmcgWrN5/rNvDhnWKmRQPTarBoGmqUSYIVj82rFPMpHhoUg0G\nTVONMkGw+rFhnWImxUOTajBommqUCYLVjw3rFDMpHppUg0HTVKNMEKx+bFinmEnx0KQaDJqm\nGmWCYPVjwzrFTIqHJtVg0DTVKBMEqx8b1ilmUjw0qQaDpqlGmSBY/diwTjGT4qFJNRg0TTXK\nBMHqx4Z1ipkUD02qwaBpqlEmCFY/NqxTzKR4aFINBk1TjTJBsPqxYZ1iJsVDk2owaJpqlAmC\n1Y8N6xQzKR6aVINB01SjTBCsfmxYp5hJ8dCkGgyaphplgmD1Y8M6xUyKhybVYNA01SgTBKsf\nG9YpZlI8NKkGg6apRpkgWP3YsE4xk+KhSTUYNE01ygTB6seGdYqZFA9NqsGgaapRJghWPzas\nU8ykeGhSDQZNU40yQbD6sWGdYibFQ5NqMGiaapQJgtWPDesUMykemlSDQdNUo0w0Ddb0ji6n\ny20E64t1ipkUD02qwaBpqukTrOn6ZXn58f2PMzGO3hHaQ8yIAV6I5sE6KVjv/zfp3piG947Q\nHjb8hyVmUjw0qQaDpqlGmWj8kvB6cXttuLgzZkp6R2gPG9YpZlI8NKkGg6applOwrslSpgjW\nk3WKmRQPTarBoGmqUSaa/5VwWmSKl4TP1ilmUjw0qQaDpqlGmbB4051gldYpZlI8NKkGg6ap\npk+w+FjDPRvWKWZSPDSpBoOmqaZTsL4kZkp6R2gPG9YpZlI8NKkGg6apRpkgWP3YsE4xk+Kh\nSTUYNE01ygTB6seGdYqZFA9NqsGgaapRJghWPzasU8ykeGhSDQZNU40yQbD6sWGdYibFQ5Nq\nMGiaapQJgtWPDesUMykemlSDQdNUo0wQrH5sWKeYSfHQpBoMmqYaZYJg9WPDOsVMiocm1WDQ\nNNUoEwSrH6kzIcEAAAvSSURBVBvWKWZSPDSpBoOmqUaZIFj92LBOMZPioUk1GDRNNcoEwerH\nhnWKmRQPTarBoGmqUSYIVj82rFPMpHhoUg0GTVONMkGw+rFhnWImxUOTajBommqUiWzBcp/5\nF9akGgyaphplgmChIVhozDXKBMFCQ7DQmGuUCYKFhmChMdcoEwQLDcFCY65RJrIFy/XvfXW4\n7yNDC5qMGmWCYBEsgoXGXKNMECyCRbDQmGuUCYJFsAgWGnONMkGwCBbBQmOuUSYIFsEiWGjM\nNcoEwSJYBAuNuUaZIFgEi2ChMdcoEwSLYBEsNOYaZYJgESyChcZco0wQLIJFsNCYa5QJgkWw\nCBYac40yQbAIFsFCY65RJggWwSJYaMw1ygTBIlgEC425RpkgWASLYKEx1ygTBItgESw05hpl\n4liwptUb36n4XTFTQrBsNakGg6apRpmoD1YhSpdYfVWswn0xU0KwbDWpBoOmqUaZiA/Wl/d+\ncVfMlBAsW02qwaBpqlEmtgdrOl1f6l1e8n18na71mW4vBJc1Wt403X7B/IPT7UcJVgn3fWRo\nQZNRUxWsae6OLhWs5a23wj38wOIXTLeHnr/+OLP5H/IlfYIV828HgCfseYZ1Uo9uwTqtZOzh\nB9Yub79Mj49peJ9gxfzbP+P+Hz5DC5qMmmPBur4mvA/WpFsJVhTu+8jQgiaj5lCw7kt1fTk4\nLR+1MVi3V5QEq4D7PjK0oMmoORKsx+7cB2vxV0KCdRj3fWRoQZNRczRY07I791/0Oay1HyBY\nu3DfR4YWNBk1R4J1unwsYb64xef2+fbbd5M+B3H/18JbqBa9IlgruO8jQwuajJqaYH0HBOtr\n3PeRoQVNRo0yQbAIFsFCY65RJnoG6/7/RzpmSgiWrSbVYNA01SgT/M/LECyChcZco0wQLIJF\nsNCYa5QJgkWwCBYac40yQbAIFsFCY65RJggWwSJYaMw1ygTBIlgEC425RpkgWASLYKEx1ygT\nBItgESw05hplgmARLIKFxlyjTBAsgkWw0JhrlAmCRbAIFhpzjTKRLVjuM//CmlSDQdNUo0wQ\nLDQEC425RpkgWGgIFhpzjTJBsNAQLDTmGmUiW7AavS/uvsCOmlSDQdNUo0wQrCrcF9hRk2ow\naJpqlAmCVYX7AjtqUg0GTVONMkGwqnBfYEdNqsGgaapRJghWFe4L7KhJNRg0TTXKBMGqwn2B\nHTWpBoOmqUaZIFhVuC+woybVYNA01SgTBKsK9wV21KQaDJqmGmWCYFXhvsCOmlSDQdNUo0wQ\nrCrcF9hRk2owaJpqlAmCVYX7AjtqUg0GTVONMkGwqnBfYEdNqsGgaapRJghWFe4L7KhJNRg0\nTTXKBMGqwn2BHTWpBoOmqUaZIFhVuC+woybVYNA01SgTBKsK9wV21KQaDJqmGmWCYFXhvsCO\nmlSDQdNUo0wQrCrcF9hRk2owaJpqlAmCVYX7AjtqUg0GTVONMkGwqnBfYEdNqsGgaapRJtoE\nazpN0/vX6fz18yXBeglNqsGgaappHqzpo1qXL4+Xp9OPMzGmYrBifj0A9KTVM6zFd9PyRt0T\n03CeYdlqUg0GTVNNp2BdXgPegjXdvSaMmRKCZatJNRg0TTV9gnV9XXj3DEvETAnBstWkGgya\nppouwbp774pgvZwm1WDQNNV0C9a0+qY7wXoFTarBoGmq6RKs85tW12rxsYYX1KQaDJqmmtbB\n2kLMlBAsW02qwaBpqlEmCFYV7gvsqEk1GDRNNcoEwarCfYEdNakGg6apRpkgWFW4L7CjJtVg\n0DTVKBMEqwr3BXbUpBoMmqYaZYJgVeG+wI6aVINB01SjTBCsKtwX2FGTajBommqUCYJVhfsC\nO2pSDQZNU40yQbCqcF9gR02qwaBpqlEmCFYV7gvsqEk1GDRNNcoEwarCfYEdNakGg6apRpkg\nWFW4L7CjJtVg0DTVKBMEqwr3BXbUpBoMmqYaZYJgVeG+wI6aVINB01SjTGQLlvvMv7Am1WDQ\nNNUoEwQLDcFCY65RJggWGoKFxlyjTBAsNAQLjblGmRg3WOvvrbvP/AtrUg0GTVONMkGwqkDj\nakGTUaNMEKwq0Lha0GTUKBMEqwo0rhY0GTXKBMGqAo2rBU1GjTJBsKpA42pBk1GjTBCsKtC4\nWtBk1CgTBKsKNK4WNBk1ygTBqgKNqwVNRo0yQbCqQONqQZNRo0wQrCrQuFrQZNQoEwSrCjSu\nFjQZNcoEwaoCjasFTUaNMkGwqkDjakGTUaNMEKwq0Lha0GTUKBMEqwo0rhY0GTXKBMGqAo2r\nBU1GjTLxvcGaJn377LF7x0CwBtOkGgyaphpl4luDNRW+X7117xgI1mCaVINB01SjTBCsKtC4\nWtBk1CgT3xms6Z33JE0fLwynS5zmL7e7bsnaOwaCNZgm1WDQNNW0CdZHna61ugbr7us0P8P6\ncWbvLy8ECwDy8v3Bun4zKVvT3a0ze6PLM6zBNKkGg6aphmAdBI2rBU1GTa9g3V4PEqzX06Qa\nDJqmGoJ1EDSuFjQZNX2Cdc3T/N1EsF5Kk2owaJpqugfr+rGGEx9reB1NqsGgaappFKwn3H+W\ndO8YCNZgmlSDQdNUo0x0CpY+RHpj7xgI1mCaVINB01SjTPR6hrX8jPuFvWMgWINpUg0GTVON\nMsH/vEwVaFwtaDJqlAmCVQUaVwuajBplgmBVgcbVgiajRpkgWFWgcbWgyahRJghWFWhcLWgy\napQJglUFGlcLmowaZYJgVYHG1YImo0aZIFhVoHG1oMmoUSYIVhVoXC1oMmqUCYJVBRpXC5qM\nGmWCYFWBxtWCJqNGmRg3WMFTgiaHBU1GjTJBsNAQLDTmGmWCYKEhWGjMNcoEwUJDsNCYa5QJ\ngoWGYKEx1ygTBAsNwUJjrlEmCBYagoXGXKNM+AQrhh+9/wGhpBpNqsEwmk4QLGdSjSbVYBhN\nJwiWM6lGk2owjKYTBMuZVKNJNRhG04lswQKAxBAsABgGggUAw0CwAGAYCBYADAPBAoBhyBWs\n6Z3e/4Y4pjzDuQwjy3g0mvGHMw9ilMGkCtZ0+5KCRCPRyiQY1PVkJxiJ1mSYtSFYvqQZyXTK\nFKzpRLD6QbBsSTOQ0ylVsBIN5ArB6sQws76JHO+RXMgYrDzLQ7A6McysbyLTaDIG6/ZldKbT\nQKMhWN4kGU2uIz6tfDcwBKsXw8z6dpKMhmD5MtbaECxbMo1mrEPxjFSjWfzJc4TRECxbbs/U\nE5DqiGsgCQYzLS5GGE6qYA3zcd1tJBpN1k+6d/6HHGea/9g5ymhyBQsAUkOwAGAYCBYADAPB\nAoBhIFgAMAwECwCGgWABwDAQLAAYBoIFAMNAsKAdbzu223+G+OA1NIZgQTv2BGvPY+FlYFdA\nOwgWHIRdAe04R+j9/35/+/3067e33/+5Xvv563znrz/e3v749fGov6efb28fxfrf729v058f\nN/76/fLd6fbNP+ef+KfbaKADBAvacQnWe4Pe/vrt/csf52vv0Xmb3qvzz/R2/e7t7efHre8P\n/u/bB3+eb5yu310e+Pv7r/v45rfeg4KWECxoxyVYf5z+Opfnr8u1n/+cfp479Ofbz9Plu48s\nXV4S/vb21+n09+2B/3mbzg/84/S/803/vvzYf3qPChpCsKAdl/L8On/5Z7729/trvPPTpN/O\nt3989/GI+T2sX//998/bj33c+Nvb9VXgbx+P+HiuBa8CwYJ2XN/DWny5ZKn03fkp19vlxeHj\n3R9X5jvhZWC1oR27g/XH22//+e8vggUzrDa0Yy1YHy8Ef96/JDwtH3L65z5YDy8J4aVgzaEd\na8H6efrn59u/7990P+kh/zvffResP98f8/f8zemv84/By0CwoB2rwTp/mOF0/7GGy2M//iL4\n+T2sX/OnGS4/cX7XHl4GggXtWH1J+PPycdHlB0fPVz8+wnB6v+3n/x7e2Pr75/WBvz7u7TMU\n6APBgp7wljnsgv0CPSFYsAv2C/SEYMEu2C/QE4IFu2C/AMAwECwAGAaCBQDDQLAAYBgIFgAM\nA8ECgGEgWAAwDAQLAIbh/wGuXAL0ZXG8HwAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Hint: columns of character data type should be converted into factor before traning and testing the model\n",
    "\n",
    "# read the dataset\n",
    "adult <- read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', col_names = FALSE)\n",
    "\n",
    "# rename the columns\n",
    "names(adult) <- c('age', 'workclass', 'fnlwgt', 'education', 'educationNum', 'maritalStatus', 'occupation', 'relationship', 'race', 'sex', 'capitalGain', 'capitalLoss', 'hoursPerWeek', 'nativeCountry', 'income')\n",
    "\n",
    "# convert string columns into facto\n",
    "adult$workclass <- as.factor(adult$workclass)\n",
    "adult$education <- as.factor(adult$education)\n",
    "adult$maritalStatus <- as.factor(adult$maritalStatus)\n",
    "adult$occupation <- as.factor(adult$occupation)\n",
    "adult$relationship <- as.factor(adult$relationship)\n",
    "adult$race <- as.factor(adult$race)\n",
    "adult$sex <- as.factor(adult$sex)\n",
    "adult$nativeCountry <- as.factor(adult$nativeCountry)\n",
    "adult$income <- as.factor(adult$income)\n",
    "\n",
    "# train a random forest to predict income\n",
    "adult_rf <- randomForest(income ~ ., data = adult, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the features importance\n",
    "imp <- importance(adult_rf, type=1)\n",
    "featureImportance <- tibble(Feature=row.names(imp), Importance=imp[,1])\n",
    "\n",
    "# show the importance scores\n",
    "featureImportance\n",
    "\n",
    "# visualizing the importance scores\n",
    "ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +\n",
    "    geom_bar(stat=\"identity\", fill='darkblue') +\n",
    "    coord_flip() +\n",
    "    xlab(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "[Unsupervised machine learning](https://en.wikipedia.org/wiki/Unsupervised_learning) is the machine learning task of uncovering the hidden structure from \"unlabeled\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Clustering**\n",
    "    \n",
    "    kmeans_model <- kmeans(x=X, centers=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise, we will use movies data on the [MovieLens 100K Dataset](http://files.grouplens.org/datasets/movielens/ml-100k/u.item) collected from the [MovieLens web site](http://movielens.org). It is available as `data/movies.txt` inside the directory of this lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m1682\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m20\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \"|\"\n",
      "\u001b[31mchr\u001b[39m  (1): Title\n",
      "\u001b[32mdbl\u001b[39m (19): unknown, Action, Adventure, Animation, Children's, Comedy, Crime, ...\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    }
   ],
   "source": [
    "movies <- read_delim('movies.txt', delim = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 20</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Title</th><th scope=col>unknown</th><th scope=col>Action</th><th scope=col>Adventure</th><th scope=col>Animation</th><th scope=col>Children's</th><th scope=col>Comedy</th><th scope=col>Crime</th><th scope=col>Documentary</th><th scope=col>Drama</th><th scope=col>Fantasy</th><th scope=col>Film-Noir</th><th scope=col>Horror</th><th scope=col>Musical</th><th scope=col>Mystery</th><th scope=col>Romance</th><th scope=col>Sci-Fi</th><th scope=col>Thriller</th><th scope=col>War</th><th scope=col>Western</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Toy Story (1995)                                    </td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>GoldenEye (1995)                                    </td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>Four Rooms (1995)                                   </td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>Get Shorty (1995)                                   </td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>Copycat (1995)                                      </td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 20\n",
       "\\begin{tabular}{llllllllllllllllllll}\n",
       " Title & unknown & Action & Adventure & Animation & Children's & Comedy & Crime & Documentary & Drama & Fantasy & Film-Noir & Horror & Musical & Mystery & Romance & Sci-Fi & Thriller & War & Western\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t Toy Story (1995)                                     & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t GoldenEye (1995)                                     & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t Four Rooms (1995)                                    & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t Get Shorty (1995)                                    & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t Copycat (1995)                                       & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 20\n",
       "\n",
       "| Title &lt;chr&gt; | unknown &lt;dbl&gt; | Action &lt;dbl&gt; | Adventure &lt;dbl&gt; | Animation &lt;dbl&gt; | Children's &lt;dbl&gt; | Comedy &lt;dbl&gt; | Crime &lt;dbl&gt; | Documentary &lt;dbl&gt; | Drama &lt;dbl&gt; | Fantasy &lt;dbl&gt; | Film-Noir &lt;dbl&gt; | Horror &lt;dbl&gt; | Musical &lt;dbl&gt; | Mystery &lt;dbl&gt; | Romance &lt;dbl&gt; | Sci-Fi &lt;dbl&gt; | Thriller &lt;dbl&gt; | War &lt;dbl&gt; | Western &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Toy Story (1995)                                     | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| GoldenEye (1995)                                     | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
       "| Four Rooms (1995)                                    | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
       "| Get Shorty (1995)                                    | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| Copycat (1995)                                       | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
       "| Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  Title                                                unknown Action Adventure\n",
       "1 Toy Story (1995)                                     0       0      0        \n",
       "2 GoldenEye (1995)                                     0       1      1        \n",
       "3 Four Rooms (1995)                                    0       0      0        \n",
       "4 Get Shorty (1995)                                    0       1      0        \n",
       "5 Copycat (1995)                                       0       0      0        \n",
       "6 Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) 0       0      0        \n",
       "  Animation Children's Comedy Crime Documentary Drama Fantasy Film-Noir Horror\n",
       "1 1         1          1      0     0           0     0       0         0     \n",
       "2 0         0          0      0     0           0     0       0         0     \n",
       "3 0         0          0      0     0           0     0       0         0     \n",
       "4 0         0          1      0     0           1     0       0         0     \n",
       "5 0         0          0      1     0           1     0       0         0     \n",
       "6 0         0          0      0     0           1     0       0         0     \n",
       "  Musical Mystery Romance Sci-Fi Thriller War Western\n",
       "1 0       0       0       0      0        0   0      \n",
       "2 0       0       0       0      1        0   0      \n",
       "3 0       0       0       0      1        0   0      \n",
       "4 0       0       0       0      0        0   0      \n",
       "5 0       0       0       0      1        0   0      \n",
       "6 0       0       0       0      0        0   0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>502</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 1\n",
       "\\begin{tabular}{l}\n",
       " n\\\\\n",
       " <int>\\\\\n",
       "\\hline\n",
       "\t 502\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 1\n",
       "\n",
       "| n &lt;int&gt; |\n",
       "|---|\n",
       "| 502 |\n",
       "\n"
      ],
      "text/plain": [
       "  n  \n",
       "1 502"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>97</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 1\n",
       "\\begin{tabular}{l}\n",
       " n\\\\\n",
       " <int>\\\\\n",
       "\\hline\n",
       "\t 97\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 1\n",
       "\n",
       "| n &lt;int&gt; |\n",
       "|---|\n",
       "| 97 |\n",
       "\n"
      ],
      "text/plain": [
       "  n \n",
       "1 97"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspecting and preprocessing\n",
    "\n",
    "# check top records\n",
    "head(movies)\n",
    "\n",
    "# remove duplicates\n",
    "movies <- distinct(movies)\n",
    "\n",
    "# Mow many movies are tagged as Comedy\n",
    "filter(movies, Comedy == 1) %>% count()\n",
    "\n",
    "# How many movies are tagged as Romance and Drama?\n",
    "filter(movies, Romance == 1 & Drama == 1) %>% count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a k-means cluster\n",
    "k = 5\n",
    "iters = 1000\n",
    "set.seed(1)\n",
    "\n",
    "movies <- select(movies, -Title)\n",
    "movie_kmeans <- kmeans(movies, centers = k, iter.max=iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Clustering Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 9\n",
      " $ cluster     : int [1:1664] 1 2 2 1 3 5 5 1 5 5 ...\n",
      " $ centers     : num [1:5, 1:19] 0 0 0 0 0.00291 ...\n",
      "  ..- attr(*, \"dimnames\")=List of 2\n",
      "  .. ..$ : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n",
      "  .. ..$ : chr [1:19] \"unknown\" \"Action\" \"Adventure\" \"Animation\" ...\n",
      " $ totss       : num 2246\n",
      " $ withinss    : num [1:5] 216 441 124 102 491\n",
      " $ tot.withinss: num 1374\n",
      " $ betweenss   : num 872\n",
      " $ size        : int [1:5] 395 313 121 147 688\n",
      " $ iter        : int 2\n",
      " $ ifault      : int 0\n",
      " - attr(*, \"class\")= chr \"kmeans\"\n"
     ]
    }
   ],
   "source": [
    "# view clustering output\n",
    "str(movie_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1</li><li>2</li><li>2</li><li>1</li><li>3</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>3</li><li>3</li><li>1</li><li>5</li><li>5</li><li>4</li><li>2</li><li>5</li><li>5</li><li>5</li><li>2</li><li>5</li><li>3</li><li>2</li><li>1</li><li>1</li><li>2</li><li>2</li><li>1</li><li>5</li><li>3</li><li>5</li><li>2</li><li>1</li><li>5</li><li>5</li><li>5</li><li>2</li><li>2</li><li>1</li><li>1</li><li>1</li><li>3</li><li>3</li><li>1</li><li>5</li><li>1</li><li>5</li><li>4</li><li>2</li><li>5</li><li>5</li><li>2</li><li>2</li><li>3</li><li>3</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>2</li><li>1</li><li>5</li><li>1</li><li>4</li><li>1</li><li>2</li><li>4</li><li>4</li><li>5</li><li>1</li><li>1</li><li>1</li><li>5</li><li>3</li><li>3</li><li>5</li><li>2</li><li>1</li><li>4</li><li>2</li><li>4</li><li>2</li><li>1</li><li>5</li><li>5</li><li>4</li><li>2</li><li>4</li><li>1</li><li>4</li><li>1</li><li>1</li><li>1</li><li>2</li><li>5</li><li>3</li><li>5</li><li>3</li><li>2</li><li>5</li><li>5</li><li>1</li><li>1</li><li>3</li><li>5</li><li>1</li><li>1</li><li>1</li><li>4</li><li>2</li><li>5</li><li>5</li><li>5</li><li>1</li><li>2</li><li>2</li><li>5</li><li>1</li><li>2</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>3</li><li>2</li><li>3</li><li>3</li><li>5</li><li>5</li><li>5</li><li>5</li><li>3</li><li>5</li><li>5</li><li>1</li><li>1</li><li>2</li><li>2</li><li>2</li><li>5</li><li>2</li><li>2</li><li>5</li><li>2</li><li>2</li><li>5</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>4</li><li>3</li><li>5</li><li>1</li><li>2</li><li>5</li><li>4</li><li>5</li><li>1</li><li>2</li><li>5</li><li>5</li><li>1</li><li>1</li><li>1</li><li>4</li><li>1</li><li>2</li><li>4</li><li>2</li><li>2</li><li>2</li><li>2</li><li>5</li><li>2</li><li>5</li><li>2</li><li>3</li><li>2</li><li>2</li><li>4</li><li>1</li><li>3</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>5</li><li>1</li><li>2</li><li>5</li><li>5</li><li>2</li><li>5</li><li>5</li><li>⋯</li><li>1</li><li>5</li><li>2</li><li>3</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>4</li><li>5</li><li>1</li><li>1</li><li>2</li><li>1</li><li>4</li><li>1</li><li>5</li><li>1</li><li>5</li><li>1</li><li>1</li><li>1</li><li>5</li><li>4</li><li>2</li><li>1</li><li>3</li><li>5</li><li>1</li><li>3</li><li>1</li><li>3</li><li>1</li><li>5</li><li>1</li><li>5</li><li>4</li><li>5</li><li>5</li><li>5</li><li>3</li><li>5</li><li>4</li><li>2</li><li>2</li><li>5</li><li>4</li><li>3</li><li>1</li><li>5</li><li>5</li><li>5</li><li>2</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>2</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>2</li><li>2</li><li>1</li><li>5</li><li>2</li><li>2</li><li>5</li><li>2</li><li>2</li><li>1</li><li>5</li><li>2</li><li>3</li><li>5</li><li>5</li><li>4</li><li>5</li><li>5</li><li>5</li><li>1</li><li>4</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>2</li><li>4</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>3</li><li>5</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>2</li><li>2</li><li>5</li><li>3</li><li>5</li><li>1</li><li>2</li><li>5</li><li>5</li><li>3</li><li>1</li><li>5</li><li>1</li><li>4</li><li>1</li><li>4</li><li>5</li><li>1</li><li>2</li><li>5</li><li>2</li><li>5</li><li>1</li><li>2</li><li>1</li><li>5</li><li>2</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>5</li><li>3</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>3</li><li>5</li><li>1</li><li>3</li><li>4</li><li>5</li><li>4</li><li>4</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>1</li><li>3</li><li>1</li><li>5</li><li>5</li><li>2</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>4</li><li>1</li><li>5</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item ⋯\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 2\n",
       "3. 2\n",
       "4. 1\n",
       "5. 3\n",
       "6. 5\n",
       "7. 5\n",
       "8. 1\n",
       "9. 5\n",
       "10. 5\n",
       "11. 3\n",
       "12. 3\n",
       "13. 1\n",
       "14. 5\n",
       "15. 5\n",
       "16. 4\n",
       "17. 2\n",
       "18. 5\n",
       "19. 5\n",
       "20. 5\n",
       "21. 2\n",
       "22. 5\n",
       "23. 3\n",
       "24. 2\n",
       "25. 1\n",
       "26. 1\n",
       "27. 2\n",
       "28. 2\n",
       "29. 1\n",
       "30. 5\n",
       "31. 3\n",
       "32. 5\n",
       "33. 2\n",
       "34. 1\n",
       "35. 5\n",
       "36. 5\n",
       "37. 5\n",
       "38. 2\n",
       "39. 2\n",
       "40. 1\n",
       "41. 1\n",
       "42. 1\n",
       "43. 3\n",
       "44. 3\n",
       "45. 1\n",
       "46. 5\n",
       "47. 1\n",
       "48. 5\n",
       "49. 4\n",
       "50. 2\n",
       "51. 5\n",
       "52. 5\n",
       "53. 2\n",
       "54. 2\n",
       "55. 3\n",
       "56. 3\n",
       "57. 5\n",
       "58. 5\n",
       "59. 5\n",
       "60. 5\n",
       "61. 5\n",
       "62. 2\n",
       "63. 1\n",
       "64. 5\n",
       "65. 1\n",
       "66. 4\n",
       "67. 1\n",
       "68. 2\n",
       "69. 4\n",
       "70. 4\n",
       "71. 5\n",
       "72. 1\n",
       "73. 1\n",
       "74. 1\n",
       "75. 5\n",
       "76. 3\n",
       "77. 3\n",
       "78. 5\n",
       "79. 2\n",
       "80. 1\n",
       "81. 4\n",
       "82. 2\n",
       "83. 4\n",
       "84. 2\n",
       "85. 1\n",
       "86. 5\n",
       "87. 5\n",
       "88. 4\n",
       "89. 2\n",
       "90. 4\n",
       "91. 1\n",
       "92. 4\n",
       "93. 1\n",
       "94. 1\n",
       "95. 1\n",
       "96. 2\n",
       "97. 5\n",
       "98. 3\n",
       "99. 5\n",
       "100. 3\n",
       "101. 2\n",
       "102. 5\n",
       "103. 5\n",
       "104. 1\n",
       "105. 1\n",
       "106. 3\n",
       "107. 5\n",
       "108. 1\n",
       "109. 1\n",
       "110. 1\n",
       "111. 4\n",
       "112. 2\n",
       "113. 5\n",
       "114. 5\n",
       "115. 5\n",
       "116. 1\n",
       "117. 2\n",
       "118. 2\n",
       "119. 5\n",
       "120. 1\n",
       "121. 2\n",
       "122. 1\n",
       "123. 1\n",
       "124. 5\n",
       "125. 5\n",
       "126. 5\n",
       "127. 3\n",
       "128. 2\n",
       "129. 3\n",
       "130. 3\n",
       "131. 5\n",
       "132. 5\n",
       "133. 5\n",
       "134. 5\n",
       "135. 3\n",
       "136. 5\n",
       "137. 5\n",
       "138. 1\n",
       "139. 1\n",
       "140. 2\n",
       "141. 2\n",
       "142. 2\n",
       "143. 5\n",
       "144. 2\n",
       "145. 2\n",
       "146. 5\n",
       "147. 2\n",
       "148. 2\n",
       "149. 5\n",
       "150. 1\n",
       "151. 1\n",
       "152. 1\n",
       "153. 1\n",
       "154. 1\n",
       "155. 4\n",
       "156. 3\n",
       "157. 5\n",
       "158. 1\n",
       "159. 2\n",
       "160. 5\n",
       "161. 4\n",
       "162. 5\n",
       "163. 1\n",
       "164. 2\n",
       "165. 5\n",
       "166. 5\n",
       "167. 1\n",
       "168. 1\n",
       "169. 1\n",
       "170. 4\n",
       "171. 1\n",
       "172. 2\n",
       "173. 4\n",
       "174. 2\n",
       "175. 2\n",
       "176. 2\n",
       "177. 2\n",
       "178. 5\n",
       "179. 2\n",
       "180. 5\n",
       "181. 2\n",
       "182. 3\n",
       "183. 2\n",
       "184. 2\n",
       "185. 4\n",
       "186. 1\n",
       "187. 3\n",
       "188. 5\n",
       "189. 1\n",
       "190. 5\n",
       "191. 5\n",
       "192. 5\n",
       "193. 5\n",
       "194. 1\n",
       "195. 2\n",
       "196. 5\n",
       "197. 5\n",
       "198. 2\n",
       "199. 5\n",
       "200. 5\n",
       "201. ⋯\n",
       "202. 1\n",
       "203. 5\n",
       "204. 2\n",
       "205. 3\n",
       "206. 1\n",
       "207. 1\n",
       "208. 5\n",
       "209. 5\n",
       "210. 5\n",
       "211. 1\n",
       "212. 5\n",
       "213. 5\n",
       "214. 4\n",
       "215. 5\n",
       "216. 1\n",
       "217. 1\n",
       "218. 2\n",
       "219. 1\n",
       "220. 4\n",
       "221. 1\n",
       "222. 5\n",
       "223. 1\n",
       "224. 5\n",
       "225. 1\n",
       "226. 1\n",
       "227. 1\n",
       "228. 5\n",
       "229. 4\n",
       "230. 2\n",
       "231. 1\n",
       "232. 3\n",
       "233. 5\n",
       "234. 1\n",
       "235. 3\n",
       "236. 1\n",
       "237. 3\n",
       "238. 1\n",
       "239. 5\n",
       "240. 1\n",
       "241. 5\n",
       "242. 4\n",
       "243. 5\n",
       "244. 5\n",
       "245. 5\n",
       "246. 3\n",
       "247. 5\n",
       "248. 4\n",
       "249. 2\n",
       "250. 2\n",
       "251. 5\n",
       "252. 4\n",
       "253. 3\n",
       "254. 1\n",
       "255. 5\n",
       "256. 5\n",
       "257. 5\n",
       "258. 2\n",
       "259. 5\n",
       "260. 5\n",
       "261. 1\n",
       "262. 5\n",
       "263. 5\n",
       "264. 1\n",
       "265. 5\n",
       "266. 5\n",
       "267. 2\n",
       "268. 5\n",
       "269. 5\n",
       "270. 5\n",
       "271. 1\n",
       "272. 5\n",
       "273. 5\n",
       "274. 5\n",
       "275. 2\n",
       "276. 2\n",
       "277. 1\n",
       "278. 5\n",
       "279. 2\n",
       "280. 2\n",
       "281. 5\n",
       "282. 2\n",
       "283. 2\n",
       "284. 1\n",
       "285. 5\n",
       "286. 2\n",
       "287. 3\n",
       "288. 5\n",
       "289. 5\n",
       "290. 4\n",
       "291. 5\n",
       "292. 5\n",
       "293. 5\n",
       "294. 1\n",
       "295. 4\n",
       "296. 1\n",
       "297. 1\n",
       "298. 5\n",
       "299. 5\n",
       "300. 5\n",
       "301. 5\n",
       "302. 5\n",
       "303. 5\n",
       "304. 5\n",
       "305. 5\n",
       "306. 2\n",
       "307. 4\n",
       "308. 5\n",
       "309. 5\n",
       "310. 5\n",
       "311. 5\n",
       "312. 5\n",
       "313. 3\n",
       "314. 5\n",
       "315. 1\n",
       "316. 1\n",
       "317. 5\n",
       "318. 5\n",
       "319. 5\n",
       "320. 1\n",
       "321. 5\n",
       "322. 2\n",
       "323. 2\n",
       "324. 5\n",
       "325. 3\n",
       "326. 5\n",
       "327. 1\n",
       "328. 2\n",
       "329. 5\n",
       "330. 5\n",
       "331. 3\n",
       "332. 1\n",
       "333. 5\n",
       "334. 1\n",
       "335. 4\n",
       "336. 1\n",
       "337. 4\n",
       "338. 5\n",
       "339. 1\n",
       "340. 2\n",
       "341. 5\n",
       "342. 2\n",
       "343. 5\n",
       "344. 1\n",
       "345. 2\n",
       "346. 1\n",
       "347. 5\n",
       "348. 2\n",
       "349. 1\n",
       "350. 1\n",
       "351. 5\n",
       "352. 5\n",
       "353. 5\n",
       "354. 1\n",
       "355. 5\n",
       "356. 1\n",
       "357. 5\n",
       "358. 5\n",
       "359. 5\n",
       "360. 5\n",
       "361. 3\n",
       "362. 5\n",
       "363. 5\n",
       "364. 5\n",
       "365. 5\n",
       "366. 5\n",
       "367. 1\n",
       "368. 5\n",
       "369. 5\n",
       "370. 3\n",
       "371. 5\n",
       "372. 1\n",
       "373. 3\n",
       "374. 4\n",
       "375. 5\n",
       "376. 4\n",
       "377. 4\n",
       "378. 5\n",
       "379. 1\n",
       "380. 5\n",
       "381. 5\n",
       "382. 5\n",
       "383. 5\n",
       "384. 1\n",
       "385. 5\n",
       "386. 5\n",
       "387. 5\n",
       "388. 1\n",
       "389. 3\n",
       "390. 1\n",
       "391. 5\n",
       "392. 5\n",
       "393. 2\n",
       "394. 5\n",
       "395. 5\n",
       "396. 5\n",
       "397. 5\n",
       "398. 5\n",
       "399. 4\n",
       "400. 1\n",
       "401. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   [1] 1 2 2 1 3 5 5 1 5 5 3 3 1 5 5 4 2 5 5 5 2 5 3 2 1 1 2 2 1 5 3 5 2 1 5 5 5\n",
       "  [38] 2 2 1 1 1 3 3 1 5 1 5 4 2 5 5 2 2 3 3 5 5 5 5 5 2 1 5 1 4 1 2 4 4 5 1 1 1\n",
       "  [75] 5 3 3 5 2 1 4 2 4 2 1 5 5 4 2 4 1 4 1 1 1 2 5 3 5 3 2 5 5 1 1 3 5 1 1 1 4\n",
       " [112] 2 5 5 5 1 2 2 5 1 2 1 1 5 5 5 3 2 3 3 5 5 5 5 3 5 5 1 1 2 2 2 5 2 2 5 2 2\n",
       " [149] 5 1 1 1 1 1 4 3 5 1 2 5 4 5 1 2 5 5 1 1 1 4 1 2 4 2 2 2 2 5 2 5 2 3 2 2 4\n",
       " [186] 1 3 5 1 5 5 5 5 1 2 5 5 2 5 5 2 4 5 1 5 2 5 1 1 2 1 5 5 5 5 4 4 2 5 4 5 2\n",
       " [223] 3 5 1 2 2 2 2 2 1 1 2 2 1 1 5 1 3 1 4 1 1 2 2 5 2 1 1 2 1 2 5 2 4 4 2 5 1\n",
       " [260] 2 1 5 2 2 2 2 5 1 3 2 5 2 4 5 5 5 5 5 5 2 5 5 4 5 5 5 2 5 1 2 5 3 1 2 5 5\n",
       " [297] 2 3 2 1 3 2 5 5 3 5 3 5 3 3 5 2 3 1 5 5 4 5 1 2 2 5 3 5 3 2 3 5 2 3 2 2 1\n",
       " [334] 3 1 1 5 5 1 1 2 5 1 3 1 2 2 5 1 2 4 2 3 5 2 2 5 3 1 2 1 5 5 1 1 1 3 5 1 2\n",
       " [371] 2 5 1 1 5 5 2 4 1 1 1 4 1 5 1 2 1 1 5 1 4 1 1 2 2 1 1 1 4 3 5 2 2 1 1 1 1\n",
       " [408] 4 1 5 1 1 5 5 5 1 5 5 1 5 5 1 2 5 1 5 1 2 5 1 2 1 5 5 5 5 5 5 5 5 2 5 5 5\n",
       " [445] 5 2 2 4 2 2 5 2 1 5 5 5 5 5 5 2 5 4 2 5 5 5 5 5 2 5 2 5 1 1 4 2 1 1 1 5 5\n",
       " [482] 4 4 4 5 4 4 2 5 5 1 1 5 1 2 5 5 1 5 3 2 5 5 5 5 5 2 4 2 4 5 1 4 5 2 2 5 1\n",
       " [519] 1 1 5 2 5 5 5 2 5 4 5 5 4 5 5 5 1 2 2 4 5 3 4 2 3 5 5 2 5 2 5 2 5 5 5 3 5\n",
       " [556] 4 5 2 5 1 2 2 5 2 5 5 1 2 2 4 1 2 1 2 1 4 3 5 3 5 1 2 5 5 5 2 3 2 5 5 2 5\n",
       " [593] 2 1 2 5 2 4 2 1 5 5 4 4 1 5 3 5 1 5 2 2 5 5 3 5 5 2 1 5 5 5 5 3 1 1 5 5 5\n",
       " [630] 5 5 2 1 5 5 5 5 3 5 5 5 5 5 4 3 5 5 1 3 2 1 3 2 5 1 5 5 5 1 5 2 5 5 2 5 5\n",
       " [667] 5 2 5 5 5 5 3 2 5 2 1 2 2 5 1 1 2 5 2 4 5 4 1 3 5 5 5 1 5 4 5 5 4 5 5 5 4\n",
       " [704] 1 5 1 5 5 1 5 3 1 1 5 1 1 1 5 1 5 5 1 5 5 4 4 5 1 5 5 1 4 4 5 3 3 2 5 1 1\n",
       " [741] 4 2 4 5 2 2 5 3 2 1 5 2 2 2 2 5 1 4 4 5 5 2 2 3 2 5 2 5 4 5 5 4 2 1 4 5 4\n",
       " [778] 5 4 4 1 2 1 1 5 1 1 5 1 4 2 2 5 2 1 2 5 1 1 3 5 5 5 2 5 2 5 5 5 5 5 1 1 1\n",
       " [815] 4 5 3 1 2 2 2 2 2 2 2 5 2 2 4 4 5 2 2 5 2 1 1 3 1 5 5 2 4 5 5 3 1 1 2 1 5\n",
       " [852] 5 1 2 5 1 5 1 4 5 5 4 4 1 4 4 5 5 1 4 1 2 5 5 5 5 5 4 5 5 4 2 5 1 1 1 2 5\n",
       " [889] 2 5 5 5 1 3 5 1 5 5 1 1 5 5 3 2 1 3 5 2 2 4 2 5 5 5 5 5 2 1 5 5 1 2 2 1 5\n",
       " [926] 5 5 4 5 4 3 1 1 5 2 1 4 5 4 4 5 5 2 1 1 5 5 5 1 5 1 5 5 5 3 1 1 4 5 5 5 1\n",
       " [963] 1 5 5 3 2 2 2 2 3 5 5 2 2 2 5 2 2 4 5 5 3 1 1 5 1 1 1 1 1 1 1 1 5 5 5 1 5\n",
       "[1000] 5 5 3 1 2 1 5 2 5 5 2 2 5 5 1 4 2 1 2 4 1 1 2 1 2 2 1 1 4 2 5 4 4 2 1 1 5\n",
       "[1037] 2 1 4 1 5 1 1 5 1 5 5 1 5 1 2 1 5 5 3 5 5 1 5 3 1 1 4 2 1 5 2 2 5 1 1 3 5\n",
       "[1074] 3 5 5 5 2 2 2 2 2 1 1 1 1 4 5 1 5 4 5 4 1 3 2 3 5 5 3 1 5 5 5 1 4 2 5 1 5\n",
       "[1111] 1 5 3 5 4 2 5 5 5 5 5 2 5 2 5 5 5 5 3 2 1 5 5 3 5 5 5 5 5 5 5 3 4 5 2 4 3\n",
       "[1148] 5 5 2 5 5 1 5 5 1 1 1 5 5 1 5 1 5 3 4 5 1 1 1 1 1 1 1 5 1 1 3 1 5 4 3 5 5\n",
       "[1185] 3 5 5 1 3 1 5 5 1 4 1 5 1 2 3 1 2 5 5 3 5 2 4 2 1 4 5 5 2 5 5 1 3 5 2 3 1\n",
       "[1222] 5 5 5 1 3 5 1 5 2 2 1 1 5 2 3 1 5 2 1 2 1 5 2 1 5 5 4 4 5 5 4 5 5 5 5 5 5\n",
       "[1259] 4 1 1 1 3 2 2 5 2 5 2 3 5 5 1 5 5 4 1 1 4 5 1 5 2 5 1 1 5 1 5 5 1 1 2 2 1\n",
       "[1296] 5 5 1 5 5 1 1 2 2 5 5 5 5 5 5 1 1 5 3 5 5 5 5 5 5 5 5 1 5 5 1 1 5 1 4 5 5\n",
       "[1333] 5 5 5 5 5 5 1 5 1 5 4 5 5 1 4 2 5 1 1 2 5 2 5 5 5 5 5 3 1 2 5 1 5 1 4 5 4\n",
       "[1370] 5 5 5 2 5 4 2 4 5 5 3 5 5 2 4 5 5 5 5 2 5 5 5 1 1 5 5 2 1 5 5 2 5 2 2 2 2\n",
       "[1407] 5 5 2 1 5 2 5 4 1 1 5 1 5 5 2 5 5 4 1 5 1 5 3 5 5 5 5 1 5 1 5 5 5 4 2 1 3\n",
       "[1444] 5 5 1 1 4 5 1 1 3 1 5 1 5 5 5 2 5 2 1 5 4 1 5 2 3 1 1 5 5 5 1 5 5 4 5 1 1\n",
       "[1481] 2 1 4 1 5 1 5 1 1 1 5 4 2 1 3 5 1 3 1 3 1 5 1 5 4 5 5 5 3 5 4 2 2 5 4 3 1\n",
       "[1518] 5 5 5 2 5 5 1 5 5 1 5 5 2 5 5 5 1 5 5 5 2 2 1 5 2 2 5 2 2 1 5 2 3 5 5 4 5\n",
       "[1555] 5 5 1 4 1 1 5 5 5 5 5 5 5 5 2 4 5 5 5 5 5 3 5 1 1 5 5 5 1 5 2 2 5 3 5 1 2\n",
       "[1592] 5 5 3 1 5 1 4 1 4 5 1 2 5 2 5 1 2 1 5 2 1 1 5 5 5 1 5 1 5 5 5 5 3 5 5 5 5\n",
       "[1629] 5 1 5 5 3 5 1 3 4 5 4 4 5 1 5 5 5 5 1 5 5 5 1 3 1 5 5 2 5 5 5 5 5 4 1 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cluster vector, i.e., cluster index of each row\n",
    "movie_kmeans$cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 19 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>unknown</th><th scope=col>Action</th><th scope=col>Adventure</th><th scope=col>Animation</th><th scope=col>Children's</th><th scope=col>Comedy</th><th scope=col>Crime</th><th scope=col>Documentary</th><th scope=col>Drama</th><th scope=col>Fantasy</th><th scope=col>Film-Noir</th><th scope=col>Horror</th><th scope=col>Musical</th><th scope=col>Mystery</th><th scope=col>Romance</th><th scope=col>Sci-Fi</th><th scope=col>Thriller</th><th scope=col>War</th><th scope=col>Western</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0.000000000</td><td>0.05822785</td><td>0.027848101</td><td>0.02278481</td><td>0.09620253</td><td>1.00000000</td><td>0.02531646</td><td>0.002531646</td><td>0.18734177</td><td>0.012658228</td><td>0.000000000</td><td>0.027848101</td><td>0.035443038</td><td>0.01518987</td><td>0.00000000</td><td>0.022784810</td><td>0.01772152</td><td>0.030379747</td><td>0.017721519</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0.000000000</td><td>0.59744409</td><td>0.329073482</td><td>0.01916933</td><td>0.10543131</td><td>0.01597444</td><td>0.03833866</td><td>0.000000000</td><td>0.03833866</td><td>0.025559105</td><td>0.025559105</td><td>0.089456869</td><td>0.009584665</td><td>0.07987220</td><td>0.04153355</td><td>0.258785942</td><td>0.51757188</td><td>0.044728435</td><td>0.006389776</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.000000000</td><td>0.05785124</td><td>0.008264463</td><td>0.00000000</td><td>0.00000000</td><td>0.04132231</td><td>0.68595041</td><td>0.000000000</td><td>0.70247934</td><td>0.008264463</td><td>0.082644628</td><td>0.008264463</td><td>0.000000000</td><td>0.09090909</td><td>0.04958678</td><td>0.024793388</td><td>0.56198347</td><td>0.008264463</td><td>0.000000000</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0.000000000</td><td>0.04761905</td><td>0.054421769</td><td>0.01360544</td><td>0.02721088</td><td>0.65986395</td><td>0.01360544</td><td>0.000000000</td><td>0.07482993</td><td>0.013605442</td><td>0.006802721</td><td>0.013605442</td><td>0.081632653</td><td>0.02721088</td><td>1.00000000</td><td>0.013605442</td><td>0.07482993</td><td>0.034013605</td><td>0.000000000</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.002906977</td><td>0.03633721</td><td>0.014534884</td><td>0.03633721</td><td>0.06540698</td><td>0.00000000</td><td>0.00000000</td><td>0.071220930</td><td>0.77616279</td><td>0.008720930</td><td>0.007267442</td><td>0.069767442</td><td>0.039244186</td><td>0.02034884</td><td>0.11337209</td><td>0.007267442</td><td>0.00000000</td><td>0.056686047</td><td>0.026162791</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 19 of type dbl\n",
       "\\begin{tabular}{r|lllllllllllllllllll}\n",
       "  & unknown & Action & Adventure & Animation & Children's & Comedy & Crime & Documentary & Drama & Fantasy & Film-Noir & Horror & Musical & Mystery & Romance & Sci-Fi & Thriller & War & Western\\\\\n",
       "\\hline\n",
       "\t1 & 0.000000000 & 0.05822785 & 0.027848101 & 0.02278481 & 0.09620253 & 1.00000000 & 0.02531646 & 0.002531646 & 0.18734177 & 0.012658228 & 0.000000000 & 0.027848101 & 0.035443038 & 0.01518987 & 0.00000000 & 0.022784810 & 0.01772152 & 0.030379747 & 0.017721519\\\\\n",
       "\t2 & 0.000000000 & 0.59744409 & 0.329073482 & 0.01916933 & 0.10543131 & 0.01597444 & 0.03833866 & 0.000000000 & 0.03833866 & 0.025559105 & 0.025559105 & 0.089456869 & 0.009584665 & 0.07987220 & 0.04153355 & 0.258785942 & 0.51757188 & 0.044728435 & 0.006389776\\\\\n",
       "\t3 & 0.000000000 & 0.05785124 & 0.008264463 & 0.00000000 & 0.00000000 & 0.04132231 & 0.68595041 & 0.000000000 & 0.70247934 & 0.008264463 & 0.082644628 & 0.008264463 & 0.000000000 & 0.09090909 & 0.04958678 & 0.024793388 & 0.56198347 & 0.008264463 & 0.000000000\\\\\n",
       "\t4 & 0.000000000 & 0.04761905 & 0.054421769 & 0.01360544 & 0.02721088 & 0.65986395 & 0.01360544 & 0.000000000 & 0.07482993 & 0.013605442 & 0.006802721 & 0.013605442 & 0.081632653 & 0.02721088 & 1.00000000 & 0.013605442 & 0.07482993 & 0.034013605 & 0.000000000\\\\\n",
       "\t5 & 0.002906977 & 0.03633721 & 0.014534884 & 0.03633721 & 0.06540698 & 0.00000000 & 0.00000000 & 0.071220930 & 0.77616279 & 0.008720930 & 0.007267442 & 0.069767442 & 0.039244186 & 0.02034884 & 0.11337209 & 0.007267442 & 0.00000000 & 0.056686047 & 0.026162791\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 19 of type dbl\n",
       "\n",
       "| <!--/--> | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 0.000000000 | 0.05822785 | 0.027848101 | 0.02278481 | 0.09620253 | 1.00000000 | 0.02531646 | 0.002531646 | 0.18734177 | 0.012658228 | 0.000000000 | 0.027848101 | 0.035443038 | 0.01518987 | 0.00000000 | 0.022784810 | 0.01772152 | 0.030379747 | 0.017721519 |\n",
       "| 2 | 0.000000000 | 0.59744409 | 0.329073482 | 0.01916933 | 0.10543131 | 0.01597444 | 0.03833866 | 0.000000000 | 0.03833866 | 0.025559105 | 0.025559105 | 0.089456869 | 0.009584665 | 0.07987220 | 0.04153355 | 0.258785942 | 0.51757188 | 0.044728435 | 0.006389776 |\n",
       "| 3 | 0.000000000 | 0.05785124 | 0.008264463 | 0.00000000 | 0.00000000 | 0.04132231 | 0.68595041 | 0.000000000 | 0.70247934 | 0.008264463 | 0.082644628 | 0.008264463 | 0.000000000 | 0.09090909 | 0.04958678 | 0.024793388 | 0.56198347 | 0.008264463 | 0.000000000 |\n",
       "| 4 | 0.000000000 | 0.04761905 | 0.054421769 | 0.01360544 | 0.02721088 | 0.65986395 | 0.01360544 | 0.000000000 | 0.07482993 | 0.013605442 | 0.006802721 | 0.013605442 | 0.081632653 | 0.02721088 | 1.00000000 | 0.013605442 | 0.07482993 | 0.034013605 | 0.000000000 |\n",
       "| 5 | 0.002906977 | 0.03633721 | 0.014534884 | 0.03633721 | 0.06540698 | 0.00000000 | 0.00000000 | 0.071220930 | 0.77616279 | 0.008720930 | 0.007267442 | 0.069767442 | 0.039244186 | 0.02034884 | 0.11337209 | 0.007267442 | 0.00000000 | 0.056686047 | 0.026162791 |\n",
       "\n"
      ],
      "text/plain": [
       "  unknown     Action     Adventure   Animation  Children's Comedy    \n",
       "1 0.000000000 0.05822785 0.027848101 0.02278481 0.09620253 1.00000000\n",
       "2 0.000000000 0.59744409 0.329073482 0.01916933 0.10543131 0.01597444\n",
       "3 0.000000000 0.05785124 0.008264463 0.00000000 0.00000000 0.04132231\n",
       "4 0.000000000 0.04761905 0.054421769 0.01360544 0.02721088 0.65986395\n",
       "5 0.002906977 0.03633721 0.014534884 0.03633721 0.06540698 0.00000000\n",
       "  Crime      Documentary Drama      Fantasy     Film-Noir   Horror     \n",
       "1 0.02531646 0.002531646 0.18734177 0.012658228 0.000000000 0.027848101\n",
       "2 0.03833866 0.000000000 0.03833866 0.025559105 0.025559105 0.089456869\n",
       "3 0.68595041 0.000000000 0.70247934 0.008264463 0.082644628 0.008264463\n",
       "4 0.01360544 0.000000000 0.07482993 0.013605442 0.006802721 0.013605442\n",
       "5 0.00000000 0.071220930 0.77616279 0.008720930 0.007267442 0.069767442\n",
       "  Musical     Mystery    Romance    Sci-Fi      Thriller   War        \n",
       "1 0.035443038 0.01518987 0.00000000 0.022784810 0.01772152 0.030379747\n",
       "2 0.009584665 0.07987220 0.04153355 0.258785942 0.51757188 0.044728435\n",
       "3 0.000000000 0.09090909 0.04958678 0.024793388 0.56198347 0.008264463\n",
       "4 0.081632653 0.02721088 1.00000000 0.013605442 0.07482993 0.034013605\n",
       "5 0.039244186 0.02034884 0.11337209 0.007267442 0.00000000 0.056686047\n",
       "  Western    \n",
       "1 0.017721519\n",
       "2 0.006389776\n",
       "3 0.000000000\n",
       "4 0.000000000\n",
       "5 0.026162791"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# centroid values\n",
    "movie_kmeans$centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>395</li><li>313</li><li>121</li><li>147</li><li>688</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 395\n",
       "\\item 313\n",
       "\\item 121\n",
       "\\item 147\n",
       "\\item 688\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 395\n",
       "2. 313\n",
       "3. 121\n",
       "4. 147\n",
       "5. 688\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 395 313 121 147 688"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# size of clusters, i.e., number of movies in each cluster\n",
    "movie_kmeans$size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>215.599999999991</li><li>440.862619808313</li><li>124.297520661157</li><li>102.068027210884</li><li>490.784883720944</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 215.599999999991\n",
       "\\item 440.862619808313\n",
       "\\item 124.297520661157\n",
       "\\item 102.068027210884\n",
       "\\item 490.784883720944\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 215.599999999991\n",
       "2. 440.862619808313\n",
       "3. 124.297520661157\n",
       "4. 102.068027210884\n",
       "5. 490.784883720944\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 215.6000 440.8626 124.2975 102.0680 490.7849"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# within-cluster sum of squares\n",
    "movie_kmeans$withinss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determining number of clusters**\n",
    "\n",
    "One way to select the number of clusters is by using a **scree plot**. A standard scree plot has the number of clusters on the x-axis, and the sum of the within-cluster sum of squares on the y-axis. The within-cluster sum of squares for a cluster is the sum, across all points in the cluster, of the squared distance between each point and the centroid of the cluster. To determine the best number of clusters using this plot, we want to look for a bend, or elbow, in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAANlBMVEUAAAAzMzNNTU1oaGh8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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(99)\n",
    "\n",
    "# Call kmeans function with centers = 3, centers = 4, etc\n",
    "num_clusters = seq(5, 15,1)\n",
    "\n",
    "# within-cluster sum of squares for all clusters\n",
    "sum_withinss = sapply(num_clusters, function(x) sum(kmeans(movies, centers=x, iter.max=2000)$withinss))\n",
    "\n",
    "# visualize\n",
    "ggplot(mapping = aes(x=num_clusters, y=sum_withinss)) +\n",
    "    geom_line() +\n",
    "    geom_point()\n",
    "    \n",
    "# 12 seems like a good pick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Top 10 algorithms in data mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)\n",
    "\n",
    "[R for Statistical Learning](https://daviddalpiaz.github.io/r4sl/)\n",
    "\n",
    "[The caret Package](http://topepo.github.io/caret/)\n",
    "\n",
    "[Linear Regression: r-statistics.co](http://r-statistics.co/Linear-Regression.html)\n",
    "\n",
    "[Tutorial: SVM in R](http://math.stanford.edu/~yuany/course/2015.fall/SVM_in_R.pdf)\n",
    "\n",
    "[Artificial Neural Networks in R](https://rpubs.com/julianhatwell/annr)\n",
    "\n",
    "[Cluster analysis in R: determine the optimal number of clusters](https://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
