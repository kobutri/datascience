{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Model Evaluation\n",
    "\n",
    "## Submission Date: 09/01/2022\n",
    "\n",
    "## Points: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# theoretical exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        a. pos. a. neg.   \n",
       "p. pos.       2       2  4\n",
       "p. neg.       1       5  6\n",
       "              3       7 10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab <- matrix(c(2,2,4,1,5,6,3,7,10), ncol=3, byrow=TRUE)\n",
    "rownames(tab) <- c('p. pos.','p. neg.','')\n",
    "colnames(tab) <- c('a. pos.','a. neg.','')\n",
    "tab <- as.table(tab)\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy  \n",
    "a = 2+5 / 10 = 0.7\n",
    "\n",
    "### precision  \n",
    "p (DB = Y) = 2 / 4 = 0.5  \n",
    "p (DB = N) = 5 / 6 = 0.83  \n",
    "  \n",
    "### recall   \n",
    "r (DB = Y) = 2 / 3 = 0.67  \n",
    "r (DB = N) = 5 / 7 = 0.71  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cancer prediction:\n",
    "false negative -> when someone has cancer and is diagnosed with not having it he wont get the right treatment\n",
    "\n",
    "virus detection:\n",
    "false negative -> one undetected virus can cause more harm than not using one program which was falsely accused of containing a virus\n",
    "\n",
    "spam mail:\n",
    "false positive -> when a important mail is flagged as spam it's worse than one spam mail which got into the inbox\n",
    "\n",
    "faulty item prediction:\n",
    "false negativ -> when a defect product gets to the costumer it causes much harm in the trust to the company so its important that every defect product gets detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy\n",
    "a = 9980 / 10000 = 0.998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> keywords:<font color='black'> \n",
    "    \n",
    "**Tasks:** regression, classification, clustering  \n",
    "**Algorithms:** linear regression, logistic regression, random forest, SVM, k-means  \n",
    "**Evaluation Measures:** RMSE, precision, recall, F1-score  \n",
    "**Concepts:** training and testing, overfitting and underfitting, cross-validation  \n",
    "**R packages used:** tidyverse, rpart, randomForest, e1071, caret, mice, mltest  \n",
    "  \n",
    "  \n",
    "    \n",
    "The goal of this lab session is to get familar with various machine learning based tasks in R. Many packages in R have similar interface that uses a formula and other parameters.\n",
    "\n",
    "**formula:** is used to express the form of a model. For example, suppose you have a response variable y and independent variables x1, x2 and x3. To express that y depends linearly on x1, x2 and x3 you would use the formula `y ~ x1 + x2 + x3.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) is the machine learning task of inferring a function from labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) is the processes to estimate the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "We will use the `lm()` function in the `stats` package which is part of base R. No external package needed.\n",
    "\n",
    "    lm_model <- lm(y ∼ x1 + x2, data=mydata)\n",
    "    summary(lm_model)\n",
    "\n",
    "The vector of coefficients for the model is contained in `lm_model$coefficients.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: We will start with building a simple model using the `cars` dataset that comes with R. The dataset contains the speed of cars and the distances taken to stop. In this example, we will build a linear regression model with only a **single feature**, i.e. to compute `dist` from `speed.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>speed</th><th scope=col>dist</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>4</td><td> 2</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4</td><td>10</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>7</td><td> 4</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>7</td><td>22</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>8</td><td>16</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>9</td><td>10</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & speed & dist\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 4 &  2\\\\\n",
       "\t2 & 4 & 10\\\\\n",
       "\t3 & 7 &  4\\\\\n",
       "\t4 & 7 & 22\\\\\n",
       "\t5 & 8 & 16\\\\\n",
       "\t6 & 9 & 10\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | speed &lt;dbl&gt; | dist &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 4 |  2 |\n",
       "| 2 | 4 | 10 |\n",
       "| 3 | 7 |  4 |\n",
       "| 4 | 7 | 22 |\n",
       "| 5 | 8 | 16 |\n",
       "| 6 | 9 | 10 |\n",
       "\n"
      ],
      "text/plain": [
       "  speed dist\n",
       "1 4      2  \n",
       "2 4     10  \n",
       "3 7      4  \n",
       "4 7     22  \n",
       "5 8     16  \n",
       "6 9     10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view some first rows of the dataset\n",
    "head(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAAQlBMVEUAAAAzMzMzZv89PT1N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\nLIZhwgzBYhgmzPw/ygCpJs2YixAAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now, lets visually examine the data\n",
    "\n",
    "# load the required packages for plotting\n",
    "library(tidyverse)\n",
    "\n",
    "# draw the scatter plot between 'speed' and 'dist'\n",
    "ggplot(data = cars, mapping = aes(x = speed, y = dist)) +\n",
    "    geom_point() +\n",
    "    geom_smooth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above plot suggests that 'dist' can be computed from 'speed' through a linear function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear regression model\n",
    "cars_lm <- lm(dist ~ speed, data = cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the fitted model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = dist ~ speed, data = cars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-29.069  -9.525  -2.272   9.215  43.201 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \n",
       "speed         3.9324     0.4155   9.464 1.49e-12 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 15.38 on 48 degrees of freedom\n",
       "Multiple R-squared:  0.6511,\tAdjusted R-squared:  0.6438 \n",
       "F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## summary of fitted model\n",
    "summary(cars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to this [blog](https://boostedml.com/2019/06/linear-regression-in-r-interpreting-summarylm.html) for the intepretation of the summary of the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the fitted model for prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>1</dt><dd>-9.71427737226271</dd><dt>2</dt><dd>-5.78186861313862</dd><dt>3</dt><dd>-1.84945985401454</dd><dt>4</dt><dd>9.94776642335772</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] -9.71427737226271\n",
       "\\item[2] -5.78186861313862\n",
       "\\item[3] -1.84945985401454\n",
       "\\item[4] 9.94776642335772\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   -9.714277372262712\n",
       ":   -5.781868613138623\n",
       ":   -1.849459854014544\n",
       ":   9.94776642335772\n",
       "\n"
      ],
      "text/plain": [
       "        1         2         3         4 \n",
       "-9.714277 -5.781869 -1.849460  9.947766 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example unseen data\n",
    "df <- data.frame('speed' = c(2,3,4,7))\n",
    "# prediction\n",
    "predict(cars_lm, newdata = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: We will use another dataset that comes with R, `mtcars`, to build a model with **multiple features** to predict the fuel consumption `mpg.` The features describe different aspects of an automobile design and performance. We will also explore **which features to use**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 11</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>mpg</th><th scope=col>cyl</th><th scope=col>disp</th><th scope=col>hp</th><th scope=col>drat</th><th scope=col>wt</th><th scope=col>qsec</th><th scope=col>vs</th><th scope=col>am</th><th scope=col>gear</th><th scope=col>carb</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Mazda RX4</th><td>21.0</td><td>6</td><td>160</td><td>110</td><td>3.90</td><td>2.620</td><td>16.46</td><td>0</td><td>1</td><td>4</td><td>4</td></tr>\n",
       "\t<tr><th scope=row>Mazda RX4 Wag</th><td>21.0</td><td>6</td><td>160</td><td>110</td><td>3.90</td><td>2.875</td><td>17.02</td><td>0</td><td>1</td><td>4</td><td>4</td></tr>\n",
       "\t<tr><th scope=row>Datsun 710</th><td>22.8</td><td>4</td><td>108</td><td> 93</td><td>3.85</td><td>2.320</td><td>18.61</td><td>1</td><td>1</td><td>4</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>Hornet 4 Drive</th><td>21.4</td><td>6</td><td>258</td><td>110</td><td>3.08</td><td>3.215</td><td>19.44</td><td>1</td><td>0</td><td>3</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>Hornet Sportabout</th><td>18.7</td><td>8</td><td>360</td><td>175</td><td>3.15</td><td>3.440</td><td>17.02</td><td>0</td><td>0</td><td>3</td><td>2</td></tr>\n",
       "\t<tr><th scope=row>Valiant</th><td>18.1</td><td>6</td><td>225</td><td>105</td><td>2.76</td><td>3.460</td><td>20.22</td><td>1</td><td>0</td><td>3</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 11\n",
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & mpg & cyl & disp & hp & drat & wt & qsec & vs & am & gear & carb\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tMazda RX4 & 21.0 & 6 & 160 & 110 & 3.90 & 2.620 & 16.46 & 0 & 1 & 4 & 4\\\\\n",
       "\tMazda RX4 Wag & 21.0 & 6 & 160 & 110 & 3.90 & 2.875 & 17.02 & 0 & 1 & 4 & 4\\\\\n",
       "\tDatsun 710 & 22.8 & 4 & 108 &  93 & 3.85 & 2.320 & 18.61 & 1 & 1 & 4 & 1\\\\\n",
       "\tHornet 4 Drive & 21.4 & 6 & 258 & 110 & 3.08 & 3.215 & 19.44 & 1 & 0 & 3 & 1\\\\\n",
       "\tHornet Sportabout & 18.7 & 8 & 360 & 175 & 3.15 & 3.440 & 17.02 & 0 & 0 & 3 & 2\\\\\n",
       "\tValiant & 18.1 & 6 & 225 & 105 & 2.76 & 3.460 & 20.22 & 1 & 0 & 3 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 11\n",
       "\n",
       "| <!--/--> | mpg &lt;dbl&gt; | cyl &lt;dbl&gt; | disp &lt;dbl&gt; | hp &lt;dbl&gt; | drat &lt;dbl&gt; | wt &lt;dbl&gt; | qsec &lt;dbl&gt; | vs &lt;dbl&gt; | am &lt;dbl&gt; | gear &lt;dbl&gt; | carb &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Mazda RX4 | 21.0 | 6 | 160 | 110 | 3.90 | 2.620 | 16.46 | 0 | 1 | 4 | 4 |\n",
       "| Mazda RX4 Wag | 21.0 | 6 | 160 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 |\n",
       "| Datsun 710 | 22.8 | 4 | 108 |  93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 |\n",
       "| Hornet 4 Drive | 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 |\n",
       "| Hornet Sportabout | 18.7 | 8 | 360 | 175 | 3.15 | 3.440 | 17.02 | 0 | 0 | 3 | 2 |\n",
       "| Valiant | 18.1 | 6 | 225 | 105 | 2.76 | 3.460 | 20.22 | 1 | 0 | 3 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "                  mpg  cyl disp hp  drat wt    qsec  vs am gear carb\n",
       "Mazda RX4         21.0 6   160  110 3.90 2.620 16.46 0  1  4    4   \n",
       "Mazda RX4 Wag     21.0 6   160  110 3.90 2.875 17.02 0  1  4    4   \n",
       "Datsun 710        22.8 4   108   93 3.85 2.320 18.61 1  1  4    1   \n",
       "Hornet 4 Drive    21.4 6   258  110 3.08 3.215 19.44 1  0  3    1   \n",
       "Hornet Sportabout 18.7 8   360  175 3.15 3.440 17.02 0  0  3    2   \n",
       "Valiant           18.1 6   225  105 2.76 3.460 20.22 1  0  3    1   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + qsec + am + carb, data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.1184 -1.5414 -0.1392  1.2917  4.3604 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  12.8972     7.4725   1.726 0.095784 .  \n",
       "wt           -3.4343     0.8200  -4.188 0.000269 ***\n",
       "qsec          1.0191     0.3378   3.017 0.005507 ** \n",
       "am            3.5114     1.4875   2.361 0.025721 *  \n",
       "carb         -0.4886     0.4212  -1.160 0.256212    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.444 on 27 degrees of freedom\n",
       "Multiple R-squared:  0.8568,\tAdjusted R-squared:  0.8356 \n",
       "F-statistic: 40.39 on 4 and 27 DF,  p-value: 5.064e-11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the dataset mtcars to create a linear regression model to predict `mpg` using `wt, qsec, am` and `carb`. \n",
    "mtcars_lm <- lm(mpg ~ wt + qsec + am + carb, data = mtcars)\n",
    "summary(mtcars_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "[Classification](https://en.wikipedia.org/wiki/Statistical_classification) is the problem of determining label(s) (e.g., categories or clasesses, etc.) of each data observation based on learning from labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widely used (shallow) classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Logistic Regression**](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "Provided in `stats` package, which is automatically loaded when starting R  \n",
    "\n",
    "    glm_model <- glm(y ∼ x1 + x2, family = binomial, data = mydata)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**K-Nearest Neighbor**](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "\n",
    "Install and load the `class` package\n",
    "\n",
    "    knn_model <- knn(train=X_train, test=X_test, cl=as.factor(labels), k=K)\n",
    "    \n",
    "`knn_model` is a factor vector of class attributes for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Decision Trees (CART)**](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "\n",
    "Install and load the `rpart` package.\n",
    "\n",
    "    cart_model <- rpart(y ∼ x1 + x2, data=mydata, method=\"class\")\n",
    " \n",
    "You can use `plot.rpart` and `text.rpart` to plot the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Random Forest**](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "Install and load the `randomForest` package\n",
    "\n",
    "\n",
    "    rf_model <- randomForest(y ~ x1 + x2, data=train, importance=TRUE, ntree=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Support Vector Machines (SVM)**](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "Install and load the `e1071` package.\n",
    "\n",
    "    svm_model <- svm(x=X, y=as.factor(labels), kernel =\"radial\", cost=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we will use the above models to predict `survivors` in the [Titanic dataset](https://www.kaggle.com/c/titanic). The dataset also provided under `data/titanic.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m1309\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m12\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (5): Name, Sex, Ticket, Cabin, Embarked\n",
      "\u001b[32mdbl\u001b[39m (7): PassengerId, Pclass, Age, SibSp, Parch, Fare, Survived\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 12</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>PassengerId</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>SibSp</th><th scope=col>Parch</th><th scope=col>Ticket</th><th scope=col>Fare</th><th scope=col>Cabin</th><th scope=col>Embarked</th><th scope=col>Survived</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>3</td><td>Braund, Mr. Owen Harris                            </td><td>male  </td><td>22</td><td>1</td><td>0</td><td>A/5 21171       </td><td> 7.2500</td><td>NA  </td><td>S</td><td>0</td></tr>\n",
       "\t<tr><td>2</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38</td><td>1</td><td>0</td><td>PC 17599        </td><td>71.2833</td><td>C85 </td><td>C</td><td>1</td></tr>\n",
       "\t<tr><td>3</td><td>3</td><td>Heikkinen, Miss. Laina                             </td><td>female</td><td>26</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td> 7.9250</td><td>NA  </td><td>S</td><td>1</td></tr>\n",
       "\t<tr><td>4</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)       </td><td>female</td><td>35</td><td>1</td><td>0</td><td>113803          </td><td>53.1000</td><td>C123</td><td>S</td><td>1</td></tr>\n",
       "\t<tr><td>5</td><td>3</td><td>Allen, Mr. William Henry                           </td><td>male  </td><td>35</td><td>0</td><td>0</td><td>373450          </td><td> 8.0500</td><td>NA  </td><td>S</td><td>0</td></tr>\n",
       "\t<tr><td>6</td><td>3</td><td>Moran, Mr. James                                   </td><td>male  </td><td>NA</td><td>0</td><td>0</td><td>330877          </td><td> 8.4583</td><td>NA  </td><td>Q</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 12\n",
       "\\begin{tabular}{llllllllllll}\n",
       " PassengerId & Pclass & Name & Sex & Age & SibSp & Parch & Ticket & Fare & Cabin & Embarked & Survived\\\\\n",
       " <dbl> & <dbl> & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <chr> & <dbl> & <chr> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 3 & Braund, Mr. Owen Harris                             & male   & 22 & 1 & 0 & A/5 21171        &  7.2500 & NA   & S & 0\\\\\n",
       "\t 2 & 1 & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female & 38 & 1 & 0 & PC 17599         & 71.2833 & C85  & C & 1\\\\\n",
       "\t 3 & 3 & Heikkinen, Miss. Laina                              & female & 26 & 0 & 0 & STON/O2. 3101282 &  7.9250 & NA   & S & 1\\\\\n",
       "\t 4 & 1 & Futrelle, Mrs. Jacques Heath (Lily May Peel)        & female & 35 & 1 & 0 & 113803           & 53.1000 & C123 & S & 1\\\\\n",
       "\t 5 & 3 & Allen, Mr. William Henry                            & male   & 35 & 0 & 0 & 373450           &  8.0500 & NA   & S & 0\\\\\n",
       "\t 6 & 3 & Moran, Mr. James                                    & male   & NA & 0 & 0 & 330877           &  8.4583 & NA   & Q & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 12\n",
       "\n",
       "| PassengerId &lt;dbl&gt; | Pclass &lt;dbl&gt; | Name &lt;chr&gt; | Sex &lt;chr&gt; | Age &lt;dbl&gt; | SibSp &lt;dbl&gt; | Parch &lt;dbl&gt; | Ticket &lt;chr&gt; | Fare &lt;dbl&gt; | Cabin &lt;chr&gt; | Embarked &lt;chr&gt; | Survived &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 3 | Braund, Mr. Owen Harris                             | male   | 22 | 1 | 0 | A/5 21171        |  7.2500 | NA   | S | 0 |\n",
       "| 2 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38 | 1 | 0 | PC 17599         | 71.2833 | C85  | C | 1 |\n",
       "| 3 | 3 | Heikkinen, Miss. Laina                              | female | 26 | 0 | 0 | STON/O2. 3101282 |  7.9250 | NA   | S | 1 |\n",
       "| 4 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female | 35 | 1 | 0 | 113803           | 53.1000 | C123 | S | 1 |\n",
       "| 5 | 3 | Allen, Mr. William Henry                            | male   | 35 | 0 | 0 | 373450           |  8.0500 | NA   | S | 0 |\n",
       "| 6 | 3 | Moran, Mr. James                                    | male   | NA | 0 | 0 | 330877           |  8.4583 | NA   | Q | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  PassengerId Pclass Name                                                Sex   \n",
       "1 1           3      Braund, Mr. Owen Harris                             male  \n",
       "2 2           1      Cumings, Mrs. John Bradley (Florence Briggs Thayer) female\n",
       "3 3           3      Heikkinen, Miss. Laina                              female\n",
       "4 4           1      Futrelle, Mrs. Jacques Heath (Lily May Peel)        female\n",
       "5 5           3      Allen, Mr. William Henry                            male  \n",
       "6 6           3      Moran, Mr. James                                    male  \n",
       "  Age SibSp Parch Ticket           Fare    Cabin Embarked Survived\n",
       "1 22  1     0     A/5 21171         7.2500 NA    S        0       \n",
       "2 38  1     0     PC 17599         71.2833 C85   C        1       \n",
       "3 26  0     0     STON/O2. 3101282  7.9250 NA    S        1       \n",
       "4 35  1     0     113803           53.1000 C123  S        1       \n",
       "5 35  0     0     373450            8.0500 NA    S        0       \n",
       "6 NA  0     0     330877            8.4583 NA    Q        0       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  PassengerId       Pclass          Name               Sex           \n",
       " Min.   :   1   Min.   :1.000   Length:1309        Length:1309       \n",
       " 1st Qu.: 328   1st Qu.:2.000   Class :character   Class :character  \n",
       " Median : 655   Median :3.000   Mode  :character   Mode  :character  \n",
       " Mean   : 655   Mean   :2.295                                        \n",
       " 3rd Qu.: 982   3rd Qu.:3.000                                        \n",
       " Max.   :1309   Max.   :3.000                                        \n",
       "                                                                     \n",
       "      Age            SibSp            Parch          Ticket         \n",
       " Min.   : 0.17   Min.   :0.0000   Min.   :0.000   Length:1309       \n",
       " 1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   Class :character  \n",
       " Median :28.00   Median :0.0000   Median :0.000   Mode  :character  \n",
       " Mean   :29.88   Mean   :0.4989   Mean   :0.385                     \n",
       " 3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000                     \n",
       " Max.   :80.00   Max.   :8.0000   Max.   :9.000                     \n",
       " NA's   :263                                                        \n",
       "      Fare            Cabin             Embarked            Survived     \n",
       " Min.   :  0.000   Length:1309        Length:1309        Min.   :0.0000  \n",
       " 1st Qu.:  7.896   Class :character   Class :character   1st Qu.:0.0000  \n",
       " Median : 14.454   Mode  :character   Mode  :character   Median :0.0000  \n",
       " Mean   : 33.295                                         Mean   :0.3838  \n",
       " 3rd Qu.: 31.275                                         3rd Qu.:1.0000  \n",
       " Max.   :512.329                                         Max.   :1.0000  \n",
       " NA's   :1                                               NA's   :418     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load, view data examples, and summarize the dataset\n",
    "titanic <- read_csv('titanic.csv', skip=5)\n",
    "head(titanic)\n",
    "summary(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: most models do not work with `character` type, we need to convert strings to factors for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic$Sex <- as.factor(titanic$Sex)\n",
    "titanic$Cabin <- as.factor(titanic$Cabin)\n",
    "titanic$Embarked <- as.factor(titanic$Embarked)\n",
    "titanic$Survived <- as.factor(titanic$Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Testing data**\n",
    "\n",
    "split the titanic data into training and testing sets based on the feature we want to predict `Survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>891</li><li>12</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 891\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 891\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 891  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>418</li><li>12</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 418\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 418\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 418  12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_training <- filter(titanic, !is.na(Survived))\n",
    "dim(titanic_training)\n",
    "titanic_testing <- filter(titanic, is.na(Survived))\n",
    "dim(titanic_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  PassengerId        Pclass          Name               Sex     \n",
       " Min.   :  1.0   Min.   :1.000   Length:891         female:314  \n",
       " 1st Qu.:223.5   1st Qu.:2.000   Class :character   male  :577  \n",
       " Median :446.0   Median :3.000   Mode  :character               \n",
       " Mean   :446.0   Mean   :2.309                                  \n",
       " 3rd Qu.:668.5   3rd Qu.:3.000                                  \n",
       " Max.   :891.0   Max.   :3.000                                  \n",
       "                                                                \n",
       "      Age            SibSp           Parch           Ticket         \n",
       " Min.   : 0.42   Min.   :0.000   Min.   :0.0000   Length:891        \n",
       " 1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000   Class :character  \n",
       " Median :28.00   Median :0.000   Median :0.0000   Mode  :character  \n",
       " Mean   :29.70   Mean   :0.523   Mean   :0.3816                     \n",
       " 3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000                     \n",
       " Max.   :80.00   Max.   :8.000   Max.   :6.0000                     \n",
       " NA's   :177                                                        \n",
       "      Fare                Cabin     Embarked   Survived\n",
       " Min.   :  0.00   B96 B98    :  4   C   :168   0:549   \n",
       " 1st Qu.:  7.91   C23 C25 C27:  4   Q   : 77   1:342   \n",
       " Median : 14.45   G6         :  4   S   :644           \n",
       " Mean   : 32.20   C22 C26    :  3   NA's:  2           \n",
       " 3rd Qu.: 31.00   D          :  3                      \n",
       " Max.   :512.33   (Other)    :186                      \n",
       "                  NA's       :687                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if there is missing value in training data\n",
    "summary(titanic_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  PassengerId         Pclass          Name               Sex     \n",
       " Min.   : 892.0   Min.   :1.000   Length:418         female:152  \n",
       " 1st Qu.: 996.2   1st Qu.:1.000   Class :character   male  :266  \n",
       " Median :1100.5   Median :3.000   Mode  :character               \n",
       " Mean   :1100.5   Mean   :2.266                                  \n",
       " 3rd Qu.:1204.8   3rd Qu.:3.000                                  \n",
       " Max.   :1309.0   Max.   :3.000                                  \n",
       "                                                                 \n",
       "      Age            SibSp            Parch           Ticket         \n",
       " Min.   : 0.17   Min.   :0.0000   Min.   :0.0000   Length:418        \n",
       " 1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.0000   Class :character  \n",
       " Median :27.00   Median :0.0000   Median :0.0000   Mode  :character  \n",
       " Mean   :30.27   Mean   :0.4474   Mean   :0.3923                     \n",
       " 3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.0000                     \n",
       " Max.   :76.00   Max.   :8.0000   Max.   :9.0000                     \n",
       " NA's   :86                                                          \n",
       "      Fare                     Cabin     Embarked Survived  \n",
       " Min.   :  0.000   B57 B59 B63 B66:  3   C:102    0   :  0  \n",
       " 1st Qu.:  7.896   A34            :  2   Q: 46    1   :  0  \n",
       " Median : 14.454   B45            :  2   S:270    NA's:418  \n",
       " Mean   : 35.627   C101           :  2                      \n",
       " 3rd Qu.: 31.500   C116           :  2                      \n",
       " Max.   :512.329   (Other)        : 80                      \n",
       " NA's   :1         NA's           :327                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if there is missing value in test data\n",
    "summary(titanic_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting `Survived` based on `Pclass` and `Sex` using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Survived ~ Pclass + Sex, family = binomial, data = titanic_training)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.2030  -0.7036  -0.4519   0.6719   2.1599  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)   3.2946     0.2974  11.077   <2e-16 ***\n",
       "Pclass       -0.9606     0.1061  -9.057   <2e-16 ***\n",
       "Sexmale      -2.6434     0.1838 -14.380   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 1186.7  on 890  degrees of freedom\n",
       "Residual deviance:  827.2  on 888  degrees of freedom\n",
       "AIC: 833.2\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "titanic_glm <- glm(Survived ~ Pclass + Sex, data = titanic_training, family = binomial)\n",
    "\n",
    "# examine the model\n",
    "summary(titanic_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>1</dt><dd>0.0970522051145409</dd><dt>2</dt><dd>0.601802744773117</dd><dt>3</dt><dd>0.219280773210838</dd><dt>4</dt><dd>0.0970522051145409</dd><dt>5</dt><dd>0.601802744773117</dd><dt>6</dt><dd>0.0970522051145409</dd><dt>7</dt><dd>0.601802744773117</dd><dt>8</dt><dd>0.219280773210838</dd><dt>9</dt><dd>0.601802744773117</dd><dt>10</dt><dd>0.0970522051145409</dd><dt>11</dt><dd>0.0970522051145409</dd><dt>12</dt><dd>0.423283291840528</dd><dt>13</dt><dd>0.911661154107878</dd><dt>14</dt><dd>0.219280773210838</dd><dt>15</dt><dd>0.911661154107878</dd><dt>16</dt><dd>0.797950739188506</dd><dt>17</dt><dd>0.219280773210838</dd><dt>18</dt><dd>0.0970522051145409</dd><dt>19</dt><dd>0.601802744773117</dd><dt>20</dt><dd>0.601802744773117</dd><dt>21</dt><dd>0.423283291840528</dd><dt>22</dt><dd>0.0970522051145409</dd><dt>23</dt><dd>0.911661154107878</dd><dt>24</dt><dd>0.423283291840528</dd><dt>25</dt><dd>0.911661154107878</dd><dt>26</dt><dd>0.0970522051145409</dd><dt>27</dt><dd>0.911661154107878</dd><dt>28</dt><dd>0.0970522051145409</dd><dt>29</dt><dd>0.423283291840528</dd><dt>30</dt><dd>0.0970522051145409</dd><dt>31</dt><dd>0.219280773210838</dd><dt>32</dt><dd>0.219280773210838</dd><dt>33</dt><dd>0.601802744773117</dd><dt>34</dt><dd>0.601802744773117</dd><dt>35</dt><dd>0.423283291840528</dd><dt>36</dt><dd>0.0970522051145409</dd><dt>37</dt><dd>0.601802744773117</dd><dt>38</dt><dd>0.601802744773117</dd><dt>39</dt><dd>0.0970522051145409</dd><dt>40</dt><dd>0.0970522051145409</dd><dt>41</dt><dd>0.0970522051145409</dd><dt>42</dt><dd>0.423283291840528</dd><dt>43</dt><dd>0.0970522051145409</dd><dt>44</dt><dd>0.797950739188506</dd><dt>45</dt><dd>0.911661154107878</dd><dt>46</dt><dd>0.0970522051145409</dd><dt>47</dt><dd>0.423283291840528</dd><dt>48</dt><dd>0.0970522051145409</dd><dt>49</dt><dd>0.911661154107878</dd><dt>50</dt><dd>0.601802744773117</dd><dt>51</dt><dd>0.423283291840528</dd><dt>52</dt><dd>0.219280773210838</dd><dt>53</dt><dd>0.797950739188506</dd><dt>54</dt><dd>0.911661154107878</dd><dt>55</dt><dd>0.219280773210838</dd><dt>56</dt><dd>0.0970522051145409</dd><dt>57</dt><dd>0.0970522051145409</dd><dt>58</dt><dd>0.0970522051145409</dd><dt>59</dt><dd>0.0970522051145409</dd><dt>60</dt><dd>0.911661154107878</dd><dt>61</dt><dd>0.0970522051145409</dd><dt>62</dt><dd>0.219280773210838</dd><dt>63</dt><dd>0.0970522051145409</dd><dt>64</dt><dd>0.601802744773117</dd><dt>65</dt><dd>0.423283291840528</dd><dt>66</dt><dd>0.797950739188506</dd><dt>67</dt><dd>0.601802744773117</dd><dt>68</dt><dd>0.423283291840528</dd><dt>69</dt><dd>0.423283291840528</dd><dt>70</dt><dd>0.911661154107878</dd><dt>71</dt><dd>0.601802744773117</dd><dt>72</dt><dd>0.0970522051145409</dd><dt>73</dt><dd>0.601802744773117</dd><dt>74</dt><dd>0.423283291840528</dd><dt>75</dt><dd>0.911661154107878</dd><dt>76</dt><dd>0.423283291840528</dd><dt>77</dt><dd>0.0970522051145409</dd><dt>78</dt><dd>0.911661154107878</dd><dt>79</dt><dd>0.219280773210838</dd><dt>80</dt><dd>0.601802744773117</dd><dt>81</dt><dd>0.0970522051145409</dd><dt>82</dt><dd>0.423283291840528</dd><dt>83</dt><dd>0.423283291840528</dd><dt>84</dt><dd>0.0970522051145409</dd><dt>85</dt><dd>0.219280773210838</dd><dt>86</dt><dd>0.0970522051145409</dd><dt>87</dt><dd>0.601802744773117</dd><dt>88</dt><dd>0.601802744773117</dd><dt>89</dt><dd>0.601802744773117</dd><dt>90</dt><dd>0.219280773210838</dd><dt>91</dt><dd>0.601802744773117</dd><dt>92</dt><dd>0.0970522051145409</dd><dt>93</dt><dd>0.911661154107878</dd><dt>94</dt><dd>0.0970522051145409</dd><dt>95</dt><dd>0.423283291840528</dd><dt>96</dt><dd>0.0970522051145409</dd><dt>97</dt><dd>0.911661154107878</dd><dt>98</dt><dd>0.0970522051145409</dd><dt>99</dt><dd>0.601802744773117</dd><dt>100</dt><dd>0.0970522051145409</dd><dt>101</dt><dd>0.911661154107878</dd><dt>102</dt><dd>0.219280773210838</dd><dt>103</dt><dd>0.0970522051145409</dd><dt>104</dt><dd>0.0970522051145409</dd><dt>105</dt><dd>0.601802744773117</dd><dt>106</dt><dd>0.0970522051145409</dd><dt>107</dt><dd>0.0970522051145409</dd><dt>108</dt><dd>0.0970522051145409</dd><dt>109</dt><dd>0.0970522051145409</dd><dt>110</dt><dd>0.219280773210838</dd><dt>111</dt><dd>0.219280773210838</dd><dt>112</dt><dd>0.601802744773117</dd><dt>113</dt><dd>0.911661154107878</dd><dt>114</dt><dd>0.601802744773117</dd><dt>115</dt><dd>0.911661154107878</dd><dt>116</dt><dd>0.0970522051145409</dd><dt>117</dt><dd>0.0970522051145409</dd><dt>118</dt><dd>0.601802744773117</dd><dt>119</dt><dd>0.423283291840528</dd><dt>120</dt><dd>0.797950739188506</dd><dt>121</dt><dd>0.797950739188506</dd><dt>122</dt><dd>0.0970522051145409</dd><dt>123</dt><dd>0.911661154107878</dd><dt>124</dt><dd>0.0970522051145409</dd><dt>125</dt><dd>0.0970522051145409</dd><dt>126</dt><dd>0.601802744773117</dd><dt>127</dt><dd>0.0970522051145409</dd><dt>128</dt><dd>0.601802744773117</dd><dt>129</dt><dd>0.219280773210838</dd><dt>130</dt><dd>0.0970522051145409</dd><dt>131</dt><dd>0.0970522051145409</dd><dt>132</dt><dd>0.423283291840528</dd><dt>133</dt><dd>0.601802744773117</dd><dt>134</dt><dd>0.0970522051145409</dd><dt>135</dt><dd>0.0970522051145409</dd><dt>136</dt><dd>0.0970522051145409</dd><dt>137</dt><dd>0.0970522051145409</dd><dt>138</dt><dd>0.219280773210838</dd><dt>139</dt><dd>0.601802744773117</dd><dt>140</dt><dd>0.0970522051145409</dd><dt>141</dt><dd>0.601802744773117</dd><dt>142</dt><dd>0.911661154107878</dd><dt>143</dt><dd>0.423283291840528</dd><dt>144</dt><dd>0.219280773210838</dd><dt>145</dt><dd>0.423283291840528</dd><dt>146</dt><dd>0.0970522051145409</dd><dt>147</dt><dd>0.423283291840528</dd><dt>148</dt><dd>0.0970522051145409</dd><dt>149</dt><dd>0.423283291840528</dd><dt>150</dt><dd>0.219280773210838</dd><dt>151</dt><dd>0.911661154107878</dd><dt>152</dt><dd>0.0970522051145409</dd><dt>153</dt><dd>0.0970522051145409</dd><dt>154</dt><dd>0.601802744773117</dd><dt>155</dt><dd>0.0970522051145409</dd><dt>156</dt><dd>0.0970522051145409</dd><dt>157</dt><dd>0.911661154107878</dd><dt>158</dt><dd>0.601802744773117</dd><dt>159</dt><dd>0.423283291840528</dd><dt>160</dt><dd>0.601802744773117</dd><dt>161</dt><dd>0.601802744773117</dd><dt>162</dt><dd>0.0970522051145409</dd><dt>163</dt><dd>0.797950739188506</dd><dt>164</dt><dd>0.0970522051145409</dd><dt>165</dt><dd>0.219280773210838</dd><dt>166</dt><dd>0.601802744773117</dd><dt>167</dt><dd>0.423283291840528</dd><dt>168</dt><dd>0.0970522051145409</dd><dt>169</dt><dd>0.911661154107878</dd><dt>170</dt><dd>0.601802744773117</dd><dt>171</dt><dd>0.0970522051145409</dd><dt>172</dt><dd>0.0970522051145409</dd><dt>173</dt><dd>0.0970522051145409</dd><dt>174</dt><dd>0.0970522051145409</dd><dt>175</dt><dd>0.0970522051145409</dd><dt>176</dt><dd>0.797950739188506</dd><dt>177</dt><dd>0.797950739188506</dd><dt>178</dt><dd>0.423283291840528</dd><dt>179</dt><dd>0.797950739188506</dd><dt>180</dt><dd>0.911661154107878</dd><dt>181</dt><dd>0.219280773210838</dd><dt>182</dt><dd>0.423283291840528</dd><dt>183</dt><dd>0.911661154107878</dd><dt>184</dt><dd>0.0970522051145409</dd><dt>185</dt><dd>0.911661154107878</dd><dt>186</dt><dd>0.219280773210838</dd><dt>187</dt><dd>0.797950739188506</dd><dt>188</dt><dd>0.0970522051145409</dd><dt>189</dt><dd>0.601802744773117</dd><dt>190</dt><dd>0.219280773210838</dd><dt>191</dt><dd>0.219280773210838</dd><dt>192</dt><dd>0.423283291840528</dd><dt>193</dt><dd>0.0970522051145409</dd><dt>194</dt><dd>0.219280773210838</dd><dt>195</dt><dd>0.219280773210838</dd><dt>196</dt><dd>0.0970522051145409</dd><dt>197</dt><dd>0.423283291840528</dd><dt>198</dt><dd>0.601802744773117</dd><dt>199</dt><dd>0.219280773210838</dd><dt>200</dt><dd>0.601802744773117</dd><dt>201</dt><dd>⋯</dd><dt>202</dt><dd>0.911661154107878</dd><dt>203</dt><dd>0.0970522051145409</dd><dt>204</dt><dd>0.797950739188506</dd><dt>205</dt><dd>0.0970522051145409</dd><dt>206</dt><dd>0.797950739188506</dd><dt>207</dt><dd>0.0970522051145409</dd><dt>208</dt><dd>0.911661154107878</dd><dt>209</dt><dd>0.601802744773117</dd><dt>210</dt><dd>0.0970522051145409</dd><dt>211</dt><dd>0.601802744773117</dd><dt>212</dt><dd>0.0970522051145409</dd><dt>213</dt><dd>0.219280773210838</dd><dt>214</dt><dd>0.219280773210838</dd><dt>215</dt><dd>0.911661154107878</dd><dt>216</dt><dd>0.0970522051145409</dd><dt>217</dt><dd>0.0970522051145409</dd><dt>218</dt><dd>0.423283291840528</dd><dt>219</dt><dd>0.0970522051145409</dd><dt>220</dt><dd>0.423283291840528</dd><dt>221</dt><dd>0.0970522051145409</dd><dt>222</dt><dd>0.797950739188506</dd><dt>223</dt><dd>0.911661154107878</dd><dt>224</dt><dd>0.911661154107878</dd><dt>225</dt><dd>0.797950739188506</dd><dt>226</dt><dd>0.423283291840528</dd><dt>227</dt><dd>0.0970522051145409</dd><dt>228</dt><dd>0.0970522051145409</dd><dt>229</dt><dd>0.423283291840528</dd><dt>230</dt><dd>0.797950739188506</dd><dt>231</dt><dd>0.219280773210838</dd><dt>232</dt><dd>0.797950739188506</dd><dt>233</dt><dd>0.601802744773117</dd><dt>234</dt><dd>0.797950739188506</dd><dt>235</dt><dd>0.0970522051145409</dd><dt>236</dt><dd>0.423283291840528</dd><dt>237</dt><dd>0.0970522051145409</dd><dt>238</dt><dd>0.0970522051145409</dd><dt>239</dt><dd>0.0970522051145409</dd><dt>240</dt><dd>0.0970522051145409</dd><dt>241</dt><dd>0.0970522051145409</dd><dt>242</dt><dd>0.797950739188506</dd><dt>243</dt><dd>0.0970522051145409</dd><dt>244</dt><dd>0.0970522051145409</dd><dt>245</dt><dd>0.0970522051145409</dd><dt>246</dt><dd>0.797950739188506</dd><dt>247</dt><dd>0.601802744773117</dd><dt>248</dt><dd>0.219280773210838</dd><dt>249</dt><dd>0.0970522051145409</dd><dt>250</dt><dd>0.423283291840528</dd><dt>251</dt><dd>0.0970522051145409</dd><dt>252</dt><dd>0.601802744773117</dd><dt>253</dt><dd>0.0970522051145409</dd><dt>254</dt><dd>0.423283291840528</dd><dt>255</dt><dd>0.0970522051145409</dd><dt>256</dt><dd>0.911661154107878</dd><dt>257</dt><dd>0.601802744773117</dd><dt>258</dt><dd>0.0970522051145409</dd><dt>259</dt><dd>0.797950739188506</dd><dt>260</dt><dd>0.219280773210838</dd><dt>261</dt><dd>0.219280773210838</dd><dt>262</dt><dd>0.219280773210838</dd><dt>263</dt><dd>0.219280773210838</dd><dt>264</dt><dd>0.601802744773117</dd><dt>265</dt><dd>0.0970522051145409</dd><dt>266</dt><dd>0.601802744773117</dd><dt>267</dt><dd>0.601802744773117</dd><dt>268</dt><dd>0.601802744773117</dd><dt>269</dt><dd>0.0970522051145409</dd><dt>270</dt><dd>0.0970522051145409</dd><dt>271</dt><dd>0.423283291840528</dd><dt>272</dt><dd>0.0970522051145409</dd><dt>273</dt><dd>0.0970522051145409</dd><dt>274</dt><dd>0.423283291840528</dd><dt>275</dt><dd>0.601802744773117</dd><dt>276</dt><dd>0.0970522051145409</dd><dt>277</dt><dd>0.423283291840528</dd><dt>278</dt><dd>0.0970522051145409</dd><dt>279</dt><dd>0.0970522051145409</dd><dt>280</dt><dd>0.797950739188506</dd><dt>281</dt><dd>0.0970522051145409</dd><dt>282</dt><dd>0.423283291840528</dd><dt>283</dt><dd>0.0970522051145409</dd><dt>284</dt><dd>0.0970522051145409</dd><dt>285</dt><dd>0.219280773210838</dd><dt>286</dt><dd>0.219280773210838</dd><dt>287</dt><dd>0.0970522051145409</dd><dt>288</dt><dd>0.601802744773117</dd><dt>289</dt><dd>0.911661154107878</dd><dt>290</dt><dd>0.423283291840528</dd><dt>291</dt><dd>0.0970522051145409</dd><dt>292</dt><dd>0.423283291840528</dd><dt>293</dt><dd>0.601802744773117</dd><dt>294</dt><dd>0.0970522051145409</dd><dt>295</dt><dd>0.0970522051145409</dd><dt>296</dt><dd>0.0970522051145409</dd><dt>297</dt><dd>0.601802744773117</dd><dt>298</dt><dd>0.911661154107878</dd><dt>299</dt><dd>0.601802744773117</dd><dt>300</dt><dd>0.423283291840528</dd><dt>301</dt><dd>0.219280773210838</dd><dt>302</dt><dd>0.0970522051145409</dd><dt>303</dt><dd>0.219280773210838</dd><dt>304</dt><dd>0.0970522051145409</dd><dt>305</dt><dd>0.0970522051145409</dd><dt>306</dt><dd>0.219280773210838</dd><dt>307</dt><dd>0.423283291840528</dd><dt>308</dt><dd>0.911661154107878</dd><dt>309</dt><dd>0.0970522051145409</dd><dt>310</dt><dd>0.797950739188506</dd><dt>311</dt><dd>0.423283291840528</dd><dt>312</dt><dd>0.219280773210838</dd><dt>313</dt><dd>0.219280773210838</dd><dt>314</dt><dd>0.797950739188506</dd><dt>315</dt><dd>0.423283291840528</dd><dt>316</dt><dd>0.0970522051145409</dd><dt>317</dt><dd>0.601802744773117</dd><dt>318</dt><dd>0.0970522051145409</dd><dt>319</dt><dd>0.423283291840528</dd><dt>320</dt><dd>0.219280773210838</dd><dt>321</dt><dd>0.0970522051145409</dd><dt>322</dt><dd>0.219280773210838</dd><dt>323</dt><dd>0.0970522051145409</dd><dt>324</dt><dd>0.219280773210838</dd><dt>325</dt><dd>0.0970522051145409</dd><dt>326</dt><dd>0.0970522051145409</dd><dt>327</dt><dd>0.911661154107878</dd><dt>328</dt><dd>0.0970522051145409</dd><dt>329</dt><dd>0.601802744773117</dd><dt>330</dt><dd>0.219280773210838</dd><dt>331</dt><dd>0.601802744773117</dd><dt>332</dt><dd>0.219280773210838</dd><dt>333</dt><dd>0.797950739188506</dd><dt>334</dt><dd>0.911661154107878</dd><dt>335</dt><dd>0.219280773210838</dd><dt>336</dt><dd>0.219280773210838</dd><dt>337</dt><dd>0.219280773210838</dd><dt>338</dt><dd>0.601802744773117</dd><dt>339</dt><dd>0.423283291840528</dd><dt>340</dt><dd>0.911661154107878</dd><dt>341</dt><dd>0.0970522051145409</dd><dt>342</dt><dd>0.0970522051145409</dd><dt>343</dt><dd>0.601802744773117</dd><dt>344</dt><dd>0.0970522051145409</dd><dt>345</dt><dd>0.797950739188506</dd><dt>346</dt><dd>0.797950739188506</dd><dt>347</dt><dd>0.0970522051145409</dd><dt>348</dt><dd>0.911661154107878</dd><dt>349</dt><dd>0.601802744773117</dd><dt>350</dt><dd>0.0970522051145409</dd><dt>351</dt><dd>0.601802744773117</dd><dt>352</dt><dd>0.911661154107878</dd><dt>353</dt><dd>0.219280773210838</dd><dt>354</dt><dd>0.219280773210838</dd><dt>355</dt><dd>0.911661154107878</dd><dt>356</dt><dd>0.423283291840528</dd><dt>357</dt><dd>0.219280773210838</dd><dt>358</dt><dd>0.911661154107878</dd><dt>359</dt><dd>0.911661154107878</dd><dt>360</dt><dd>0.601802744773117</dd><dt>361</dt><dd>0.219280773210838</dd><dt>362</dt><dd>0.423283291840528</dd><dt>363</dt><dd>0.0970522051145409</dd><dt>364</dt><dd>0.0970522051145409</dd><dt>365</dt><dd>0.0970522051145409</dd><dt>366</dt><dd>0.601802744773117</dd><dt>367</dt><dd>0.601802744773117</dd><dt>368</dt><dd>0.219280773210838</dd><dt>369</dt><dd>0.797950739188506</dd><dt>370</dt><dd>0.0970522051145409</dd><dt>371</dt><dd>0.219280773210838</dd><dt>372</dt><dd>0.0970522051145409</dd><dt>373</dt><dd>0.0970522051145409</dd><dt>374</dt><dd>0.423283291840528</dd><dt>375</dt><dd>0.911661154107878</dd><dt>376</dt><dd>0.0970522051145409</dd><dt>377</dt><dd>0.219280773210838</dd><dt>378</dt><dd>0.0970522051145409</dd><dt>379</dt><dd>0.911661154107878</dd><dt>380</dt><dd>0.0970522051145409</dd><dt>381</dt><dd>0.911661154107878</dd><dt>382</dt><dd>0.0970522051145409</dd><dt>383</dt><dd>0.0970522051145409</dd><dt>384</dt><dd>0.911661154107878</dd><dt>385</dt><dd>0.219280773210838</dd><dt>386</dt><dd>0.911661154107878</dd><dt>387</dt><dd>0.423283291840528</dd><dt>388</dt><dd>0.423283291840528</dd><dt>389</dt><dd>0.219280773210838</dd><dt>390</dt><dd>0.219280773210838</dd><dt>391</dt><dd>0.423283291840528</dd><dt>392</dt><dd>0.601802744773117</dd><dt>393</dt><dd>0.601802744773117</dd><dt>394</dt><dd>0.601802744773117</dd><dt>395</dt><dd>0.911661154107878</dd><dt>396</dt><dd>0.601802744773117</dd><dt>397</dt><dd>0.0970522051145409</dd><dt>398</dt><dd>0.911661154107878</dd><dt>399</dt><dd>0.0970522051145409</dd><dt>400</dt><dd>0.0970522051145409</dd><dt>401</dt><dd>0.0970522051145409</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.0970522051145409\n",
       "\\item[2] 0.601802744773117\n",
       "\\item[3] 0.219280773210838\n",
       "\\item[4] 0.0970522051145409\n",
       "\\item[5] 0.601802744773117\n",
       "\\item[6] 0.0970522051145409\n",
       "\\item[7] 0.601802744773117\n",
       "\\item[8] 0.219280773210838\n",
       "\\item[9] 0.601802744773117\n",
       "\\item[10] 0.0970522051145409\n",
       "\\item[11] 0.0970522051145409\n",
       "\\item[12] 0.423283291840528\n",
       "\\item[13] 0.911661154107878\n",
       "\\item[14] 0.219280773210838\n",
       "\\item[15] 0.911661154107878\n",
       "\\item[16] 0.797950739188506\n",
       "\\item[17] 0.219280773210838\n",
       "\\item[18] 0.0970522051145409\n",
       "\\item[19] 0.601802744773117\n",
       "\\item[20] 0.601802744773117\n",
       "\\item[21] 0.423283291840528\n",
       "\\item[22] 0.0970522051145409\n",
       "\\item[23] 0.911661154107878\n",
       "\\item[24] 0.423283291840528\n",
       "\\item[25] 0.911661154107878\n",
       "\\item[26] 0.0970522051145409\n",
       "\\item[27] 0.911661154107878\n",
       "\\item[28] 0.0970522051145409\n",
       "\\item[29] 0.423283291840528\n",
       "\\item[30] 0.0970522051145409\n",
       "\\item[31] 0.219280773210838\n",
       "\\item[32] 0.219280773210838\n",
       "\\item[33] 0.601802744773117\n",
       "\\item[34] 0.601802744773117\n",
       "\\item[35] 0.423283291840528\n",
       "\\item[36] 0.0970522051145409\n",
       "\\item[37] 0.601802744773117\n",
       "\\item[38] 0.601802744773117\n",
       "\\item[39] 0.0970522051145409\n",
       "\\item[40] 0.0970522051145409\n",
       "\\item[41] 0.0970522051145409\n",
       "\\item[42] 0.423283291840528\n",
       "\\item[43] 0.0970522051145409\n",
       "\\item[44] 0.797950739188506\n",
       "\\item[45] 0.911661154107878\n",
       "\\item[46] 0.0970522051145409\n",
       "\\item[47] 0.423283291840528\n",
       "\\item[48] 0.0970522051145409\n",
       "\\item[49] 0.911661154107878\n",
       "\\item[50] 0.601802744773117\n",
       "\\item[51] 0.423283291840528\n",
       "\\item[52] 0.219280773210838\n",
       "\\item[53] 0.797950739188506\n",
       "\\item[54] 0.911661154107878\n",
       "\\item[55] 0.219280773210838\n",
       "\\item[56] 0.0970522051145409\n",
       "\\item[57] 0.0970522051145409\n",
       "\\item[58] 0.0970522051145409\n",
       "\\item[59] 0.0970522051145409\n",
       "\\item[60] 0.911661154107878\n",
       "\\item[61] 0.0970522051145409\n",
       "\\item[62] 0.219280773210838\n",
       "\\item[63] 0.0970522051145409\n",
       "\\item[64] 0.601802744773117\n",
       "\\item[65] 0.423283291840528\n",
       "\\item[66] 0.797950739188506\n",
       "\\item[67] 0.601802744773117\n",
       "\\item[68] 0.423283291840528\n",
       "\\item[69] 0.423283291840528\n",
       "\\item[70] 0.911661154107878\n",
       "\\item[71] 0.601802744773117\n",
       "\\item[72] 0.0970522051145409\n",
       "\\item[73] 0.601802744773117\n",
       "\\item[74] 0.423283291840528\n",
       "\\item[75] 0.911661154107878\n",
       "\\item[76] 0.423283291840528\n",
       "\\item[77] 0.0970522051145409\n",
       "\\item[78] 0.911661154107878\n",
       "\\item[79] 0.219280773210838\n",
       "\\item[80] 0.601802744773117\n",
       "\\item[81] 0.0970522051145409\n",
       "\\item[82] 0.423283291840528\n",
       "\\item[83] 0.423283291840528\n",
       "\\item[84] 0.0970522051145409\n",
       "\\item[85] 0.219280773210838\n",
       "\\item[86] 0.0970522051145409\n",
       "\\item[87] 0.601802744773117\n",
       "\\item[88] 0.601802744773117\n",
       "\\item[89] 0.601802744773117\n",
       "\\item[90] 0.219280773210838\n",
       "\\item[91] 0.601802744773117\n",
       "\\item[92] 0.0970522051145409\n",
       "\\item[93] 0.911661154107878\n",
       "\\item[94] 0.0970522051145409\n",
       "\\item[95] 0.423283291840528\n",
       "\\item[96] 0.0970522051145409\n",
       "\\item[97] 0.911661154107878\n",
       "\\item[98] 0.0970522051145409\n",
       "\\item[99] 0.601802744773117\n",
       "\\item[100] 0.0970522051145409\n",
       "\\item[101] 0.911661154107878\n",
       "\\item[102] 0.219280773210838\n",
       "\\item[103] 0.0970522051145409\n",
       "\\item[104] 0.0970522051145409\n",
       "\\item[105] 0.601802744773117\n",
       "\\item[106] 0.0970522051145409\n",
       "\\item[107] 0.0970522051145409\n",
       "\\item[108] 0.0970522051145409\n",
       "\\item[109] 0.0970522051145409\n",
       "\\item[110] 0.219280773210838\n",
       "\\item[111] 0.219280773210838\n",
       "\\item[112] 0.601802744773117\n",
       "\\item[113] 0.911661154107878\n",
       "\\item[114] 0.601802744773117\n",
       "\\item[115] 0.911661154107878\n",
       "\\item[116] 0.0970522051145409\n",
       "\\item[117] 0.0970522051145409\n",
       "\\item[118] 0.601802744773117\n",
       "\\item[119] 0.423283291840528\n",
       "\\item[120] 0.797950739188506\n",
       "\\item[121] 0.797950739188506\n",
       "\\item[122] 0.0970522051145409\n",
       "\\item[123] 0.911661154107878\n",
       "\\item[124] 0.0970522051145409\n",
       "\\item[125] 0.0970522051145409\n",
       "\\item[126] 0.601802744773117\n",
       "\\item[127] 0.0970522051145409\n",
       "\\item[128] 0.601802744773117\n",
       "\\item[129] 0.219280773210838\n",
       "\\item[130] 0.0970522051145409\n",
       "\\item[131] 0.0970522051145409\n",
       "\\item[132] 0.423283291840528\n",
       "\\item[133] 0.601802744773117\n",
       "\\item[134] 0.0970522051145409\n",
       "\\item[135] 0.0970522051145409\n",
       "\\item[136] 0.0970522051145409\n",
       "\\item[137] 0.0970522051145409\n",
       "\\item[138] 0.219280773210838\n",
       "\\item[139] 0.601802744773117\n",
       "\\item[140] 0.0970522051145409\n",
       "\\item[141] 0.601802744773117\n",
       "\\item[142] 0.911661154107878\n",
       "\\item[143] 0.423283291840528\n",
       "\\item[144] 0.219280773210838\n",
       "\\item[145] 0.423283291840528\n",
       "\\item[146] 0.0970522051145409\n",
       "\\item[147] 0.423283291840528\n",
       "\\item[148] 0.0970522051145409\n",
       "\\item[149] 0.423283291840528\n",
       "\\item[150] 0.219280773210838\n",
       "\\item[151] 0.911661154107878\n",
       "\\item[152] 0.0970522051145409\n",
       "\\item[153] 0.0970522051145409\n",
       "\\item[154] 0.601802744773117\n",
       "\\item[155] 0.0970522051145409\n",
       "\\item[156] 0.0970522051145409\n",
       "\\item[157] 0.911661154107878\n",
       "\\item[158] 0.601802744773117\n",
       "\\item[159] 0.423283291840528\n",
       "\\item[160] 0.601802744773117\n",
       "\\item[161] 0.601802744773117\n",
       "\\item[162] 0.0970522051145409\n",
       "\\item[163] 0.797950739188506\n",
       "\\item[164] 0.0970522051145409\n",
       "\\item[165] 0.219280773210838\n",
       "\\item[166] 0.601802744773117\n",
       "\\item[167] 0.423283291840528\n",
       "\\item[168] 0.0970522051145409\n",
       "\\item[169] 0.911661154107878\n",
       "\\item[170] 0.601802744773117\n",
       "\\item[171] 0.0970522051145409\n",
       "\\item[172] 0.0970522051145409\n",
       "\\item[173] 0.0970522051145409\n",
       "\\item[174] 0.0970522051145409\n",
       "\\item[175] 0.0970522051145409\n",
       "\\item[176] 0.797950739188506\n",
       "\\item[177] 0.797950739188506\n",
       "\\item[178] 0.423283291840528\n",
       "\\item[179] 0.797950739188506\n",
       "\\item[180] 0.911661154107878\n",
       "\\item[181] 0.219280773210838\n",
       "\\item[182] 0.423283291840528\n",
       "\\item[183] 0.911661154107878\n",
       "\\item[184] 0.0970522051145409\n",
       "\\item[185] 0.911661154107878\n",
       "\\item[186] 0.219280773210838\n",
       "\\item[187] 0.797950739188506\n",
       "\\item[188] 0.0970522051145409\n",
       "\\item[189] 0.601802744773117\n",
       "\\item[190] 0.219280773210838\n",
       "\\item[191] 0.219280773210838\n",
       "\\item[192] 0.423283291840528\n",
       "\\item[193] 0.0970522051145409\n",
       "\\item[194] 0.219280773210838\n",
       "\\item[195] 0.219280773210838\n",
       "\\item[196] 0.0970522051145409\n",
       "\\item[197] 0.423283291840528\n",
       "\\item[198] 0.601802744773117\n",
       "\\item[199] 0.219280773210838\n",
       "\\item[200] 0.601802744773117\n",
       "\\item[201] ⋯\n",
       "\\item[202] 0.911661154107878\n",
       "\\item[203] 0.0970522051145409\n",
       "\\item[204] 0.797950739188506\n",
       "\\item[205] 0.0970522051145409\n",
       "\\item[206] 0.797950739188506\n",
       "\\item[207] 0.0970522051145409\n",
       "\\item[208] 0.911661154107878\n",
       "\\item[209] 0.601802744773117\n",
       "\\item[210] 0.0970522051145409\n",
       "\\item[211] 0.601802744773117\n",
       "\\item[212] 0.0970522051145409\n",
       "\\item[213] 0.219280773210838\n",
       "\\item[214] 0.219280773210838\n",
       "\\item[215] 0.911661154107878\n",
       "\\item[216] 0.0970522051145409\n",
       "\\item[217] 0.0970522051145409\n",
       "\\item[218] 0.423283291840528\n",
       "\\item[219] 0.0970522051145409\n",
       "\\item[220] 0.423283291840528\n",
       "\\item[221] 0.0970522051145409\n",
       "\\item[222] 0.797950739188506\n",
       "\\item[223] 0.911661154107878\n",
       "\\item[224] 0.911661154107878\n",
       "\\item[225] 0.797950739188506\n",
       "\\item[226] 0.423283291840528\n",
       "\\item[227] 0.0970522051145409\n",
       "\\item[228] 0.0970522051145409\n",
       "\\item[229] 0.423283291840528\n",
       "\\item[230] 0.797950739188506\n",
       "\\item[231] 0.219280773210838\n",
       "\\item[232] 0.797950739188506\n",
       "\\item[233] 0.601802744773117\n",
       "\\item[234] 0.797950739188506\n",
       "\\item[235] 0.0970522051145409\n",
       "\\item[236] 0.423283291840528\n",
       "\\item[237] 0.0970522051145409\n",
       "\\item[238] 0.0970522051145409\n",
       "\\item[239] 0.0970522051145409\n",
       "\\item[240] 0.0970522051145409\n",
       "\\item[241] 0.0970522051145409\n",
       "\\item[242] 0.797950739188506\n",
       "\\item[243] 0.0970522051145409\n",
       "\\item[244] 0.0970522051145409\n",
       "\\item[245] 0.0970522051145409\n",
       "\\item[246] 0.797950739188506\n",
       "\\item[247] 0.601802744773117\n",
       "\\item[248] 0.219280773210838\n",
       "\\item[249] 0.0970522051145409\n",
       "\\item[250] 0.423283291840528\n",
       "\\item[251] 0.0970522051145409\n",
       "\\item[252] 0.601802744773117\n",
       "\\item[253] 0.0970522051145409\n",
       "\\item[254] 0.423283291840528\n",
       "\\item[255] 0.0970522051145409\n",
       "\\item[256] 0.911661154107878\n",
       "\\item[257] 0.601802744773117\n",
       "\\item[258] 0.0970522051145409\n",
       "\\item[259] 0.797950739188506\n",
       "\\item[260] 0.219280773210838\n",
       "\\item[261] 0.219280773210838\n",
       "\\item[262] 0.219280773210838\n",
       "\\item[263] 0.219280773210838\n",
       "\\item[264] 0.601802744773117\n",
       "\\item[265] 0.0970522051145409\n",
       "\\item[266] 0.601802744773117\n",
       "\\item[267] 0.601802744773117\n",
       "\\item[268] 0.601802744773117\n",
       "\\item[269] 0.0970522051145409\n",
       "\\item[270] 0.0970522051145409\n",
       "\\item[271] 0.423283291840528\n",
       "\\item[272] 0.0970522051145409\n",
       "\\item[273] 0.0970522051145409\n",
       "\\item[274] 0.423283291840528\n",
       "\\item[275] 0.601802744773117\n",
       "\\item[276] 0.0970522051145409\n",
       "\\item[277] 0.423283291840528\n",
       "\\item[278] 0.0970522051145409\n",
       "\\item[279] 0.0970522051145409\n",
       "\\item[280] 0.797950739188506\n",
       "\\item[281] 0.0970522051145409\n",
       "\\item[282] 0.423283291840528\n",
       "\\item[283] 0.0970522051145409\n",
       "\\item[284] 0.0970522051145409\n",
       "\\item[285] 0.219280773210838\n",
       "\\item[286] 0.219280773210838\n",
       "\\item[287] 0.0970522051145409\n",
       "\\item[288] 0.601802744773117\n",
       "\\item[289] 0.911661154107878\n",
       "\\item[290] 0.423283291840528\n",
       "\\item[291] 0.0970522051145409\n",
       "\\item[292] 0.423283291840528\n",
       "\\item[293] 0.601802744773117\n",
       "\\item[294] 0.0970522051145409\n",
       "\\item[295] 0.0970522051145409\n",
       "\\item[296] 0.0970522051145409\n",
       "\\item[297] 0.601802744773117\n",
       "\\item[298] 0.911661154107878\n",
       "\\item[299] 0.601802744773117\n",
       "\\item[300] 0.423283291840528\n",
       "\\item[301] 0.219280773210838\n",
       "\\item[302] 0.0970522051145409\n",
       "\\item[303] 0.219280773210838\n",
       "\\item[304] 0.0970522051145409\n",
       "\\item[305] 0.0970522051145409\n",
       "\\item[306] 0.219280773210838\n",
       "\\item[307] 0.423283291840528\n",
       "\\item[308] 0.911661154107878\n",
       "\\item[309] 0.0970522051145409\n",
       "\\item[310] 0.797950739188506\n",
       "\\item[311] 0.423283291840528\n",
       "\\item[312] 0.219280773210838\n",
       "\\item[313] 0.219280773210838\n",
       "\\item[314] 0.797950739188506\n",
       "\\item[315] 0.423283291840528\n",
       "\\item[316] 0.0970522051145409\n",
       "\\item[317] 0.601802744773117\n",
       "\\item[318] 0.0970522051145409\n",
       "\\item[319] 0.423283291840528\n",
       "\\item[320] 0.219280773210838\n",
       "\\item[321] 0.0970522051145409\n",
       "\\item[322] 0.219280773210838\n",
       "\\item[323] 0.0970522051145409\n",
       "\\item[324] 0.219280773210838\n",
       "\\item[325] 0.0970522051145409\n",
       "\\item[326] 0.0970522051145409\n",
       "\\item[327] 0.911661154107878\n",
       "\\item[328] 0.0970522051145409\n",
       "\\item[329] 0.601802744773117\n",
       "\\item[330] 0.219280773210838\n",
       "\\item[331] 0.601802744773117\n",
       "\\item[332] 0.219280773210838\n",
       "\\item[333] 0.797950739188506\n",
       "\\item[334] 0.911661154107878\n",
       "\\item[335] 0.219280773210838\n",
       "\\item[336] 0.219280773210838\n",
       "\\item[337] 0.219280773210838\n",
       "\\item[338] 0.601802744773117\n",
       "\\item[339] 0.423283291840528\n",
       "\\item[340] 0.911661154107878\n",
       "\\item[341] 0.0970522051145409\n",
       "\\item[342] 0.0970522051145409\n",
       "\\item[343] 0.601802744773117\n",
       "\\item[344] 0.0970522051145409\n",
       "\\item[345] 0.797950739188506\n",
       "\\item[346] 0.797950739188506\n",
       "\\item[347] 0.0970522051145409\n",
       "\\item[348] 0.911661154107878\n",
       "\\item[349] 0.601802744773117\n",
       "\\item[350] 0.0970522051145409\n",
       "\\item[351] 0.601802744773117\n",
       "\\item[352] 0.911661154107878\n",
       "\\item[353] 0.219280773210838\n",
       "\\item[354] 0.219280773210838\n",
       "\\item[355] 0.911661154107878\n",
       "\\item[356] 0.423283291840528\n",
       "\\item[357] 0.219280773210838\n",
       "\\item[358] 0.911661154107878\n",
       "\\item[359] 0.911661154107878\n",
       "\\item[360] 0.601802744773117\n",
       "\\item[361] 0.219280773210838\n",
       "\\item[362] 0.423283291840528\n",
       "\\item[363] 0.0970522051145409\n",
       "\\item[364] 0.0970522051145409\n",
       "\\item[365] 0.0970522051145409\n",
       "\\item[366] 0.601802744773117\n",
       "\\item[367] 0.601802744773117\n",
       "\\item[368] 0.219280773210838\n",
       "\\item[369] 0.797950739188506\n",
       "\\item[370] 0.0970522051145409\n",
       "\\item[371] 0.219280773210838\n",
       "\\item[372] 0.0970522051145409\n",
       "\\item[373] 0.0970522051145409\n",
       "\\item[374] 0.423283291840528\n",
       "\\item[375] 0.911661154107878\n",
       "\\item[376] 0.0970522051145409\n",
       "\\item[377] 0.219280773210838\n",
       "\\item[378] 0.0970522051145409\n",
       "\\item[379] 0.911661154107878\n",
       "\\item[380] 0.0970522051145409\n",
       "\\item[381] 0.911661154107878\n",
       "\\item[382] 0.0970522051145409\n",
       "\\item[383] 0.0970522051145409\n",
       "\\item[384] 0.911661154107878\n",
       "\\item[385] 0.219280773210838\n",
       "\\item[386] 0.911661154107878\n",
       "\\item[387] 0.423283291840528\n",
       "\\item[388] 0.423283291840528\n",
       "\\item[389] 0.219280773210838\n",
       "\\item[390] 0.219280773210838\n",
       "\\item[391] 0.423283291840528\n",
       "\\item[392] 0.601802744773117\n",
       "\\item[393] 0.601802744773117\n",
       "\\item[394] 0.601802744773117\n",
       "\\item[395] 0.911661154107878\n",
       "\\item[396] 0.601802744773117\n",
       "\\item[397] 0.0970522051145409\n",
       "\\item[398] 0.911661154107878\n",
       "\\item[399] 0.0970522051145409\n",
       "\\item[400] 0.0970522051145409\n",
       "\\item[401] 0.0970522051145409\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.09705220511454092\n",
       ":   0.6018027447731173\n",
       ":   0.2192807732108384\n",
       ":   0.09705220511454095\n",
       ":   0.6018027447731176\n",
       ":   0.09705220511454097\n",
       ":   0.6018027447731178\n",
       ":   0.2192807732108389\n",
       ":   0.60180274477311710\n",
       ":   0.097052205114540911\n",
       ":   0.097052205114540912\n",
       ":   0.42328329184052813\n",
       ":   0.91166115410787814\n",
       ":   0.21928077321083815\n",
       ":   0.91166115410787816\n",
       ":   0.79795073918850617\n",
       ":   0.21928077321083818\n",
       ":   0.097052205114540919\n",
       ":   0.60180274477311720\n",
       ":   0.60180274477311721\n",
       ":   0.42328329184052822\n",
       ":   0.097052205114540923\n",
       ":   0.91166115410787824\n",
       ":   0.42328329184052825\n",
       ":   0.91166115410787826\n",
       ":   0.097052205114540927\n",
       ":   0.91166115410787828\n",
       ":   0.097052205114540929\n",
       ":   0.42328329184052830\n",
       ":   0.097052205114540931\n",
       ":   0.21928077321083832\n",
       ":   0.21928077321083833\n",
       ":   0.60180274477311734\n",
       ":   0.60180274477311735\n",
       ":   0.42328329184052836\n",
       ":   0.097052205114540937\n",
       ":   0.60180274477311738\n",
       ":   0.60180274477311739\n",
       ":   0.097052205114540940\n",
       ":   0.097052205114540941\n",
       ":   0.097052205114540942\n",
       ":   0.42328329184052843\n",
       ":   0.097052205114540944\n",
       ":   0.79795073918850645\n",
       ":   0.91166115410787846\n",
       ":   0.097052205114540947\n",
       ":   0.42328329184052848\n",
       ":   0.097052205114540949\n",
       ":   0.91166115410787850\n",
       ":   0.60180274477311751\n",
       ":   0.42328329184052852\n",
       ":   0.21928077321083853\n",
       ":   0.79795073918850654\n",
       ":   0.91166115410787855\n",
       ":   0.21928077321083856\n",
       ":   0.097052205114540957\n",
       ":   0.097052205114540958\n",
       ":   0.097052205114540959\n",
       ":   0.097052205114540960\n",
       ":   0.91166115410787861\n",
       ":   0.097052205114540962\n",
       ":   0.21928077321083863\n",
       ":   0.097052205114540964\n",
       ":   0.60180274477311765\n",
       ":   0.42328329184052866\n",
       ":   0.79795073918850667\n",
       ":   0.60180274477311768\n",
       ":   0.42328329184052869\n",
       ":   0.42328329184052870\n",
       ":   0.91166115410787871\n",
       ":   0.60180274477311772\n",
       ":   0.097052205114540973\n",
       ":   0.60180274477311774\n",
       ":   0.42328329184052875\n",
       ":   0.91166115410787876\n",
       ":   0.42328329184052877\n",
       ":   0.097052205114540978\n",
       ":   0.91166115410787879\n",
       ":   0.21928077321083880\n",
       ":   0.60180274477311781\n",
       ":   0.097052205114540982\n",
       ":   0.42328329184052883\n",
       ":   0.42328329184052884\n",
       ":   0.097052205114540985\n",
       ":   0.21928077321083886\n",
       ":   0.097052205114540987\n",
       ":   0.60180274477311788\n",
       ":   0.60180274477311789\n",
       ":   0.60180274477311790\n",
       ":   0.21928077321083891\n",
       ":   0.60180274477311792\n",
       ":   0.097052205114540993\n",
       ":   0.91166115410787894\n",
       ":   0.097052205114540995\n",
       ":   0.42328329184052896\n",
       ":   0.097052205114540997\n",
       ":   0.91166115410787898\n",
       ":   0.097052205114540999\n",
       ":   0.601802744773117100\n",
       ":   0.0970522051145409101\n",
       ":   0.911661154107878102\n",
       ":   0.219280773210838103\n",
       ":   0.0970522051145409104\n",
       ":   0.0970522051145409105\n",
       ":   0.601802744773117106\n",
       ":   0.0970522051145409107\n",
       ":   0.0970522051145409108\n",
       ":   0.0970522051145409109\n",
       ":   0.0970522051145409110\n",
       ":   0.219280773210838111\n",
       ":   0.219280773210838112\n",
       ":   0.601802744773117113\n",
       ":   0.911661154107878114\n",
       ":   0.601802744773117115\n",
       ":   0.911661154107878116\n",
       ":   0.0970522051145409117\n",
       ":   0.0970522051145409118\n",
       ":   0.601802744773117119\n",
       ":   0.423283291840528120\n",
       ":   0.797950739188506121\n",
       ":   0.797950739188506122\n",
       ":   0.0970522051145409123\n",
       ":   0.911661154107878124\n",
       ":   0.0970522051145409125\n",
       ":   0.0970522051145409126\n",
       ":   0.601802744773117127\n",
       ":   0.0970522051145409128\n",
       ":   0.601802744773117129\n",
       ":   0.219280773210838130\n",
       ":   0.0970522051145409131\n",
       ":   0.0970522051145409132\n",
       ":   0.423283291840528133\n",
       ":   0.601802744773117134\n",
       ":   0.0970522051145409135\n",
       ":   0.0970522051145409136\n",
       ":   0.0970522051145409137\n",
       ":   0.0970522051145409138\n",
       ":   0.219280773210838139\n",
       ":   0.601802744773117140\n",
       ":   0.0970522051145409141\n",
       ":   0.601802744773117142\n",
       ":   0.911661154107878143\n",
       ":   0.423283291840528144\n",
       ":   0.219280773210838145\n",
       ":   0.423283291840528146\n",
       ":   0.0970522051145409147\n",
       ":   0.423283291840528148\n",
       ":   0.0970522051145409149\n",
       ":   0.423283291840528150\n",
       ":   0.219280773210838151\n",
       ":   0.911661154107878152\n",
       ":   0.0970522051145409153\n",
       ":   0.0970522051145409154\n",
       ":   0.601802744773117155\n",
       ":   0.0970522051145409156\n",
       ":   0.0970522051145409157\n",
       ":   0.911661154107878158\n",
       ":   0.601802744773117159\n",
       ":   0.423283291840528160\n",
       ":   0.601802744773117161\n",
       ":   0.601802744773117162\n",
       ":   0.0970522051145409163\n",
       ":   0.797950739188506164\n",
       ":   0.0970522051145409165\n",
       ":   0.219280773210838166\n",
       ":   0.601802744773117167\n",
       ":   0.423283291840528168\n",
       ":   0.0970522051145409169\n",
       ":   0.911661154107878170\n",
       ":   0.601802744773117171\n",
       ":   0.0970522051145409172\n",
       ":   0.0970522051145409173\n",
       ":   0.0970522051145409174\n",
       ":   0.0970522051145409175\n",
       ":   0.0970522051145409176\n",
       ":   0.797950739188506177\n",
       ":   0.797950739188506178\n",
       ":   0.423283291840528179\n",
       ":   0.797950739188506180\n",
       ":   0.911661154107878181\n",
       ":   0.219280773210838182\n",
       ":   0.423283291840528183\n",
       ":   0.911661154107878184\n",
       ":   0.0970522051145409185\n",
       ":   0.911661154107878186\n",
       ":   0.219280773210838187\n",
       ":   0.797950739188506188\n",
       ":   0.0970522051145409189\n",
       ":   0.601802744773117190\n",
       ":   0.219280773210838191\n",
       ":   0.219280773210838192\n",
       ":   0.423283291840528193\n",
       ":   0.0970522051145409194\n",
       ":   0.219280773210838195\n",
       ":   0.219280773210838196\n",
       ":   0.0970522051145409197\n",
       ":   0.423283291840528198\n",
       ":   0.601802744773117199\n",
       ":   0.219280773210838200\n",
       ":   0.601802744773117201\n",
       ":   ⋯202\n",
       ":   0.911661154107878203\n",
       ":   0.0970522051145409204\n",
       ":   0.797950739188506205\n",
       ":   0.0970522051145409206\n",
       ":   0.797950739188506207\n",
       ":   0.0970522051145409208\n",
       ":   0.911661154107878209\n",
       ":   0.601802744773117210\n",
       ":   0.0970522051145409211\n",
       ":   0.601802744773117212\n",
       ":   0.0970522051145409213\n",
       ":   0.219280773210838214\n",
       ":   0.219280773210838215\n",
       ":   0.911661154107878216\n",
       ":   0.0970522051145409217\n",
       ":   0.0970522051145409218\n",
       ":   0.423283291840528219\n",
       ":   0.0970522051145409220\n",
       ":   0.423283291840528221\n",
       ":   0.0970522051145409222\n",
       ":   0.797950739188506223\n",
       ":   0.911661154107878224\n",
       ":   0.911661154107878225\n",
       ":   0.797950739188506226\n",
       ":   0.423283291840528227\n",
       ":   0.0970522051145409228\n",
       ":   0.0970522051145409229\n",
       ":   0.423283291840528230\n",
       ":   0.797950739188506231\n",
       ":   0.219280773210838232\n",
       ":   0.797950739188506233\n",
       ":   0.601802744773117234\n",
       ":   0.797950739188506235\n",
       ":   0.0970522051145409236\n",
       ":   0.423283291840528237\n",
       ":   0.0970522051145409238\n",
       ":   0.0970522051145409239\n",
       ":   0.0970522051145409240\n",
       ":   0.0970522051145409241\n",
       ":   0.0970522051145409242\n",
       ":   0.797950739188506243\n",
       ":   0.0970522051145409244\n",
       ":   0.0970522051145409245\n",
       ":   0.0970522051145409246\n",
       ":   0.797950739188506247\n",
       ":   0.601802744773117248\n",
       ":   0.219280773210838249\n",
       ":   0.0970522051145409250\n",
       ":   0.423283291840528251\n",
       ":   0.0970522051145409252\n",
       ":   0.601802744773117253\n",
       ":   0.0970522051145409254\n",
       ":   0.423283291840528255\n",
       ":   0.0970522051145409256\n",
       ":   0.911661154107878257\n",
       ":   0.601802744773117258\n",
       ":   0.0970522051145409259\n",
       ":   0.797950739188506260\n",
       ":   0.219280773210838261\n",
       ":   0.219280773210838262\n",
       ":   0.219280773210838263\n",
       ":   0.219280773210838264\n",
       ":   0.601802744773117265\n",
       ":   0.0970522051145409266\n",
       ":   0.601802744773117267\n",
       ":   0.601802744773117268\n",
       ":   0.601802744773117269\n",
       ":   0.0970522051145409270\n",
       ":   0.0970522051145409271\n",
       ":   0.423283291840528272\n",
       ":   0.0970522051145409273\n",
       ":   0.0970522051145409274\n",
       ":   0.423283291840528275\n",
       ":   0.601802744773117276\n",
       ":   0.0970522051145409277\n",
       ":   0.423283291840528278\n",
       ":   0.0970522051145409279\n",
       ":   0.0970522051145409280\n",
       ":   0.797950739188506281\n",
       ":   0.0970522051145409282\n",
       ":   0.423283291840528283\n",
       ":   0.0970522051145409284\n",
       ":   0.0970522051145409285\n",
       ":   0.219280773210838286\n",
       ":   0.219280773210838287\n",
       ":   0.0970522051145409288\n",
       ":   0.601802744773117289\n",
       ":   0.911661154107878290\n",
       ":   0.423283291840528291\n",
       ":   0.0970522051145409292\n",
       ":   0.423283291840528293\n",
       ":   0.601802744773117294\n",
       ":   0.0970522051145409295\n",
       ":   0.0970522051145409296\n",
       ":   0.0970522051145409297\n",
       ":   0.601802744773117298\n",
       ":   0.911661154107878299\n",
       ":   0.601802744773117300\n",
       ":   0.423283291840528301\n",
       ":   0.219280773210838302\n",
       ":   0.0970522051145409303\n",
       ":   0.219280773210838304\n",
       ":   0.0970522051145409305\n",
       ":   0.0970522051145409306\n",
       ":   0.219280773210838307\n",
       ":   0.423283291840528308\n",
       ":   0.911661154107878309\n",
       ":   0.0970522051145409310\n",
       ":   0.797950739188506311\n",
       ":   0.423283291840528312\n",
       ":   0.219280773210838313\n",
       ":   0.219280773210838314\n",
       ":   0.797950739188506315\n",
       ":   0.423283291840528316\n",
       ":   0.0970522051145409317\n",
       ":   0.601802744773117318\n",
       ":   0.0970522051145409319\n",
       ":   0.423283291840528320\n",
       ":   0.219280773210838321\n",
       ":   0.0970522051145409322\n",
       ":   0.219280773210838323\n",
       ":   0.0970522051145409324\n",
       ":   0.219280773210838325\n",
       ":   0.0970522051145409326\n",
       ":   0.0970522051145409327\n",
       ":   0.911661154107878328\n",
       ":   0.0970522051145409329\n",
       ":   0.601802744773117330\n",
       ":   0.219280773210838331\n",
       ":   0.601802744773117332\n",
       ":   0.219280773210838333\n",
       ":   0.797950739188506334\n",
       ":   0.911661154107878335\n",
       ":   0.219280773210838336\n",
       ":   0.219280773210838337\n",
       ":   0.219280773210838338\n",
       ":   0.601802744773117339\n",
       ":   0.423283291840528340\n",
       ":   0.911661154107878341\n",
       ":   0.0970522051145409342\n",
       ":   0.0970522051145409343\n",
       ":   0.601802744773117344\n",
       ":   0.0970522051145409345\n",
       ":   0.797950739188506346\n",
       ":   0.797950739188506347\n",
       ":   0.0970522051145409348\n",
       ":   0.911661154107878349\n",
       ":   0.601802744773117350\n",
       ":   0.0970522051145409351\n",
       ":   0.601802744773117352\n",
       ":   0.911661154107878353\n",
       ":   0.219280773210838354\n",
       ":   0.219280773210838355\n",
       ":   0.911661154107878356\n",
       ":   0.423283291840528357\n",
       ":   0.219280773210838358\n",
       ":   0.911661154107878359\n",
       ":   0.911661154107878360\n",
       ":   0.601802744773117361\n",
       ":   0.219280773210838362\n",
       ":   0.423283291840528363\n",
       ":   0.0970522051145409364\n",
       ":   0.0970522051145409365\n",
       ":   0.0970522051145409366\n",
       ":   0.601802744773117367\n",
       ":   0.601802744773117368\n",
       ":   0.219280773210838369\n",
       ":   0.797950739188506370\n",
       ":   0.0970522051145409371\n",
       ":   0.219280773210838372\n",
       ":   0.0970522051145409373\n",
       ":   0.0970522051145409374\n",
       ":   0.423283291840528375\n",
       ":   0.911661154107878376\n",
       ":   0.0970522051145409377\n",
       ":   0.219280773210838378\n",
       ":   0.0970522051145409379\n",
       ":   0.911661154107878380\n",
       ":   0.0970522051145409381\n",
       ":   0.911661154107878382\n",
       ":   0.0970522051145409383\n",
       ":   0.0970522051145409384\n",
       ":   0.911661154107878385\n",
       ":   0.219280773210838386\n",
       ":   0.911661154107878387\n",
       ":   0.423283291840528388\n",
       ":   0.423283291840528389\n",
       ":   0.219280773210838390\n",
       ":   0.219280773210838391\n",
       ":   0.423283291840528392\n",
       ":   0.601802744773117393\n",
       ":   0.601802744773117394\n",
       ":   0.601802744773117395\n",
       ":   0.911661154107878396\n",
       ":   0.601802744773117397\n",
       ":   0.0970522051145409398\n",
       ":   0.911661154107878399\n",
       ":   0.0970522051145409400\n",
       ":   0.0970522051145409401\n",
       ":   0.0970522051145409\n",
       "\n"
      ],
      "text/plain": [
       "         1          2          3          4          5          6          7 \n",
       "0.09705221 0.60180274 0.21928077 0.09705221 0.60180274 0.09705221 0.60180274 \n",
       "         8          9         10         11         12         13         14 \n",
       "0.21928077 0.60180274 0.09705221 0.09705221 0.42328329 0.91166115 0.21928077 \n",
       "        15         16         17         18         19         20         21 \n",
       "0.91166115 0.79795074 0.21928077 0.09705221 0.60180274 0.60180274 0.42328329 \n",
       "        22         23         24         25         26         27         28 \n",
       "0.09705221 0.91166115 0.42328329 0.91166115 0.09705221 0.91166115 0.09705221 \n",
       "        29         30         31         32         33         34         35 \n",
       "0.42328329 0.09705221 0.21928077 0.21928077 0.60180274 0.60180274 0.42328329 \n",
       "        36         37         38         39         40         41         42 \n",
       "0.09705221 0.60180274 0.60180274 0.09705221 0.09705221 0.09705221 0.42328329 \n",
       "        43         44         45         46         47         48         49 \n",
       "0.09705221 0.79795074 0.91166115 0.09705221 0.42328329 0.09705221 0.91166115 \n",
       "        50         51         52         53         54         55         56 \n",
       "0.60180274 0.42328329 0.21928077 0.79795074 0.91166115 0.21928077 0.09705221 \n",
       "        57         58         59         60         61         62         63 \n",
       "0.09705221 0.09705221 0.09705221 0.91166115 0.09705221 0.21928077 0.09705221 \n",
       "        64         65         66         67         68         69         70 \n",
       "0.60180274 0.42328329 0.79795074 0.60180274 0.42328329 0.42328329 0.91166115 \n",
       "        71         72         73         74         75         76         77 \n",
       "0.60180274 0.09705221 0.60180274 0.42328329 0.91166115 0.42328329 0.09705221 \n",
       "        78         79         80         81         82         83         84 \n",
       "0.91166115 0.21928077 0.60180274 0.09705221 0.42328329 0.42328329 0.09705221 \n",
       "        85         86         87         88         89         90         91 \n",
       "0.21928077 0.09705221 0.60180274 0.60180274 0.60180274 0.21928077 0.60180274 \n",
       "        92         93         94         95         96         97         98 \n",
       "0.09705221 0.91166115 0.09705221 0.42328329 0.09705221 0.91166115 0.09705221 \n",
       "        99        100        101        102        103        104        105 \n",
       "0.60180274 0.09705221 0.91166115 0.21928077 0.09705221 0.09705221 0.60180274 \n",
       "       106        107        108        109        110        111        112 \n",
       "0.09705221 0.09705221 0.09705221 0.09705221 0.21928077 0.21928077 0.60180274 \n",
       "       113        114        115        116        117        118        119 \n",
       "0.91166115 0.60180274 0.91166115 0.09705221 0.09705221 0.60180274 0.42328329 \n",
       "       120        121        122        123        124        125        126 \n",
       "0.79795074 0.79795074 0.09705221 0.91166115 0.09705221 0.09705221 0.60180274 \n",
       "       127        128        129        130        131        132        133 \n",
       "0.09705221 0.60180274 0.21928077 0.09705221 0.09705221 0.42328329 0.60180274 \n",
       "       134        135        136        137        138        139        140 \n",
       "0.09705221 0.09705221 0.09705221 0.09705221 0.21928077 0.60180274 0.09705221 \n",
       "       141        142        143        144        145        146        147 \n",
       "0.60180274 0.91166115 0.42328329 0.21928077 0.42328329 0.09705221 0.42328329 \n",
       "       148        149        150        151        152        153        154 \n",
       "0.09705221 0.42328329 0.21928077 0.91166115 0.09705221 0.09705221 0.60180274 \n",
       "       155        156        157        158        159        160        161 \n",
       "0.09705221 0.09705221 0.91166115 0.60180274 0.42328329 0.60180274 0.60180274 \n",
       "       162        163        164        165        166        167        168 \n",
       "0.09705221 0.79795074 0.09705221 0.21928077 0.60180274 0.42328329 0.09705221 \n",
       "       169        170        171        172        173        174        175 \n",
       "0.91166115 0.60180274 0.09705221 0.09705221 0.09705221 0.09705221 0.09705221 \n",
       "       176        177        178        179        180        181        182 \n",
       "0.79795074 0.79795074 0.42328329 0.79795074 0.91166115 0.21928077 0.42328329 \n",
       "       183        184        185        186        187        188        189 \n",
       "0.91166115 0.09705221 0.91166115 0.21928077 0.79795074 0.09705221 0.60180274 \n",
       "       190        191        192        193        194        195        196 \n",
       "0.21928077 0.21928077 0.42328329 0.09705221 0.21928077 0.21928077 0.09705221 \n",
       "       197        198        199        200        201        202        203 \n",
       "0.42328329 0.60180274 0.21928077 0.60180274 0.60180274 0.09705221 0.42328329 \n",
       "       204        205        206        207        208        209        210 \n",
       "0.79795074 0.21928077 0.42328329 0.60180274 0.21928077 0.91166115 0.09705221 \n",
       "       211        212        213        214        215        216        217 \n",
       "0.09705221 0.09705221 0.21928077 0.79795074 0.60180274 0.42328329 0.60180274 \n",
       "       218        219        220        221        222        223        224 \n",
       "0.42328329 0.91166115 0.09705221 0.79795074 0.09705221 0.79795074 0.09705221 \n",
       "       225        226        227        228        229        230        231 \n",
       "0.91166115 0.60180274 0.09705221 0.60180274 0.09705221 0.21928077 0.21928077 \n",
       "       232        233        234        235        236        237        238 \n",
       "0.91166115 0.09705221 0.09705221 0.42328329 0.09705221 0.42328329 0.09705221 \n",
       "       239        240        241        242        243        244        245 \n",
       "0.79795074 0.91166115 0.91166115 0.79795074 0.42328329 0.09705221 0.09705221 \n",
       "       246        247        248        249        250        251        252 \n",
       "0.42328329 0.79795074 0.21928077 0.79795074 0.60180274 0.79795074 0.09705221 \n",
       "       253        254        255        256        257        258        259 \n",
       "0.42328329 0.09705221 0.09705221 0.09705221 0.09705221 0.09705221 0.79795074 \n",
       "       260        261        262        263        264        265        266 \n",
       "0.09705221 0.09705221 0.09705221 0.79795074 0.60180274 0.21928077 0.09705221 \n",
       "       267        268        269        270        271        272        273 \n",
       "0.42328329 0.09705221 0.60180274 0.09705221 0.42328329 0.09705221 0.91166115 \n",
       "       274        275        276        277        278        279        280 \n",
       "0.60180274 0.09705221 0.79795074 0.21928077 0.21928077 0.21928077 0.21928077 \n",
       "       281        282        283        284        285        286        287 \n",
       "0.60180274 0.09705221 0.60180274 0.60180274 0.60180274 0.09705221 0.09705221 \n",
       "       288        289        290        291        292        293        294 \n",
       "0.42328329 0.09705221 0.09705221 0.42328329 0.60180274 0.09705221 0.42328329 \n",
       "       295        296        297        298        299        300        301 \n",
       "0.09705221 0.09705221 0.79795074 0.09705221 0.42328329 0.09705221 0.09705221 \n",
       "       302        303        304        305        306        307        308 \n",
       "0.21928077 0.21928077 0.09705221 0.60180274 0.91166115 0.42328329 0.09705221 \n",
       "       309        310        311        312        313        314        315 \n",
       "0.42328329 0.60180274 0.09705221 0.09705221 0.09705221 0.60180274 0.91166115 \n",
       "       316        317        318        319        320        321        322 \n",
       "0.60180274 0.42328329 0.21928077 0.09705221 0.21928077 0.09705221 0.09705221 \n",
       "       323        324        325        326        327        328        329 \n",
       "0.21928077 0.42328329 0.91166115 0.09705221 0.79795074 0.42328329 0.21928077 \n",
       "       330        331        332        333        334        335        336 \n",
       "0.21928077 0.79795074 0.42328329 0.09705221 0.60180274 0.09705221 0.42328329 \n",
       "       337        338        339        340        341        342        343 \n",
       "0.21928077 0.09705221 0.21928077 0.09705221 0.21928077 0.09705221 0.09705221 \n",
       "       344        345        346        347        348        349        350 \n",
       "0.91166115 0.09705221 0.60180274 0.21928077 0.60180274 0.21928077 0.79795074 \n",
       "       351        352        353        354        355        356        357 \n",
       "0.91166115 0.21928077 0.21928077 0.21928077 0.60180274 0.42328329 0.91166115 \n",
       "       358        359        360        361        362        363        364 \n",
       "0.09705221 0.09705221 0.60180274 0.09705221 0.79795074 0.79795074 0.09705221 \n",
       "       365        366        367        368        369        370        371 \n",
       "0.91166115 0.60180274 0.09705221 0.60180274 0.91166115 0.21928077 0.21928077 \n",
       "       372        373        374        375        376        377        378 \n",
       "0.91166115 0.42328329 0.21928077 0.91166115 0.91166115 0.60180274 0.21928077 \n",
       "       379        380        381        382        383        384        385 \n",
       "0.42328329 0.09705221 0.09705221 0.09705221 0.60180274 0.60180274 0.21928077 \n",
       "       386        387        388        389        390        391        392 \n",
       "0.79795074 0.09705221 0.21928077 0.09705221 0.09705221 0.42328329 0.91166115 \n",
       "       393        394        395        396        397        398        399 \n",
       "0.09705221 0.21928077 0.09705221 0.91166115 0.09705221 0.91166115 0.09705221 \n",
       "       400        401        402        403        404        405        406 \n",
       "0.09705221 0.91166115 0.21928077 0.91166115 0.42328329 0.42328329 0.21928077 \n",
       "       407        408        409        410        411        412        413 \n",
       "0.21928077 0.42328329 0.60180274 0.60180274 0.60180274 0.91166115 0.60180274 \n",
       "       414        415        416        417        418 \n",
       "0.09705221 0.91166115 0.09705221 0.09705221 0.09705221 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the trained model for predict the survivor in test set\n",
    "titanic_testing$Survived <- predict(titanic_glm, titanic_testing, type=\"response\") # Question: why type=\"response\"\n",
    "titanic_testing$Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(randomForest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**: train a `random forest` for predicting  `Survived` based on `Pclass` and `Sex`, then apply the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                Length Class  Mode     \n",
       "call               5   -none- call     \n",
       "type               1   -none- character\n",
       "predicted        891   factor numeric  \n",
       "err.rate          30   -none- numeric  \n",
       "confusion          6   -none- numeric  \n",
       "votes           1782   matrix numeric  \n",
       "oob.times        891   -none- numeric  \n",
       "classes            2   -none- character\n",
       "importance         8   -none- numeric  \n",
       "importanceSD       6   -none- numeric  \n",
       "localImportance    0   -none- NULL     \n",
       "proximity          0   -none- NULL     \n",
       "ntree              1   -none- numeric  \n",
       "mtry               1   -none- numeric  \n",
       "forest            14   -none- list     \n",
       "y                891   factor numeric  \n",
       "test               0   -none- NULL     \n",
       "inbag              0   -none- NULL     \n",
       "terms              3   terms  call     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# install.packages('randomForest')\n",
    "\n",
    "# train the model\n",
    "titanic_rf <- randomForest(Survived ~ Pclass + Sex, data = titanic_training, importance=TRUE, ntree=10)\n",
    "\n",
    "# examine the model\n",
    "summary(titanic_rf)\n",
    "\n",
    "# Apply the trained model for predict the survivor in test set\n",
    "\n",
    "titanic_testing$Survived <- predict(titanic_rf, titanic_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance and Evaluation\n",
    "\n",
    "#### Measures of goodness\n",
    "We trained and applied some models to predict labels for un-labeled data, but how can we say if the model is good?\n",
    "\n",
    "The goodness of prediction models are measured w.r.t some dataset with groudtruth -- i.e., we need the labels for observations in test data. Typical measures for the goodness of the classification models are [Precision, Recall](https://en.wikipedia.org/wiki/Precision_and_recall), and [F1 scores](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: we will train a model and evaluate its performance using [Iris dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m150\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m5\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (1): X5\n",
      "\u001b[32mdbl\u001b[39m (4): X1, X2, X3, X4\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "iris <- read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', col_names = FALSE)\n",
    "\n",
    "# name the columns\n",
    "names(iris) <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>sepal_length</th><th scope=col>sepal_width</th><th scope=col>petal_length</th><th scope=col>petal_width</th><th scope=col>class</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr>\n",
       "\t<tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>Iris-setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " sepal\\_length & sepal\\_width & petal\\_length & petal\\_width & class\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa\\\\\n",
       "\t 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa\\\\\n",
       "\t 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa\\\\\n",
       "\t 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa\\\\\n",
       "\t 5.0 & 3.6 & 1.4 & 0.2 & Iris-setosa\\\\\n",
       "\t 5.4 & 3.9 & 1.7 & 0.4 & Iris-setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 5\n",
       "\n",
       "| sepal_length &lt;dbl&gt; | sepal_width &lt;dbl&gt; | petal_length &lt;dbl&gt; | petal_width &lt;dbl&gt; | class &lt;chr&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa |\n",
       "| 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa |\n",
       "| 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa |\n",
       "| 5.4 | 3.9 | 1.7 | 0.4 | Iris-setosa |\n",
       "\n"
      ],
      "text/plain": [
       "  sepal_length sepal_width petal_length petal_width class      \n",
       "1 5.1          3.5         1.4          0.2         Iris-setosa\n",
       "2 4.9          3.0         1.4          0.2         Iris-setosa\n",
       "3 4.7          3.2         1.3          0.2         Iris-setosa\n",
       "4 4.6          3.1         1.5          0.2         Iris-setosa\n",
       "5 5.0          3.6         1.4          0.2         Iris-setosa\n",
       "6 5.4          3.9         1.7          0.4         Iris-setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spc_tbl_ [150 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n",
      " $ sepal_length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n",
      " $ sepal_width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n",
      " $ petal_length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n",
      " $ petal_width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n",
      " $ class       : chr [1:150] \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" \"Iris-setosa\" ...\n",
      " - attr(*, \"spec\")=\n",
      "  .. cols(\n",
      "  ..   X1 = \u001b[32mcol_double()\u001b[39m,\n",
      "  ..   X2 = \u001b[32mcol_double()\u001b[39m,\n",
      "  ..   X3 = \u001b[32mcol_double()\u001b[39m,\n",
      "  ..   X4 = \u001b[32mcol_double()\u001b[39m,\n",
      "  ..   X5 = \u001b[31mcol_character()\u001b[39m\n",
      "  .. )\n",
      " - attr(*, \"problems\")=<externalptr> \n"
     ]
    }
   ],
   "source": [
    "# view some rows\n",
    "head(iris)\n",
    "\n",
    "# view columns's data type\n",
    "str(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert `class` from string to factor\n",
    "iris$class <- as.factor(iris$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the models, often we **divide** the set of labeled data into **training and test sets**. The models will be **trained on the training set**, and **evaluated using the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the iris dataset into: 80% for training set and the remaining 20% for test set \n",
    "training_size <- floor(0.8 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a svm model to predict `class` from `sepal_length` and `sepal_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and load the 'e1071' package if not yet\n",
    "# install.packages('e1071')\n",
    "library(e1071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris_train[features], y=iris_train$class, kernel =\"linear\", cost=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the trained model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and load the 'mltest' package if not yet\n",
    "# install.packages('mltest', repos = 'https://cran.r-project.org/')\n",
    "library(mltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(svm_model, iris_test[features]))\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.766666666666667"
      ],
      "text/latex": [
       "0.766666666666667"
      ],
      "text/markdown": [
       "0.766666666666667"
      ],
      "text/plain": [
       "[1] 0.7666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# overall classification accuracy\n",
    "classifier_metrics$accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.615384615384615</dd><dt>Iris-virginica</dt><dd>0.8</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.615384615384615\n",
       "\\item[Iris-virginica] 0.8\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.615384615384615Iris-virginica\n",
       ":   0.8\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.6153846       0.8000000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# precision for classes\n",
    "classifier_metrics$precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.8</dd><dt>Iris-virginica</dt><dd>0.615384615384615</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.8\n",
       "\\item[Iris-virginica] 0.615384615384615\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.8Iris-virginica\n",
       ":   0.615384615384615\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.8000000       0.6153846 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recall for classes\n",
    "classifier_metrics$recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.695652173913043</dd><dt>Iris-virginica</dt><dd>0.695652173913043</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.695652173913043\n",
       "\\item[Iris-virginica] 0.695652173913043\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.695652173913043Iris-virginica\n",
       ":   0.695652173913043\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.6956522       0.6956522 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F1-measures for classes\n",
    "classifier_metrics$F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Divide the iris dataset into training and test sets by ratio 5:5, train a svm model to predict `class` using all feature, then examine the performance of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.96</dd><dt>Iris-virginica</dt><dd>0.92</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.96\n",
       "\\item[Iris-virginica] 0.92\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.96Iris-virginica\n",
       ":   0.92\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "           1.00            0.96            0.92 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.923076923076923</dd><dt>Iris-virginica</dt><dd>0.958333333333333</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.923076923076923\n",
       "\\item[Iris-virginica] 0.958333333333333\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.923076923076923Iris-virginica\n",
       ":   0.958333333333333\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9230769       0.9583333 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.941176470588235</dd><dt>Iris-virginica</dt><dd>0.938775510204082</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.941176470588235\n",
       "\\item[Iris-virginica] 0.938775510204082\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.941176470588235Iris-virginica\n",
       ":   0.938775510204082\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9411765       0.9387755 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "#divide the iris dataset into training and test sets by ratio 50:50\n",
    "training_size <- floor(0.5 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]\n",
    "\n",
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris_train[features], y=iris_train$class, kernel =\"linear\", cost=1)\n",
    "\n",
    "# get the prediction\n",
    "predicted_labels <- as.factor(predict(svm_model, iris_test[features]))\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics <- ml_test(predicted_labels, true_labels, output.as.table = FALSE)\n",
    "\n",
    "\n",
    "# precision for classes\n",
    "classifier_metrics$precision\n",
    "# recall for classes\n",
    "classifier_metrics$recall\n",
    "# F1-measures for classes\n",
    "classifier_metrics$F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting\n",
    "\n",
    "Underfitting happens when the trained model does not fit well with the training data, and thus often lead to poor performance on test data\n",
    "\n",
    "[Overfitting](https://en.wikipedia.org/wiki/Overfitting) happens when the trained model fit too well with the training data while perform (predict) poorly on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 -- 5 points \n",
    "\n",
    "Follow the above example and divide the iris dataset into training set and test set by ratio 5:95 and 20:80, and change the number of tree (`ntree`) in the model to 1 and 5 and 10. Examine the performance of the trained models on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=1\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.853658536585366</dd><dt>Iris-virginica</dt><dd>0.886792452830189</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.853658536585366\n",
       "\\item[Iris-virginica] 0.886792452830189\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.853658536585366Iris-virginica\n",
       ":   0.886792452830189\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.8536585       0.8867925 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=5\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.850574712643678</dd><dt>Iris-virginica</dt><dd>0.871287128712871</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.850574712643678\n",
       "\\item[Iris-virginica] 0.871287128712871\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.850574712643678Iris-virginica\n",
       ":   0.871287128712871\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.8505747       0.8712871 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=10\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>NaN</dd><dt>Iris-versicolor</dt><dd>0.569444444444445</dd><dt>Iris-virginica</dt><dd>0.860215053763441</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] NaN\n",
       "\\item[Iris-versicolor] 0.569444444444445\n",
       "\\item[Iris-virginica] 0.860215053763441\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   NaNIris-versicolor\n",
       ":   0.569444444444445Iris-virginica\n",
       ":   0.860215053763441\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "            NaN       0.5694444       0.8602151 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "training_size <- floor(0.05 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]\n",
    "\n",
    "# train a random forest model\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "iris_rf1 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=1)\n",
    "iris_rf5 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=5)\n",
    "iris_rf10 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# get the prediction\n",
    "predicted_labels1 <- as.factor(predict(iris_rf1, iris_test[features]))\n",
    "predicted_labels5 <- as.factor(predict(iris_rf5, iris_test[features]))\n",
    "predicted_labels10 <- as.factor(predict(iris_rf10, iris_test[features]))\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics1 <- ml_test(predicted_labels1, true_labels, output.as.table = FALSE)\n",
    "classifier_metrics5 <- ml_test(predicted_labels5, true_labels, output.as.table = FALSE)\n",
    "classifier_metrics10 <- ml_test(predicted_labels10, true_labels, output.as.table = FALSE)\n",
    "\n",
    "# F1-measures for classes\n",
    "print('ncol=1')\n",
    "classifier_metrics1$F1\n",
    "print('ncol=5')\n",
    "classifier_metrics5$F1\n",
    "print('ncol=10')\n",
    "classifier_metrics10$F1\n",
    "# get the groundtruth\n",
    "# get the prediction\n",
    "\n",
    "# measure the performance\n",
    "# F1-measures for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=1\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>0.828282828282828</dd><dt>Iris-versicolor</dt><dd>0.656716417910448</dd><dt>Iris-virginica</dt><dd>0.918918918918919</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 0.828282828282828\n",
       "\\item[Iris-versicolor] 0.656716417910448\n",
       "\\item[Iris-virginica] 0.918918918918919\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   0.828282828282828Iris-versicolor\n",
       ":   0.656716417910448Iris-virginica\n",
       ":   0.918918918918919\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      0.8282828       0.6567164       0.9189189 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=5\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>0.98876404494382</dd><dt>Iris-versicolor</dt><dd>0.883116883116883</dd><dt>Iris-virginica</dt><dd>0.891891891891892</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 0.98876404494382\n",
       "\\item[Iris-versicolor] 0.883116883116883\n",
       "\\item[Iris-virginica] 0.891891891891892\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   0.98876404494382Iris-versicolor\n",
       ":   0.883116883116883Iris-virginica\n",
       ":   0.891891891891892\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      0.9887640       0.8831169       0.8918919 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"ncol=10\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>Iris-setosa</dt><dd>1</dd><dt>Iris-versicolor</dt><dd>0.923076923076923</dd><dt>Iris-virginica</dt><dd>0.918918918918919</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Iris-setosa] 1\n",
       "\\item[Iris-versicolor] 0.923076923076923\n",
       "\\item[Iris-virginica] 0.918918918918919\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Iris-setosa\n",
       ":   1Iris-versicolor\n",
       ":   0.923076923076923Iris-virginica\n",
       ":   0.918918918918919\n",
       "\n"
      ],
      "text/plain": [
       "    Iris-setosa Iris-versicolor  Iris-virginica \n",
       "      1.0000000       0.9230769       0.9189189 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "training_size <- floor(0.2 * nrow(iris))\n",
    "train_indexes <- sample(seq_len(nrow(iris)), size = training_size)\n",
    "iris_train <- iris[train_indexes, ]\n",
    "iris_test <- iris[-train_indexes, ]\n",
    "\n",
    "# train a random forest model\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "iris_rf1 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=1)\n",
    "iris_rf5 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=5)\n",
    "iris_rf10 <- randomForest(class ~ sepal_length + sepal_width + petal_length + petal_width, data = iris_train, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the groundtruth\n",
    "true_labels <- as.factor(iris_test$class)\n",
    "\n",
    "# get the prediction\n",
    "predicted_labels1 <- as.factor(predict(iris_rf1, iris_test[features]))\n",
    "predicted_labels5 <- as.factor(predict(iris_rf5, iris_test[features]))\n",
    "predicted_labels10 <- as.factor(predict(iris_rf10, iris_test[features]))\n",
    "\n",
    "# measure the performance\n",
    "classifier_metrics1 <- ml_test(predicted_labels1, true_labels, output.as.table = FALSE)\n",
    "classifier_metrics5 <- ml_test(predicted_labels5, true_labels, output.as.table = FALSE)\n",
    "classifier_metrics10 <- ml_test(predicted_labels10, true_labels, output.as.table = FALSE)\n",
    "\n",
    "# F1-measures for classes\n",
    "print('ncol=1')\n",
    "classifier_metrics1$F1\n",
    "print('ncol=5')\n",
    "classifier_metrics5$F1\n",
    "print('ncol=10')\n",
    "classifier_metrics10$F1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "As we see in the previous section, the performance of models are highly dependent of the datasets. Hence, their performance can be high or low by chance, e.g., by the way we divide the original labeled dataset into training and test set. To reduce the effect of the chance, or to get more robust, reliable measures for the performance, we need to run K-fold [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "Basically, K-fold cross validation includes:\n",
    "\n",
    "- Devide the labeled dataset in to K equal folds\n",
    "- In turn, take one fold as test set, the union of other K-1 folds is used as train set\n",
    "- Train a model for each division of training & test sets, i.e., K models in total\n",
    "- Evaluate each model on corresponding test set\n",
    "- Aggregate the peformance of K models to form the final performance\n",
    "\n",
    "The prediction is for a new data observation is then formed by aggreating (e.g., voting base) the predictions of the above K models\n",
    "\n",
    "Refer to `trainControl` object and `train` function in `caret` package for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "One important aspect of the shallow classification models is the importance score of features in (trained) models. These scores inform us how important the features are in predicting the label of data observations.\n",
    "\n",
    "**Example** we will examine the importance of features in `random forest` and `svm` (with `linear kernel`) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 4 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Feature</th><th scope=col>Importance</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>sepal_length</td><td>1.6923953</td></tr>\n",
       "\t<tr><td>sepal_width </td><td>0.6277268</td></tr>\n",
       "\t<tr><td>petal_length</td><td>3.7890872</td></tr>\n",
       "\t<tr><td>petal_width </td><td>5.1027621</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 4 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Feature & Importance\\\\\n",
       " <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t sepal\\_length & 1.6923953\\\\\n",
       "\t sepal\\_width  & 0.6277268\\\\\n",
       "\t petal\\_length & 3.7890872\\\\\n",
       "\t petal\\_width  & 5.1027621\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 4 × 2\n",
       "\n",
       "| Feature &lt;chr&gt; | Importance &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| sepal_length | 1.6923953 |\n",
       "| sepal_width  | 0.6277268 |\n",
       "| petal_length | 3.7890872 |\n",
       "| petal_width  | 5.1027621 |\n",
       "\n"
      ],
      "text/plain": [
       "  Feature      Importance\n",
       "1 sepal_length 1.6923953 \n",
       "2 sepal_width  0.6277268 \n",
       "3 petal_length 3.7890872 \n",
       "4 petal_width  5.1027621 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAAOVBMVEUAAAAAAIszMzNNTU1o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\nYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finding the importance scores of features in `random forest` model is quite simple\n",
    "\n",
    "# train a random forest model\n",
    "iris_rf <- randomForest(class ~ ., data = iris, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the features importance\n",
    "imp <- importance(iris_rf, type=1)\n",
    "featureImportance <- tibble(Feature=row.names(imp), Importance=imp[,1])\n",
    "\n",
    "# show the importance scores\n",
    "featureImportance\n",
    "\n",
    "# visualizing the importance scores\n",
    "ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +\n",
    "    geom_bar(stat=\"identity\", fill='darkblue') +\n",
    "    coord_flip() +\n",
    "    xlab(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petal_length  petal_width  sepal_width sepal_length \n",
      "   3.0676386    3.0183477    0.8116943    0.1407988 \n"
     ]
    }
   ],
   "source": [
    "# It is a bit more complicated with SVM-linear model\n",
    "\n",
    "# set the features\n",
    "features <- c('sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
    "# train a svm model\n",
    "svm_model <- svm(x=iris[features], y=iris$class, kernel =\"linear\", cost=1)\n",
    "w <- t(svm_model$coefs) %*% svm_model$SV        # weight vectors\n",
    "w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight\n",
    "w <- sort(w, decreasing = T)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 -- 5 points\n",
    "\n",
    "Using [US's income dataset](https://archive.ics.uci.edu/ml/datasets/Adult), train a random forest model for predicting `income` from all other features, examine the importance of the features for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m32561\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m15\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m (9): X2, X4, X6, X7, X8, X9, X10, X14, X15\n",
      "\u001b[32mdbl\u001b[39m (6): X1, X3, X5, X11, X12, X13\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 14 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Feature</th><th scope=col>Importance</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>age          </td><td>12.03425567</td></tr>\n",
       "\t<tr><td>workclass    </td><td> 6.79721328</td></tr>\n",
       "\t<tr><td>fnlwgt       </td><td> 0.08412396</td></tr>\n",
       "\t<tr><td>education    </td><td> 7.45712473</td></tr>\n",
       "\t<tr><td>educationNum </td><td> 6.20892358</td></tr>\n",
       "\t<tr><td>maritalStatus</td><td> 7.45948047</td></tr>\n",
       "\t<tr><td>occupation   </td><td>18.84216809</td></tr>\n",
       "\t<tr><td>relationship </td><td> 5.59459534</td></tr>\n",
       "\t<tr><td>race         </td><td> 0.84636493</td></tr>\n",
       "\t<tr><td>sex          </td><td> 4.20596120</td></tr>\n",
       "\t<tr><td>capitalGain  </td><td>26.50122103</td></tr>\n",
       "\t<tr><td>capitalLoss  </td><td>14.45652832</td></tr>\n",
       "\t<tr><td>hoursPerWeek </td><td> 9.09998649</td></tr>\n",
       "\t<tr><td>nativeCountry</td><td> 4.29891276</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 14 × 2\n",
       "\\begin{tabular}{ll}\n",
       " Feature & Importance\\\\\n",
       " <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t age           & 12.03425567\\\\\n",
       "\t workclass     &  6.79721328\\\\\n",
       "\t fnlwgt        &  0.08412396\\\\\n",
       "\t education     &  7.45712473\\\\\n",
       "\t educationNum  &  6.20892358\\\\\n",
       "\t maritalStatus &  7.45948047\\\\\n",
       "\t occupation    & 18.84216809\\\\\n",
       "\t relationship  &  5.59459534\\\\\n",
       "\t race          &  0.84636493\\\\\n",
       "\t sex           &  4.20596120\\\\\n",
       "\t capitalGain   & 26.50122103\\\\\n",
       "\t capitalLoss   & 14.45652832\\\\\n",
       "\t hoursPerWeek  &  9.09998649\\\\\n",
       "\t nativeCountry &  4.29891276\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 14 × 2\n",
       "\n",
       "| Feature &lt;chr&gt; | Importance &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| age           | 12.03425567 |\n",
       "| workclass     |  6.79721328 |\n",
       "| fnlwgt        |  0.08412396 |\n",
       "| education     |  7.45712473 |\n",
       "| educationNum  |  6.20892358 |\n",
       "| maritalStatus |  7.45948047 |\n",
       "| occupation    | 18.84216809 |\n",
       "| relationship  |  5.59459534 |\n",
       "| race          |  0.84636493 |\n",
       "| sex           |  4.20596120 |\n",
       "| capitalGain   | 26.50122103 |\n",
       "| capitalLoss   | 14.45652832 |\n",
       "| hoursPerWeek  |  9.09998649 |\n",
       "| nativeCountry |  4.29891276 |\n",
       "\n"
      ],
      "text/plain": [
       "   Feature       Importance \n",
       "1  age           12.03425567\n",
       "2  workclass      6.79721328\n",
       "3  fnlwgt         0.08412396\n",
       "4  education      7.45712473\n",
       "5  educationNum   6.20892358\n",
       "6  maritalStatus  7.45948047\n",
       "7  occupation    18.84216809\n",
       "8  relationship   5.59459534\n",
       "9  race           0.84636493\n",
       "10 sex            4.20596120\n",
       "11 capitalGain   26.50122103\n",
       "12 capitalLoss   14.45652832\n",
       "13 hoursPerWeek   9.09998649\n",
       "14 nativeCountry  4.29891276"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAAOVBMVEUAAAAAAIszMzNNTU1o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\nlHsAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# Hint: columns of character data type should be converted into factor before traning and testing the model\n",
    "\n",
    "# read the dataset\n",
    "adult <- read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', col_names = FALSE)\n",
    "\n",
    "# rename the columns\n",
    "names(adult) <- c('age', 'workclass', 'fnlwgt', 'education', 'educationNum', 'maritalStatus', 'occupation', 'relationship', 'race', 'sex', 'capitalGain', 'capitalLoss', 'hoursPerWeek', 'nativeCountry', 'income')\n",
    "\n",
    "# convert string columns into facto\n",
    "adult$workclass <- as.factor(adult$workclass)\n",
    "adult$education <- as.factor(adult$education)\n",
    "adult$maritalStatus <- as.factor(adult$maritalStatus)\n",
    "adult$occupation <- as.factor(adult$occupation)\n",
    "adult$relationship <- as.factor(adult$relationship)\n",
    "adult$race <- as.factor(adult$race)\n",
    "adult$sex <- as.factor(adult$sex)\n",
    "adult$nativeCountry <- as.factor(adult$nativeCountry)\n",
    "adult$income <- as.factor(adult$income)\n",
    "\n",
    "# train a random forest to predict income\n",
    "adult_rf <- randomForest(income ~ ., data = adult, importance=TRUE, ntree=10)\n",
    "\n",
    "# get the features importance\n",
    "imp <- importance(adult_rf, type=1)\n",
    "featureImportance <- tibble(Feature=row.names(imp), Importance=imp[,1])\n",
    "\n",
    "# show the importance scores\n",
    "featureImportance\n",
    "\n",
    "# visualizing the importance scores\n",
    "ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +\n",
    "    geom_bar(stat=\"identity\", fill='darkblue') +\n",
    "    coord_flip() +\n",
    "    xlab(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "[Unsupervised machine learning](https://en.wikipedia.org/wiki/Unsupervised_learning) is the machine learning task of uncovering the hidden structure from \"unlabeled\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Clustering**\n",
    "    \n",
    "    kmeans_model <- kmeans(x=X, centers=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use movies data on the [MovieLens 100K Dataset](http://files.grouplens.org/datasets/movielens/ml-100k/u.item) collected from the [MovieLens web site](http://movielens.org). It is available as `data/movies.txt` inside the directory of this lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m1682\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m20\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \"|\"\n",
      "\u001b[31mchr\u001b[39m  (1): Title\n",
      "\u001b[32mdbl\u001b[39m (19): unknown, Action, Adventure, Animation, Children's, Comedy, Crime, ...\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    }
   ],
   "source": [
    "movies <- read_delim('movies.txt', delim = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 6 × 20</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Title</th><th scope=col>unknown</th><th scope=col>Action</th><th scope=col>Adventure</th><th scope=col>Animation</th><th scope=col>Children's</th><th scope=col>Comedy</th><th scope=col>Crime</th><th scope=col>Documentary</th><th scope=col>Drama</th><th scope=col>Fantasy</th><th scope=col>Film-Noir</th><th scope=col>Horror</th><th scope=col>Musical</th><th scope=col>Mystery</th><th scope=col>Romance</th><th scope=col>Sci-Fi</th><th scope=col>Thriller</th><th scope=col>War</th><th scope=col>Western</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>Toy Story (1995)                                    </td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>GoldenEye (1995)                                    </td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>Four Rooms (1995)                                   </td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>Get Shorty (1995)                                   </td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>Copycat (1995)                                      </td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 6 × 20\n",
       "\\begin{tabular}{llllllllllllllllllll}\n",
       " Title & unknown & Action & Adventure & Animation & Children's & Comedy & Crime & Documentary & Drama & Fantasy & Film-Noir & Horror & Musical & Mystery & Romance & Sci-Fi & Thriller & War & Western\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t Toy Story (1995)                                     & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t GoldenEye (1995)                                     & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t Four Rooms (1995)                                    & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t Get Shorty (1995)                                    & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t Copycat (1995)                                       & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\t Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 6 × 20\n",
       "\n",
       "| Title &lt;chr&gt; | unknown &lt;dbl&gt; | Action &lt;dbl&gt; | Adventure &lt;dbl&gt; | Animation &lt;dbl&gt; | Children's &lt;dbl&gt; | Comedy &lt;dbl&gt; | Crime &lt;dbl&gt; | Documentary &lt;dbl&gt; | Drama &lt;dbl&gt; | Fantasy &lt;dbl&gt; | Film-Noir &lt;dbl&gt; | Horror &lt;dbl&gt; | Musical &lt;dbl&gt; | Mystery &lt;dbl&gt; | Romance &lt;dbl&gt; | Sci-Fi &lt;dbl&gt; | Thriller &lt;dbl&gt; | War &lt;dbl&gt; | Western &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Toy Story (1995)                                     | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| GoldenEye (1995)                                     | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
       "| Four Rooms (1995)                                    | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
       "| Get Shorty (1995)                                    | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| Copycat (1995)                                       | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
       "| Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  Title                                                unknown Action Adventure\n",
       "1 Toy Story (1995)                                     0       0      0        \n",
       "2 GoldenEye (1995)                                     0       1      1        \n",
       "3 Four Rooms (1995)                                    0       0      0        \n",
       "4 Get Shorty (1995)                                    0       1      0        \n",
       "5 Copycat (1995)                                       0       0      0        \n",
       "6 Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) 0       0      0        \n",
       "  Animation Children's Comedy Crime Documentary Drama Fantasy Film-Noir Horror\n",
       "1 1         1          1      0     0           0     0       0         0     \n",
       "2 0         0          0      0     0           0     0       0         0     \n",
       "3 0         0          0      0     0           0     0       0         0     \n",
       "4 0         0          1      0     0           1     0       0         0     \n",
       "5 0         0          0      1     0           1     0       0         0     \n",
       "6 0         0          0      0     0           1     0       0         0     \n",
       "  Musical Mystery Romance Sci-Fi Thriller War Western\n",
       "1 0       0       0       0      0        0   0      \n",
       "2 0       0       0       0      1        0   0      \n",
       "3 0       0       0       0      1        0   0      \n",
       "4 0       0       0       0      0        0   0      \n",
       "5 0       0       0       0      1        0   0      \n",
       "6 0       0       0       0      0        0   0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>502</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 1\n",
       "\\begin{tabular}{l}\n",
       " n\\\\\n",
       " <int>\\\\\n",
       "\\hline\n",
       "\t 502\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 1\n",
       "\n",
       "| n &lt;int&gt; |\n",
       "|---|\n",
       "| 502 |\n",
       "\n"
      ],
      "text/plain": [
       "  n  \n",
       "1 502"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 1 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>n</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>97</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 1\n",
       "\\begin{tabular}{l}\n",
       " n\\\\\n",
       " <int>\\\\\n",
       "\\hline\n",
       "\t 97\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 1\n",
       "\n",
       "| n &lt;int&gt; |\n",
       "|---|\n",
       "| 97 |\n",
       "\n"
      ],
      "text/plain": [
       "  n \n",
       "1 97"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspecting and preprocessing\n",
    "\n",
    "# check top records\n",
    "head(movies)\n",
    "\n",
    "# remove duplicates\n",
    "movies <- distinct(movies)\n",
    "\n",
    "# Mow many movies are tagged as Comedy\n",
    "filter(movies, Comedy == 1) %>% count()\n",
    "\n",
    "# How many movies are tagged as Romance and Drama?\n",
    "filter(movies, Romance == 1 & Drama == 1) %>% count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a k-means cluster\n",
    "k = 5\n",
    "iters = 1000\n",
    "set.seed(1)\n",
    "\n",
    "movies <- select(movies, -Title)\n",
    "movie_kmeans <- kmeans(movies, centers = k, iter.max=iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Clustering Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 9\n",
      " $ cluster     : int [1:1664] 1 2 2 1 3 5 5 1 5 5 ...\n",
      " $ centers     : num [1:5, 1:19] 0 0 0 0 0.00291 ...\n",
      "  ..- attr(*, \"dimnames\")=List of 2\n",
      "  .. ..$ : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n",
      "  .. ..$ : chr [1:19] \"unknown\" \"Action\" \"Adventure\" \"Animation\" ...\n",
      " $ totss       : num 2246\n",
      " $ withinss    : num [1:5] 216 441 124 102 491\n",
      " $ tot.withinss: num 1374\n",
      " $ betweenss   : num 872\n",
      " $ size        : int [1:5] 395 313 121 147 688\n",
      " $ iter        : int 2\n",
      " $ ifault      : int 0\n",
      " - attr(*, \"class\")= chr \"kmeans\"\n"
     ]
    }
   ],
   "source": [
    "# view clustering output\n",
    "str(movie_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>1</li><li>2</li><li>2</li><li>1</li><li>3</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>3</li><li>3</li><li>1</li><li>5</li><li>5</li><li>4</li><li>2</li><li>5</li><li>5</li><li>5</li><li>2</li><li>5</li><li>3</li><li>2</li><li>1</li><li>1</li><li>2</li><li>2</li><li>1</li><li>5</li><li>3</li><li>5</li><li>2</li><li>1</li><li>5</li><li>5</li><li>5</li><li>2</li><li>2</li><li>1</li><li>1</li><li>1</li><li>3</li><li>3</li><li>1</li><li>5</li><li>1</li><li>5</li><li>4</li><li>2</li><li>5</li><li>5</li><li>2</li><li>2</li><li>3</li><li>3</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>2</li><li>1</li><li>5</li><li>1</li><li>4</li><li>1</li><li>2</li><li>4</li><li>4</li><li>5</li><li>1</li><li>1</li><li>1</li><li>5</li><li>3</li><li>3</li><li>5</li><li>2</li><li>1</li><li>4</li><li>2</li><li>4</li><li>2</li><li>1</li><li>5</li><li>5</li><li>4</li><li>2</li><li>4</li><li>1</li><li>4</li><li>1</li><li>1</li><li>1</li><li>2</li><li>5</li><li>3</li><li>5</li><li>3</li><li>2</li><li>5</li><li>5</li><li>1</li><li>1</li><li>3</li><li>5</li><li>1</li><li>1</li><li>1</li><li>4</li><li>2</li><li>5</li><li>5</li><li>5</li><li>1</li><li>2</li><li>2</li><li>5</li><li>1</li><li>2</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>3</li><li>2</li><li>3</li><li>3</li><li>5</li><li>5</li><li>5</li><li>5</li><li>3</li><li>5</li><li>5</li><li>1</li><li>1</li><li>2</li><li>2</li><li>2</li><li>5</li><li>2</li><li>2</li><li>5</li><li>2</li><li>2</li><li>5</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>4</li><li>3</li><li>5</li><li>1</li><li>2</li><li>5</li><li>4</li><li>5</li><li>1</li><li>2</li><li>5</li><li>5</li><li>1</li><li>1</li><li>1</li><li>4</li><li>1</li><li>2</li><li>4</li><li>2</li><li>2</li><li>2</li><li>2</li><li>5</li><li>2</li><li>5</li><li>2</li><li>3</li><li>2</li><li>2</li><li>4</li><li>1</li><li>3</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>5</li><li>1</li><li>2</li><li>5</li><li>5</li><li>2</li><li>5</li><li>5</li><li>⋯</li><li>1</li><li>5</li><li>2</li><li>3</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>4</li><li>5</li><li>1</li><li>1</li><li>2</li><li>1</li><li>4</li><li>1</li><li>5</li><li>1</li><li>5</li><li>1</li><li>1</li><li>1</li><li>5</li><li>4</li><li>2</li><li>1</li><li>3</li><li>5</li><li>1</li><li>3</li><li>1</li><li>3</li><li>1</li><li>5</li><li>1</li><li>5</li><li>4</li><li>5</li><li>5</li><li>5</li><li>3</li><li>5</li><li>4</li><li>2</li><li>2</li><li>5</li><li>4</li><li>3</li><li>1</li><li>5</li><li>5</li><li>5</li><li>2</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>2</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>2</li><li>2</li><li>1</li><li>5</li><li>2</li><li>2</li><li>5</li><li>2</li><li>2</li><li>1</li><li>5</li><li>2</li><li>3</li><li>5</li><li>5</li><li>4</li><li>5</li><li>5</li><li>5</li><li>1</li><li>4</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>2</li><li>4</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>3</li><li>5</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>2</li><li>2</li><li>5</li><li>3</li><li>5</li><li>1</li><li>2</li><li>5</li><li>5</li><li>3</li><li>1</li><li>5</li><li>1</li><li>4</li><li>1</li><li>4</li><li>5</li><li>1</li><li>2</li><li>5</li><li>2</li><li>5</li><li>1</li><li>2</li><li>1</li><li>5</li><li>2</li><li>1</li><li>1</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>5</li><li>3</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>3</li><li>5</li><li>1</li><li>3</li><li>4</li><li>5</li><li>4</li><li>4</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>5</li><li>1</li><li>5</li><li>5</li><li>5</li><li>1</li><li>3</li><li>1</li><li>5</li><li>5</li><li>2</li><li>5</li><li>5</li><li>5</li><li>5</li><li>5</li><li>4</li><li>1</li><li>5</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item ⋯\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 1\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 1\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 2\n",
       "3. 2\n",
       "4. 1\n",
       "5. 3\n",
       "6. 5\n",
       "7. 5\n",
       "8. 1\n",
       "9. 5\n",
       "10. 5\n",
       "11. 3\n",
       "12. 3\n",
       "13. 1\n",
       "14. 5\n",
       "15. 5\n",
       "16. 4\n",
       "17. 2\n",
       "18. 5\n",
       "19. 5\n",
       "20. 5\n",
       "21. 2\n",
       "22. 5\n",
       "23. 3\n",
       "24. 2\n",
       "25. 1\n",
       "26. 1\n",
       "27. 2\n",
       "28. 2\n",
       "29. 1\n",
       "30. 5\n",
       "31. 3\n",
       "32. 5\n",
       "33. 2\n",
       "34. 1\n",
       "35. 5\n",
       "36. 5\n",
       "37. 5\n",
       "38. 2\n",
       "39. 2\n",
       "40. 1\n",
       "41. 1\n",
       "42. 1\n",
       "43. 3\n",
       "44. 3\n",
       "45. 1\n",
       "46. 5\n",
       "47. 1\n",
       "48. 5\n",
       "49. 4\n",
       "50. 2\n",
       "51. 5\n",
       "52. 5\n",
       "53. 2\n",
       "54. 2\n",
       "55. 3\n",
       "56. 3\n",
       "57. 5\n",
       "58. 5\n",
       "59. 5\n",
       "60. 5\n",
       "61. 5\n",
       "62. 2\n",
       "63. 1\n",
       "64. 5\n",
       "65. 1\n",
       "66. 4\n",
       "67. 1\n",
       "68. 2\n",
       "69. 4\n",
       "70. 4\n",
       "71. 5\n",
       "72. 1\n",
       "73. 1\n",
       "74. 1\n",
       "75. 5\n",
       "76. 3\n",
       "77. 3\n",
       "78. 5\n",
       "79. 2\n",
       "80. 1\n",
       "81. 4\n",
       "82. 2\n",
       "83. 4\n",
       "84. 2\n",
       "85. 1\n",
       "86. 5\n",
       "87. 5\n",
       "88. 4\n",
       "89. 2\n",
       "90. 4\n",
       "91. 1\n",
       "92. 4\n",
       "93. 1\n",
       "94. 1\n",
       "95. 1\n",
       "96. 2\n",
       "97. 5\n",
       "98. 3\n",
       "99. 5\n",
       "100. 3\n",
       "101. 2\n",
       "102. 5\n",
       "103. 5\n",
       "104. 1\n",
       "105. 1\n",
       "106. 3\n",
       "107. 5\n",
       "108. 1\n",
       "109. 1\n",
       "110. 1\n",
       "111. 4\n",
       "112. 2\n",
       "113. 5\n",
       "114. 5\n",
       "115. 5\n",
       "116. 1\n",
       "117. 2\n",
       "118. 2\n",
       "119. 5\n",
       "120. 1\n",
       "121. 2\n",
       "122. 1\n",
       "123. 1\n",
       "124. 5\n",
       "125. 5\n",
       "126. 5\n",
       "127. 3\n",
       "128. 2\n",
       "129. 3\n",
       "130. 3\n",
       "131. 5\n",
       "132. 5\n",
       "133. 5\n",
       "134. 5\n",
       "135. 3\n",
       "136. 5\n",
       "137. 5\n",
       "138. 1\n",
       "139. 1\n",
       "140. 2\n",
       "141. 2\n",
       "142. 2\n",
       "143. 5\n",
       "144. 2\n",
       "145. 2\n",
       "146. 5\n",
       "147. 2\n",
       "148. 2\n",
       "149. 5\n",
       "150. 1\n",
       "151. 1\n",
       "152. 1\n",
       "153. 1\n",
       "154. 1\n",
       "155. 4\n",
       "156. 3\n",
       "157. 5\n",
       "158. 1\n",
       "159. 2\n",
       "160. 5\n",
       "161. 4\n",
       "162. 5\n",
       "163. 1\n",
       "164. 2\n",
       "165. 5\n",
       "166. 5\n",
       "167. 1\n",
       "168. 1\n",
       "169. 1\n",
       "170. 4\n",
       "171. 1\n",
       "172. 2\n",
       "173. 4\n",
       "174. 2\n",
       "175. 2\n",
       "176. 2\n",
       "177. 2\n",
       "178. 5\n",
       "179. 2\n",
       "180. 5\n",
       "181. 2\n",
       "182. 3\n",
       "183. 2\n",
       "184. 2\n",
       "185. 4\n",
       "186. 1\n",
       "187. 3\n",
       "188. 5\n",
       "189. 1\n",
       "190. 5\n",
       "191. 5\n",
       "192. 5\n",
       "193. 5\n",
       "194. 1\n",
       "195. 2\n",
       "196. 5\n",
       "197. 5\n",
       "198. 2\n",
       "199. 5\n",
       "200. 5\n",
       "201. ⋯\n",
       "202. 1\n",
       "203. 5\n",
       "204. 2\n",
       "205. 3\n",
       "206. 1\n",
       "207. 1\n",
       "208. 5\n",
       "209. 5\n",
       "210. 5\n",
       "211. 1\n",
       "212. 5\n",
       "213. 5\n",
       "214. 4\n",
       "215. 5\n",
       "216. 1\n",
       "217. 1\n",
       "218. 2\n",
       "219. 1\n",
       "220. 4\n",
       "221. 1\n",
       "222. 5\n",
       "223. 1\n",
       "224. 5\n",
       "225. 1\n",
       "226. 1\n",
       "227. 1\n",
       "228. 5\n",
       "229. 4\n",
       "230. 2\n",
       "231. 1\n",
       "232. 3\n",
       "233. 5\n",
       "234. 1\n",
       "235. 3\n",
       "236. 1\n",
       "237. 3\n",
       "238. 1\n",
       "239. 5\n",
       "240. 1\n",
       "241. 5\n",
       "242. 4\n",
       "243. 5\n",
       "244. 5\n",
       "245. 5\n",
       "246. 3\n",
       "247. 5\n",
       "248. 4\n",
       "249. 2\n",
       "250. 2\n",
       "251. 5\n",
       "252. 4\n",
       "253. 3\n",
       "254. 1\n",
       "255. 5\n",
       "256. 5\n",
       "257. 5\n",
       "258. 2\n",
       "259. 5\n",
       "260. 5\n",
       "261. 1\n",
       "262. 5\n",
       "263. 5\n",
       "264. 1\n",
       "265. 5\n",
       "266. 5\n",
       "267. 2\n",
       "268. 5\n",
       "269. 5\n",
       "270. 5\n",
       "271. 1\n",
       "272. 5\n",
       "273. 5\n",
       "274. 5\n",
       "275. 2\n",
       "276. 2\n",
       "277. 1\n",
       "278. 5\n",
       "279. 2\n",
       "280. 2\n",
       "281. 5\n",
       "282. 2\n",
       "283. 2\n",
       "284. 1\n",
       "285. 5\n",
       "286. 2\n",
       "287. 3\n",
       "288. 5\n",
       "289. 5\n",
       "290. 4\n",
       "291. 5\n",
       "292. 5\n",
       "293. 5\n",
       "294. 1\n",
       "295. 4\n",
       "296. 1\n",
       "297. 1\n",
       "298. 5\n",
       "299. 5\n",
       "300. 5\n",
       "301. 5\n",
       "302. 5\n",
       "303. 5\n",
       "304. 5\n",
       "305. 5\n",
       "306. 2\n",
       "307. 4\n",
       "308. 5\n",
       "309. 5\n",
       "310. 5\n",
       "311. 5\n",
       "312. 5\n",
       "313. 3\n",
       "314. 5\n",
       "315. 1\n",
       "316. 1\n",
       "317. 5\n",
       "318. 5\n",
       "319. 5\n",
       "320. 1\n",
       "321. 5\n",
       "322. 2\n",
       "323. 2\n",
       "324. 5\n",
       "325. 3\n",
       "326. 5\n",
       "327. 1\n",
       "328. 2\n",
       "329. 5\n",
       "330. 5\n",
       "331. 3\n",
       "332. 1\n",
       "333. 5\n",
       "334. 1\n",
       "335. 4\n",
       "336. 1\n",
       "337. 4\n",
       "338. 5\n",
       "339. 1\n",
       "340. 2\n",
       "341. 5\n",
       "342. 2\n",
       "343. 5\n",
       "344. 1\n",
       "345. 2\n",
       "346. 1\n",
       "347. 5\n",
       "348. 2\n",
       "349. 1\n",
       "350. 1\n",
       "351. 5\n",
       "352. 5\n",
       "353. 5\n",
       "354. 1\n",
       "355. 5\n",
       "356. 1\n",
       "357. 5\n",
       "358. 5\n",
       "359. 5\n",
       "360. 5\n",
       "361. 3\n",
       "362. 5\n",
       "363. 5\n",
       "364. 5\n",
       "365. 5\n",
       "366. 5\n",
       "367. 1\n",
       "368. 5\n",
       "369. 5\n",
       "370. 3\n",
       "371. 5\n",
       "372. 1\n",
       "373. 3\n",
       "374. 4\n",
       "375. 5\n",
       "376. 4\n",
       "377. 4\n",
       "378. 5\n",
       "379. 1\n",
       "380. 5\n",
       "381. 5\n",
       "382. 5\n",
       "383. 5\n",
       "384. 1\n",
       "385. 5\n",
       "386. 5\n",
       "387. 5\n",
       "388. 1\n",
       "389. 3\n",
       "390. 1\n",
       "391. 5\n",
       "392. 5\n",
       "393. 2\n",
       "394. 5\n",
       "395. 5\n",
       "396. 5\n",
       "397. 5\n",
       "398. 5\n",
       "399. 4\n",
       "400. 1\n",
       "401. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   [1] 1 2 2 1 3 5 5 1 5 5 3 3 1 5 5 4 2 5 5 5 2 5 3 2 1 1 2 2 1 5 3 5 2 1 5 5 5\n",
       "  [38] 2 2 1 1 1 3 3 1 5 1 5 4 2 5 5 2 2 3 3 5 5 5 5 5 2 1 5 1 4 1 2 4 4 5 1 1 1\n",
       "  [75] 5 3 3 5 2 1 4 2 4 2 1 5 5 4 2 4 1 4 1 1 1 2 5 3 5 3 2 5 5 1 1 3 5 1 1 1 4\n",
       " [112] 2 5 5 5 1 2 2 5 1 2 1 1 5 5 5 3 2 3 3 5 5 5 5 3 5 5 1 1 2 2 2 5 2 2 5 2 2\n",
       " [149] 5 1 1 1 1 1 4 3 5 1 2 5 4 5 1 2 5 5 1 1 1 4 1 2 4 2 2 2 2 5 2 5 2 3 2 2 4\n",
       " [186] 1 3 5 1 5 5 5 5 1 2 5 5 2 5 5 2 4 5 1 5 2 5 1 1 2 1 5 5 5 5 4 4 2 5 4 5 2\n",
       " [223] 3 5 1 2 2 2 2 2 1 1 2 2 1 1 5 1 3 1 4 1 1 2 2 5 2 1 1 2 1 2 5 2 4 4 2 5 1\n",
       " [260] 2 1 5 2 2 2 2 5 1 3 2 5 2 4 5 5 5 5 5 5 2 5 5 4 5 5 5 2 5 1 2 5 3 1 2 5 5\n",
       " [297] 2 3 2 1 3 2 5 5 3 5 3 5 3 3 5 2 3 1 5 5 4 5 1 2 2 5 3 5 3 2 3 5 2 3 2 2 1\n",
       " [334] 3 1 1 5 5 1 1 2 5 1 3 1 2 2 5 1 2 4 2 3 5 2 2 5 3 1 2 1 5 5 1 1 1 3 5 1 2\n",
       " [371] 2 5 1 1 5 5 2 4 1 1 1 4 1 5 1 2 1 1 5 1 4 1 1 2 2 1 1 1 4 3 5 2 2 1 1 1 1\n",
       " [408] 4 1 5 1 1 5 5 5 1 5 5 1 5 5 1 2 5 1 5 1 2 5 1 2 1 5 5 5 5 5 5 5 5 2 5 5 5\n",
       " [445] 5 2 2 4 2 2 5 2 1 5 5 5 5 5 5 2 5 4 2 5 5 5 5 5 2 5 2 5 1 1 4 2 1 1 1 5 5\n",
       " [482] 4 4 4 5 4 4 2 5 5 1 1 5 1 2 5 5 1 5 3 2 5 5 5 5 5 2 4 2 4 5 1 4 5 2 2 5 1\n",
       " [519] 1 1 5 2 5 5 5 2 5 4 5 5 4 5 5 5 1 2 2 4 5 3 4 2 3 5 5 2 5 2 5 2 5 5 5 3 5\n",
       " [556] 4 5 2 5 1 2 2 5 2 5 5 1 2 2 4 1 2 1 2 1 4 3 5 3 5 1 2 5 5 5 2 3 2 5 5 2 5\n",
       " [593] 2 1 2 5 2 4 2 1 5 5 4 4 1 5 3 5 1 5 2 2 5 5 3 5 5 2 1 5 5 5 5 3 1 1 5 5 5\n",
       " [630] 5 5 2 1 5 5 5 5 3 5 5 5 5 5 4 3 5 5 1 3 2 1 3 2 5 1 5 5 5 1 5 2 5 5 2 5 5\n",
       " [667] 5 2 5 5 5 5 3 2 5 2 1 2 2 5 1 1 2 5 2 4 5 4 1 3 5 5 5 1 5 4 5 5 4 5 5 5 4\n",
       " [704] 1 5 1 5 5 1 5 3 1 1 5 1 1 1 5 1 5 5 1 5 5 4 4 5 1 5 5 1 4 4 5 3 3 2 5 1 1\n",
       " [741] 4 2 4 5 2 2 5 3 2 1 5 2 2 2 2 5 1 4 4 5 5 2 2 3 2 5 2 5 4 5 5 4 2 1 4 5 4\n",
       " [778] 5 4 4 1 2 1 1 5 1 1 5 1 4 2 2 5 2 1 2 5 1 1 3 5 5 5 2 5 2 5 5 5 5 5 1 1 1\n",
       " [815] 4 5 3 1 2 2 2 2 2 2 2 5 2 2 4 4 5 2 2 5 2 1 1 3 1 5 5 2 4 5 5 3 1 1 2 1 5\n",
       " [852] 5 1 2 5 1 5 1 4 5 5 4 4 1 4 4 5 5 1 4 1 2 5 5 5 5 5 4 5 5 4 2 5 1 1 1 2 5\n",
       " [889] 2 5 5 5 1 3 5 1 5 5 1 1 5 5 3 2 1 3 5 2 2 4 2 5 5 5 5 5 2 1 5 5 1 2 2 1 5\n",
       " [926] 5 5 4 5 4 3 1 1 5 2 1 4 5 4 4 5 5 2 1 1 5 5 5 1 5 1 5 5 5 3 1 1 4 5 5 5 1\n",
       " [963] 1 5 5 3 2 2 2 2 3 5 5 2 2 2 5 2 2 4 5 5 3 1 1 5 1 1 1 1 1 1 1 1 5 5 5 1 5\n",
       "[1000] 5 5 3 1 2 1 5 2 5 5 2 2 5 5 1 4 2 1 2 4 1 1 2 1 2 2 1 1 4 2 5 4 4 2 1 1 5\n",
       "[1037] 2 1 4 1 5 1 1 5 1 5 5 1 5 1 2 1 5 5 3 5 5 1 5 3 1 1 4 2 1 5 2 2 5 1 1 3 5\n",
       "[1074] 3 5 5 5 2 2 2 2 2 1 1 1 1 4 5 1 5 4 5 4 1 3 2 3 5 5 3 1 5 5 5 1 4 2 5 1 5\n",
       "[1111] 1 5 3 5 4 2 5 5 5 5 5 2 5 2 5 5 5 5 3 2 1 5 5 3 5 5 5 5 5 5 5 3 4 5 2 4 3\n",
       "[1148] 5 5 2 5 5 1 5 5 1 1 1 5 5 1 5 1 5 3 4 5 1 1 1 1 1 1 1 5 1 1 3 1 5 4 3 5 5\n",
       "[1185] 3 5 5 1 3 1 5 5 1 4 1 5 1 2 3 1 2 5 5 3 5 2 4 2 1 4 5 5 2 5 5 1 3 5 2 3 1\n",
       "[1222] 5 5 5 1 3 5 1 5 2 2 1 1 5 2 3 1 5 2 1 2 1 5 2 1 5 5 4 4 5 5 4 5 5 5 5 5 5\n",
       "[1259] 4 1 1 1 3 2 2 5 2 5 2 3 5 5 1 5 5 4 1 1 4 5 1 5 2 5 1 1 5 1 5 5 1 1 2 2 1\n",
       "[1296] 5 5 1 5 5 1 1 2 2 5 5 5 5 5 5 1 1 5 3 5 5 5 5 5 5 5 5 1 5 5 1 1 5 1 4 5 5\n",
       "[1333] 5 5 5 5 5 5 1 5 1 5 4 5 5 1 4 2 5 1 1 2 5 2 5 5 5 5 5 3 1 2 5 1 5 1 4 5 4\n",
       "[1370] 5 5 5 2 5 4 2 4 5 5 3 5 5 2 4 5 5 5 5 2 5 5 5 1 1 5 5 2 1 5 5 2 5 2 2 2 2\n",
       "[1407] 5 5 2 1 5 2 5 4 1 1 5 1 5 5 2 5 5 4 1 5 1 5 3 5 5 5 5 1 5 1 5 5 5 4 2 1 3\n",
       "[1444] 5 5 1 1 4 5 1 1 3 1 5 1 5 5 5 2 5 2 1 5 4 1 5 2 3 1 1 5 5 5 1 5 5 4 5 1 1\n",
       "[1481] 2 1 4 1 5 1 5 1 1 1 5 4 2 1 3 5 1 3 1 3 1 5 1 5 4 5 5 5 3 5 4 2 2 5 4 3 1\n",
       "[1518] 5 5 5 2 5 5 1 5 5 1 5 5 2 5 5 5 1 5 5 5 2 2 1 5 2 2 5 2 2 1 5 2 3 5 5 4 5\n",
       "[1555] 5 5 1 4 1 1 5 5 5 5 5 5 5 5 2 4 5 5 5 5 5 3 5 1 1 5 5 5 1 5 2 2 5 3 5 1 2\n",
       "[1592] 5 5 3 1 5 1 4 1 4 5 1 2 5 2 5 1 2 1 5 2 1 1 5 5 5 1 5 1 5 5 5 5 3 5 5 5 5\n",
       "[1629] 5 1 5 5 3 5 1 3 4 5 4 4 5 1 5 5 5 5 1 5 5 5 1 3 1 5 5 2 5 5 5 5 5 4 1 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cluster vector, i.e., cluster index of each row\n",
    "movie_kmeans$cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 19 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>unknown</th><th scope=col>Action</th><th scope=col>Adventure</th><th scope=col>Animation</th><th scope=col>Children's</th><th scope=col>Comedy</th><th scope=col>Crime</th><th scope=col>Documentary</th><th scope=col>Drama</th><th scope=col>Fantasy</th><th scope=col>Film-Noir</th><th scope=col>Horror</th><th scope=col>Musical</th><th scope=col>Mystery</th><th scope=col>Romance</th><th scope=col>Sci-Fi</th><th scope=col>Thriller</th><th scope=col>War</th><th scope=col>Western</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0.000000000</td><td>0.05822785</td><td>0.027848101</td><td>0.02278481</td><td>0.09620253</td><td>1.00000000</td><td>0.02531646</td><td>0.002531646</td><td>0.18734177</td><td>0.012658228</td><td>0.000000000</td><td>0.027848101</td><td>0.035443038</td><td>0.01518987</td><td>0.00000000</td><td>0.022784810</td><td>0.01772152</td><td>0.030379747</td><td>0.017721519</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0.000000000</td><td>0.59744409</td><td>0.329073482</td><td>0.01916933</td><td>0.10543131</td><td>0.01597444</td><td>0.03833866</td><td>0.000000000</td><td>0.03833866</td><td>0.025559105</td><td>0.025559105</td><td>0.089456869</td><td>0.009584665</td><td>0.07987220</td><td>0.04153355</td><td>0.258785942</td><td>0.51757188</td><td>0.044728435</td><td>0.006389776</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.000000000</td><td>0.05785124</td><td>0.008264463</td><td>0.00000000</td><td>0.00000000</td><td>0.04132231</td><td>0.68595041</td><td>0.000000000</td><td>0.70247934</td><td>0.008264463</td><td>0.082644628</td><td>0.008264463</td><td>0.000000000</td><td>0.09090909</td><td>0.04958678</td><td>0.024793388</td><td>0.56198347</td><td>0.008264463</td><td>0.000000000</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0.000000000</td><td>0.04761905</td><td>0.054421769</td><td>0.01360544</td><td>0.02721088</td><td>0.65986395</td><td>0.01360544</td><td>0.000000000</td><td>0.07482993</td><td>0.013605442</td><td>0.006802721</td><td>0.013605442</td><td>0.081632653</td><td>0.02721088</td><td>1.00000000</td><td>0.013605442</td><td>0.07482993</td><td>0.034013605</td><td>0.000000000</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.002906977</td><td>0.03633721</td><td>0.014534884</td><td>0.03633721</td><td>0.06540698</td><td>0.00000000</td><td>0.00000000</td><td>0.071220930</td><td>0.77616279</td><td>0.008720930</td><td>0.007267442</td><td>0.069767442</td><td>0.039244186</td><td>0.02034884</td><td>0.11337209</td><td>0.007267442</td><td>0.00000000</td><td>0.056686047</td><td>0.026162791</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 19 of type dbl\n",
       "\\begin{tabular}{r|lllllllllllllllllll}\n",
       "  & unknown & Action & Adventure & Animation & Children's & Comedy & Crime & Documentary & Drama & Fantasy & Film-Noir & Horror & Musical & Mystery & Romance & Sci-Fi & Thriller & War & Western\\\\\n",
       "\\hline\n",
       "\t1 & 0.000000000 & 0.05822785 & 0.027848101 & 0.02278481 & 0.09620253 & 1.00000000 & 0.02531646 & 0.002531646 & 0.18734177 & 0.012658228 & 0.000000000 & 0.027848101 & 0.035443038 & 0.01518987 & 0.00000000 & 0.022784810 & 0.01772152 & 0.030379747 & 0.017721519\\\\\n",
       "\t2 & 0.000000000 & 0.59744409 & 0.329073482 & 0.01916933 & 0.10543131 & 0.01597444 & 0.03833866 & 0.000000000 & 0.03833866 & 0.025559105 & 0.025559105 & 0.089456869 & 0.009584665 & 0.07987220 & 0.04153355 & 0.258785942 & 0.51757188 & 0.044728435 & 0.006389776\\\\\n",
       "\t3 & 0.000000000 & 0.05785124 & 0.008264463 & 0.00000000 & 0.00000000 & 0.04132231 & 0.68595041 & 0.000000000 & 0.70247934 & 0.008264463 & 0.082644628 & 0.008264463 & 0.000000000 & 0.09090909 & 0.04958678 & 0.024793388 & 0.56198347 & 0.008264463 & 0.000000000\\\\\n",
       "\t4 & 0.000000000 & 0.04761905 & 0.054421769 & 0.01360544 & 0.02721088 & 0.65986395 & 0.01360544 & 0.000000000 & 0.07482993 & 0.013605442 & 0.006802721 & 0.013605442 & 0.081632653 & 0.02721088 & 1.00000000 & 0.013605442 & 0.07482993 & 0.034013605 & 0.000000000\\\\\n",
       "\t5 & 0.002906977 & 0.03633721 & 0.014534884 & 0.03633721 & 0.06540698 & 0.00000000 & 0.00000000 & 0.071220930 & 0.77616279 & 0.008720930 & 0.007267442 & 0.069767442 & 0.039244186 & 0.02034884 & 0.11337209 & 0.007267442 & 0.00000000 & 0.056686047 & 0.026162791\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 19 of type dbl\n",
       "\n",
       "| <!--/--> | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 0.000000000 | 0.05822785 | 0.027848101 | 0.02278481 | 0.09620253 | 1.00000000 | 0.02531646 | 0.002531646 | 0.18734177 | 0.012658228 | 0.000000000 | 0.027848101 | 0.035443038 | 0.01518987 | 0.00000000 | 0.022784810 | 0.01772152 | 0.030379747 | 0.017721519 |\n",
       "| 2 | 0.000000000 | 0.59744409 | 0.329073482 | 0.01916933 | 0.10543131 | 0.01597444 | 0.03833866 | 0.000000000 | 0.03833866 | 0.025559105 | 0.025559105 | 0.089456869 | 0.009584665 | 0.07987220 | 0.04153355 | 0.258785942 | 0.51757188 | 0.044728435 | 0.006389776 |\n",
       "| 3 | 0.000000000 | 0.05785124 | 0.008264463 | 0.00000000 | 0.00000000 | 0.04132231 | 0.68595041 | 0.000000000 | 0.70247934 | 0.008264463 | 0.082644628 | 0.008264463 | 0.000000000 | 0.09090909 | 0.04958678 | 0.024793388 | 0.56198347 | 0.008264463 | 0.000000000 |\n",
       "| 4 | 0.000000000 | 0.04761905 | 0.054421769 | 0.01360544 | 0.02721088 | 0.65986395 | 0.01360544 | 0.000000000 | 0.07482993 | 0.013605442 | 0.006802721 | 0.013605442 | 0.081632653 | 0.02721088 | 1.00000000 | 0.013605442 | 0.07482993 | 0.034013605 | 0.000000000 |\n",
       "| 5 | 0.002906977 | 0.03633721 | 0.014534884 | 0.03633721 | 0.06540698 | 0.00000000 | 0.00000000 | 0.071220930 | 0.77616279 | 0.008720930 | 0.007267442 | 0.069767442 | 0.039244186 | 0.02034884 | 0.11337209 | 0.007267442 | 0.00000000 | 0.056686047 | 0.026162791 |\n",
       "\n"
      ],
      "text/plain": [
       "  unknown     Action     Adventure   Animation  Children's Comedy    \n",
       "1 0.000000000 0.05822785 0.027848101 0.02278481 0.09620253 1.00000000\n",
       "2 0.000000000 0.59744409 0.329073482 0.01916933 0.10543131 0.01597444\n",
       "3 0.000000000 0.05785124 0.008264463 0.00000000 0.00000000 0.04132231\n",
       "4 0.000000000 0.04761905 0.054421769 0.01360544 0.02721088 0.65986395\n",
       "5 0.002906977 0.03633721 0.014534884 0.03633721 0.06540698 0.00000000\n",
       "  Crime      Documentary Drama      Fantasy     Film-Noir   Horror     \n",
       "1 0.02531646 0.002531646 0.18734177 0.012658228 0.000000000 0.027848101\n",
       "2 0.03833866 0.000000000 0.03833866 0.025559105 0.025559105 0.089456869\n",
       "3 0.68595041 0.000000000 0.70247934 0.008264463 0.082644628 0.008264463\n",
       "4 0.01360544 0.000000000 0.07482993 0.013605442 0.006802721 0.013605442\n",
       "5 0.00000000 0.071220930 0.77616279 0.008720930 0.007267442 0.069767442\n",
       "  Musical     Mystery    Romance    Sci-Fi      Thriller   War        \n",
       "1 0.035443038 0.01518987 0.00000000 0.022784810 0.01772152 0.030379747\n",
       "2 0.009584665 0.07987220 0.04153355 0.258785942 0.51757188 0.044728435\n",
       "3 0.000000000 0.09090909 0.04958678 0.024793388 0.56198347 0.008264463\n",
       "4 0.081632653 0.02721088 1.00000000 0.013605442 0.07482993 0.034013605\n",
       "5 0.039244186 0.02034884 0.11337209 0.007267442 0.00000000 0.056686047\n",
       "  Western    \n",
       "1 0.017721519\n",
       "2 0.006389776\n",
       "3 0.000000000\n",
       "4 0.000000000\n",
       "5 0.026162791"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# centroid values\n",
    "movie_kmeans$centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>395</li><li>313</li><li>121</li><li>147</li><li>688</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 395\n",
       "\\item 313\n",
       "\\item 121\n",
       "\\item 147\n",
       "\\item 688\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 395\n",
       "2. 313\n",
       "3. 121\n",
       "4. 147\n",
       "5. 688\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 395 313 121 147 688"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# size of clusters, i.e., number of movies in each cluster\n",
    "movie_kmeans$size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>215.599999999991</li><li>440.862619808313</li><li>124.297520661157</li><li>102.068027210884</li><li>490.784883720944</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 215.599999999991\n",
       "\\item 440.862619808313\n",
       "\\item 124.297520661157\n",
       "\\item 102.068027210884\n",
       "\\item 490.784883720944\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 215.599999999991\n",
       "2. 440.862619808313\n",
       "3. 124.297520661157\n",
       "4. 102.068027210884\n",
       "5. 490.784883720944\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 215.6000 440.8626 124.2975 102.0680 490.7849"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# within-cluster sum of squares\n",
    "movie_kmeans$withinss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determining number of clusters**\n",
    "\n",
    "One way to select the number of clusters is by using a **scree plot**. A standard scree plot has the number of clusters on the x-axis, and the sum of the within-cluster sum of squares on the y-axis. The within-cluster sum of squares for a cluster is the sum, across all points in the cluster, of the squared distance between each point and the centroid of the cluster. To determine the best number of clusters using this plot, we want to look for a bend, or elbow, in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAALQCAMAAAC323mdAAAANlBMVEUAAAAzMzNNTU1oaGh8\nfHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enr6+vw8PD////agy6EAAAACXBIWXMA\nABJ0AAASdAHeZh94AAAgAElEQVR4nO3di1bbuhaFYW3Tll5p8/4vuxMgkIvt2NKSlqbWP8c4\nvewWvgg3/wmUknRgjDGRJe8bwBhjW0ewGGMyI1iMMZkRLMaYzAgWY0xmBIsxJjOCxRiTGcFi\njMnMKlh/9y/nZQwHH9iH1+IJFnxoH16LJ1jwoX14LZ5gwYf24bV4ggUf2ofX4gkWfGgfXosn\nWPChfXgtnmDBh/bhtXiCBR/ah9fiCRZ8aB9eiydY8KF9eC2eYMGH9uG1eIIFH9qH1+IJFnxo\nH16LJ1jwoX14LZ5gwYf24bV4ggUf2ofX4gkWfGgfXosnWPChfXgtnmDBh/bhtXiCBR/ah9fi\nCRZ8aB9eiydY8KF9eC3ePliMMVZ9PMKCD+jDa/EECz60D6/F9xCslNL+FzKc3lUbiPf24bX4\nDoKVknOx9K7aQLy3D6/F+wcrJe9i6V21gXhvH16LJ1iKV20g3tuH1+IJluJVG4j39uG1eP9g\n/SVYkXlvH16L7yBYb39L6Fgsvas2EO/tw2vxPQTr7Xb7FUvvqg3Ee/vwWnw/wfrr9m6h3lUb\niPf24bX4joLl9iBL76oNxHv78Fp8V8FyKpbeVRuI9/bhtfi+guXzbqHeVRuI9/bhtfjOguXy\nIEvvqg3Ee/vwWnx3wXIolt5VG4j39uG1+P6C1b5YeldtIN7bh9fiOwxW8w9k6V21gXhvH16L\n7zFYrR9k6V21gXhvH16L7zNYbYuld9UG4r19eC2+02A1fbdQ76oNxHv78Fp8r8Fq+SBL76oN\nxHv78Fp8v8FqVyy9qzYQ7+3Da/EdB6tZsfSu2kC8tw+vxfccrFYfyNK7agPx3j68Ft91sBo9\nyNK7agPx3j68Ft95sJoUS++qDcR7+/BafO/BavFuod5VG4j39uG1+O6D1eBBlt5VG4j39uG1\neIFgVS+W3lUbiPf24bV4hWDVfrdQ76oNxHv78Fq8RLAqP8jSu2oD8d4+vBYvEqyqxdK7agPx\n3j68Fq8SrJrF0rtqA/HePrwWLxOsih/I0rtqA/HePrwWrxOseg+y9K7aQLy3D6/FKwWrVrH0\nrtpAvLcPr8VLBavSu4V6V20g3tuH1+K1glXnQZbeVRuI9/bhtXi1YNUolt5VG4j39uG1eLlg\nVSiW3lUbiPf24bV4vWDZfyBL76oNxHv78Fq8YLDMH2TpXbWBeG8fXouXDJZxsfSu2kC8tw+v\nxWsGy/bdQr2rNhDv7cNr8aLBMn2QpXfVBuK9fXgtXjZYhsXSu2oD8d4+vBavGyy7dwv1rtpA\nvLcPr8ULB8vsQZbeVRuI9/bhtXjpYBkVS++qDcR7+/BavHawbIqld9UG4r19eC1ePFgmH8jS\nu2oD8d4+vBavHiyLB1l6V20g3tuH1+L1g1VeLL2rNhDv7cNr8QMEq/jdQr2rNhDv7cNr8SME\nq/RBlt5VG4j39uG1+H3Bmt6+PW7ue79glRVL76oNxHv78Fr8rmC99+n9m9vvPYNVVCy9qzYQ\n7+3Da/F7gjUd+g1WyQey9K7aQLy3D6/F73qEdZGl7oJV8CBL76oNxHv78Fq8abD+O23Dq6k0\nq782YIx1vx3Bevsge2+PsLLfLdT7v5mBeG8fXovPCtahy3cJT8sqlt5VG4j39uG1+MGClVUs\nvas2EO/tw2vxGcHq828Jz8t4t1Dvqg3Ee/vwWvxwwcp4kKV31QbivX14LT4jWD1+pvvV9hZL\n76oNxHv78Fr8vmBtWZvbvbqdxdK7agPx3j68Fj9ksHZ+IEvvqg3Ee/vwWvyYwdr3IEvvqg3E\ne/vwWvyowdpTLL2rNhDv7cNr8cMGa8e7hXpXbSDe24fX4scN1vYHWXpXbSDe24fX4kcO1tZi\n6V21gXhvH16LHzpYG4uld9UG4r19eC1+7GBt+0CW3lUbiPf24bX4wYO16UGW3lUbiPf24bX4\n4YO1oVh6V20g3tuH1+LHD9bjdwv1rtpAvLcPr8UHCNbDB1l6V20g3tuH1+JDBOtUrLT8QEvv\nqg3Ee/vwWnyMYJ1ytVwsvas2EO/tw2vxoYK1UCy9qzYQ7+3Da/EES/GqDcR7+/BaPMFSvGoD\n8d4+vBYfJFh/+RhWp7y3D6/FRwnWX4LVJ+/tw2vxYYJ1Gu8S9sd7+/BaPMFSvGoD8d4+vBYf\nKlgLxdK7agPx3j68Fk+wFK/aQLy3D6/FxwrWfLH0rtpAvLcPr8UHC9ZssfSu2kC8tw+vxRMs\nxas2EO/tw2vx0YI1Vyy9qzYQ7+3Da/HhgjVTLL2rNhDv7cNr8QRL8aoNxHv78Fp8vGDdF0vv\nqg3Ee/vwWnzAYN0VS++qDcR7+/BaPMFSvGoD8d4+vBYfMVi3xdK7agPx3j68Fk+wFK/aQLy3\nD6/FhwzWTbH0rtpAvLcPr8XHDNZ1sfSu2kC8tw+vxRMsxas2EO/tw2vxQYN1VSy9qzYQ7+3D\na/FRg3VZLL2rNhDv7cNr8QRL8aoNxHv78Fp82GBdFEvvqg3Ee/vwWjzBUrxqA/HePrwWHzdY\nn8XSu2oD8d4+vBYfOFgfxdK7agPx3j68Fk+wFK/aQLy3D6/FRw7WuVh6V20g3tuH1+JDB+u9\nWHpXbSDe24fX4gmW4lUbiPf24bX42MF6K5beVRuI9/bhtfjgwXotlt5VG4j39uG1eIKleNUG\n4r19eC0+erBOxdK7agPx3j68Fk+wFK/aQLy3D6/Fhw/WsVh6V20g3tuH1+LtgyW3sAdnTHhR\nH2H9TXr/NzMQ7+3Da/EEa+a569tO7w/NSD68Fk+wjrxvsbxP78t7+/BaPMEiWKF9eC2eYJ14\n12K5nz60D6/FE6xX3rNY/qeP7MNr8QSLYIX24bV4gvXGOxarg9MH9uG1eIJFsEL78Fo8wXrn\n/YrVw+nj+vBaPME6827F6uL0YX14LZ5gEazQPrwWT7A+eK9i9XH6qD68Fk+wPnmnYnVy+qA+\nvBZPsAhWaB9eiydYF7xPsXo5fUwfXosnWAQrtA+vxROsS96lWN2cPqQPr8UTrCveo1j9nD6i\nD6/FEyyCFdqH1+IJ1jXvUKyOTh/Qh9fiCdYN375YPZ0+ng+vxRMsghXah9fiCdYt37xYXZ0+\nnA+vxROsO751sfo6fTQfXosnWAQrtA+vxROse75xsTo7fTAfXosnWAQrtA+vxROsGb5tsXo7\nfSwfXosnWHN802J1d/pQPrwWT7AIVmgfXosnWLN8y2L1d/pIPrwWT7Dm+YbF6vD0gXx4LZ5g\nEazQPrwWT7AW+HbF6vH0cXx4LZ5gLfHNitXl6cP48Fo8wSJYoX14LZ5gLfKtitXn6aP48Fo8\nwSJYoX14LZ5gLfONitXp6YP48Fo8wVrh2xSr19PH8OG1eIJFsEL78Fo8wVrjmxSr29OH8OG1\neIK1yrcoVr+nj+DDa/EEi2CF9uG1eIK1zjcoVsenD+DDa/EEi2CF9uG1eIL1gK9frJ5PP74P\nr8UTrEd89WJ1ffrhfXgtnmARrNA+vBZPsB7ytYvV9+lH9+G1eIL1mK9crM5PP7gPr8UTLIIV\n2ofX4gnWBr5usXo//dg+vBZPsLbwVYvV/emH9uG1eIJFsEL78Fr8vmBNb98eN/f9uMGqWqz+\nTz+yD6/F7wrWe5/ev7n9nmBV42uOYMEL8XuCNR3CBqtmsQROP7APr8XveoR1kaVowapYLIXT\nj+vDa/GmwfrvtA2vRnFWfyfBGLPYrmBNh2iPsOo9xJI4/bA+vBZPsDbztYqlcfpRfXgtPi9Y\nsx98J1hV+VojWPBCfFawpqtqRQlWrWKJnH5QH16LzwnWZbYiBatSsVROP6YPr8VnBGua3j+1\nPdBnur+NYI3nw2vx+4K1ZW1ut+W281WKJXP6IX14LZ5gEazQPrwWT7B28TWKpXP6EX14LZ5g\n7eMrFEvo9AP68Fo8wSJYoX14LZ5g7eTti6V0+vF8eC2eYO3lzYsldfrhfHgtnmARrNA+vBZP\nsHbz1sXSOv1oPrwWT7AIVmgfXosnWPt542KJnX4wH16LJ1gZvG2x1E4/lg+vxRMsghXah9fi\nCVYOb1osudMP5cNr8QQri7cslt7pR/LhtXiCRbBC+/BaPMHK4w2LJXj6gXx4LZ5gZfJ2xVI8\n/Tg+vBZPsAhWaB9eiydYubxZsSRPP4wPr8UTLIIV2ofX4glWNm9VLM3Tj+LDa/EEK583Kpbo\n6Qfx4bV4gkWwQvvwWjzBKuBtiqV6+jF8eC2eYJXwJsWSPf0QPrwWT7AIVmgfXosnWEW8RbF0\nTz+CD6/FE6wy3qBYwqcfwIfX4gkWwQrtw2vxBKuQLy+W8un1fXgtnmARrNA+vBZPsEr54mJJ\nn17eh9fiCVYxX1os7dOr+/BaPMEiWKF9eC2eYJXzhcUSP724D6/FEywDvqxY6qfX9uG1eIJF\nsEL78Fo8wbLgi4olf3ppH16LJ1gEK7QPr8UTLBO+pFj6p1f24bV4gmXDFxRrgNML+/BaPMEi\nWKF9eC2eYBnx+cUa4fS6PrwWT7Cs+OxiDXF6WR9eiydYBCu0D6/FEywzPrdYY5xe1YfX4gmW\nHZ9ZrEFOL+rDa/EEi2CF9uG1eIJlyOcVa5TTa/rwWjzBIlihfXgtnmBZ8uk4Rz5rBAteiCdY\n1sHaXaxhTi/pw2vxBMvyXcKUU6xRTq/pw2vx9sEKvPdged8MxgKMR1jFS1kPsUY5vaYPr8UT\nrAofw9qXrGFOL+nDa/EEy5Q/P7za8zBrnNMr+vBaPMGqxW9u1pCnl/HhtXiCVZHf1qxRT6/h\nw2vxBKsuvyFZA59ewIfX4peD9WM6HH6n6TvBKtvDh1lDn757H16LXwzWj5QOL9Px3ra3WG1u\nt+Wq8+vNGv30ffvwWvxisJ7S7+P/fvxJE8Ey2EqzApy+Yx9ei18M1vEB1q/09Po9wTLZUrJi\nnL5XH16LXwzWlF6+pT+nj2IRLKvNP8yKcvo+fXgtfjFY34/3run0AOuZYBluplmBTt+hD6/F\nLwbr8JymX8cHWnt7RbAe7bZZsU7fmw+vxS8HK3dtbrfl2vNXyQp3+q58eC2eYPnwFw+zAp6+\nIx9ei18OFp84WnnnZsU8fS8+vBa/GCw+cbTBXpsV9vRd+PBa/GKw+MTRNjv+/4In7/7G9/bh\ntfjFYPGJo62W80w7hiNY8EL8YrD4xNGGvGezCBa8EL8YLD5xtC3v1iyCBS/ELwaLTxxtzvs0\ni2DBC/HLwcpdm9ttuY54h2QRLHghnmB1xjd/mEWw4IX45WA9T3nPC9rmdluuN75tswgWvBC/\nGKzn87OCEiwHvmGzCBa8EL8YrCn92FkqgmXKt0oWwYIX4heDtfuRFcGy5ts8zCJY8EL8YrC+\npn8Ey51v0CyCBS/ELwbrZfryQrA64Gs3i2DBC/GLwUp80L0bvmqyCBa8EE+wJPiKD7MIFrwQ\nvxis7LW53ZbT4Gs1i2DBC/EES4g/N8u0XQQLXoifD9bx/UDeJeySP6Xq9ar48BWm9NaHd+cJ\nlhp/vixOvPm03vrwzvx8sErW5nZbTownWPBxeYIlxxMs+Lj8crD4ag298gQLPiy/GCy+WkO/\nfHr70LsXbzu5tz68J78YrGn3ExISrMa8TbEIFrwQvxgsvlpD/7zJgyyCBS/ELwbrma/WIMAb\nFItgwQvxi8E6fOWrNQjw5Q+yCBa8ED8frHQ5gtU1X1osggUvxBMseb7wQRbBslrO55qMc/pG\n/HywStbmdltOni8qFsEyWtZnxw1z+lY8wRqBL3mQRbBslvcPEEY5fTN+MVjndwWniWAJ8PnF\nIlg2I1hN+PlgTXwMS43PfpBFsGxGsJrw88H6cdGrvU9P2OZ2W24QPrNYBMtofAyrBT8frAOf\n6S7I5z3IIlhGy/oX6cOcvhW/GKzstbndlhuHzykWwTJa+ptxAYY5fSt+Plh8xVFRXu//4719\nu8e3H9+48HnT4wnWYLzaXcbbtw3W7jf/KKdvxs8H67hfOztFsDrh9z7IIlgmS1ffNeczp8cv\nBiulb3nNanO7LTcav+9OQ7BMlm6+b8xnTo9fDNbX03uDX39ef42Z6er7aXr7rNLz9wSrE37X\ngyyCZbF094OmfO70+MVgHQ5/fpyi9eXnRa+my+/P33z+hGD1wu+42xAsixGsRvxKsE57vvyg\n+3R+ZHUgWL3z2x9kESyDpdkfNuOzp8evBev36Ylzni6/tPvVu4QEq2t+6x2HYBmMYLXiF4P1\n61Sr1Y9h3Qfrv9Puusd8lv1vFdjepcWfsGq7+1vC9Hz3Vd15hCXFb/o/ex5hlS8t/qQJnz89\nfjFYv98eYV1/XXeCpcVv+UgWwSpeWv1pdb5gevxisM7NenomWML84zsPwSoewWrHrwXrcPj3\n/fqf5hAsOf7hgyyCVbrbtzDBqsivBOvP9y/p5vPdCZYg/+D+Q7BKd/cG1vksOD1+MVjfptta\n3QaLz3QX4dcfZBGswt2/dQlWPX4xWPxbwoH4tXsQwSrczBt3e7HkT9+aXwzWZa12fUZPm9tt\nuQD8yoMsglW2ubcswarGLwbrcgRLn1+8DxGsss2+YVX+YZQeT7Ci8EsPsghW0ebfqgSrFk+w\n4vDz9yKCVbSFNIn8S049nmAF4mcfZBGski2FiWBV4glWKH7mfkSwSkawGvMEKxZ//yCLYBVs\nuUsbiyV9eg+eYEXjb+9JBKtgBKs1T7DC8TcPsghW/taqtK1Yyqd34QlWQP7qvkSw8kewmvOb\ngrVrbW635eLxlw+yCFb21puk8OUT9XiCFZP/vDcRrOwRrPb8YrD+feOp6kfmPx5kEazcPSgS\nwarBLwbrayJYY/PJlz9P963/qEhbiqV7eid+MVgp/Txkrc3ttlxU/u1BFsHK3MMeEawK/GKw\nnnI/ptXmdlsuLp98+dfJvvUf96j/pwDR4xeD9fJ0/yxfBGs0/vggi2DlbUONCJY9vxisw08+\nhhWBTwQrb1ve4ev+OYv0+MVg8UH3IPyW5y6sOdG3/qa3GsEy5xeDxQfdw/C+xXI/ft62vdEe\n/i7R0/vxi8H6ygfdw/CuD7L8j5+zjW8xgmXNLwbr8PXbyyFnbW635eA9H2T1cPz9I1hO/GKw\nEh/DisT7Pcjq4vh7t/mt1fnzbuvxBAv+bV7F6uT4+0awvPjFYGWvze22HPzrnB5k9XL8Pdvx\nlnrwWxVP78oTLPiPuRSrn+NvH8Fy4xeDxbuEAXmPB1kdHX/rdr2V1n+z4Ol9eYIFf7n2xerq\n+NtGsPz4xWC97eXL9529IljafPMHWX0df8v2vYUIlin/IFiHf2lvsdrcbsvBX61xsXo7/uPt\nfAOt/na90zvzj4K17wkoCNYIfNsHWd0d/9H2vnUIliX/KFg/00SwwvEti9Xh8de3+42z9gJy\np/fmF4P18TH3Z4IVjz89yEptHmn1ePy17X+jECxD/lGwpr29Ilhj8G+X389vtQbBWnsRtdO7\n84vByl6b2205+JmlVsXq8/iLy3mLECw7nmDBz45gzS/rLbL8QmKn9+eXg/VjOhx+p4nPw4rJ\nE6zZ5b1BCJYZvxisHykdXqbjH1k+Dysm/9ar+s3q9PgLI1jO/GKwntLv4/9+/OHTGqLy51ZV\nblavx59d7lti8eWkTt8Dvxis4wOsX+mJTxyF/1v3U0kFjv85guXNLwZrSi/f0p/TR7EIFnzF\nh1kSx39f/ttg6SWVTt8Fvxis76dPwjo9wOITR+FfVylZKsc/jWC584vBOjyn6dfxgRafOAp/\nXpWHWTrHL/oXSwsvK3T6PvjlYOWuze22HPz22TdL6PgEy58nWPA7Z5wsneMXnZtg2fAEC373\nTB9m6Ry/7NDzL61z+k54ggWfM7tmyRy/8MAEy4QnWPCZM0qWzPFLjzv78jKn74UnWPDZM3mY\npXL84qMSLAueYMGXrDxZKscvb/Pca1A5fTc8wYIvW+nDLJHjGzyWJFgGPMGCL15Rs0SOb/EB\nu5nXIXL6fniCBW+x/GRpHN/kLxgIVjlvHywWc/ufI1xpJmcb+Q3UfjzCgi9c1sMsieMbfdLZ\n/auROH1PPMGCN1zGR7Mkjk+weuEJFrzt9jZL4fhm/xLp7hUpnL4rnmDBm29XshSOT7C64QkW\nfIXteJglcHzDf+p9+6oETt8XT7Dg62xrswSOT7D64QkWfLVtSlb/x7f8+l8Eq5AnWPAVt+Fh\nVv/HN/2ChTevrP/Td8YTLPi6e5Ss7o9v+wVWCVYZT7Dga2/9YVb3xzf+IvbXr6770/fGEyz4\nBltpVu/Ht37SDYJVxBMs+DZbSlbvxzd/ZrOrV9j76bvjCRZ8q80/zOr8+PbPxEiwSniCBd9w\nM8nq/PgVnjr28lV2fvr+eIIF33R3D7P6Pn6FXhGsEp5gwbfedbP6Pj7B6ownWPAOu0hW18ev\n0aur19r16XvkCRa8y84Ps0yfRTpjBEuLJ1jwXju1KiXnYq0ev9YtE3l82SNPsOAdl5J3sQiW\nFk+w4B3Xd7Dq3a6P1xz54hOszMG7jWBVIzZNjydY8J7z7tXa8SveLoKVyxMseNe9fuTd0fcJ\n1sfrjn3xCVbW4L19x2ItH7/qjSJYmTzBgu/A9yuWU7DOrz32xSdYWYP3992KtXj8yreIYOXx\nBAu+C9+rWF7Ben/9vV78Nn8VQrCyBh/YX+Kr31+7Dlajv7wlWFmD78F3eojlFqw3oc+L3+rT\n4whW1uC78H2KtXSXrS8TLIKVN/g+fJdiEayZESzz2205+E58j2LNH7/JLUmLfLPxMSyCBZ/v\nOxSLYM2t0T+YIlhZg+/Gb1+s2eM3uhnJ/a2/9g4xj7Asb7fl4PvxmxeLYN0vXXzrwK+/CMGC\n78lvXay54ze7Dcn7rb+aaz7obni7LQffk9+4WATrdunuB035Ry9CsOD78tsWa+b47W5A38Gq\n/4YgWFmD78tvWizXYP1N/V38NPvDZvzDFyFY8L35LYvl28v+gpUWf9KEf/wiBAu+O79hM5wf\n4Pk+KeOj98cJltntthx8d367+7Hzh9A6C9btzal88whW1uAD+94f8+/rea8JVq3bbTn4/vxm\n92Pvz6roKlj3N6buzSNYWYPv0G91R/YOlu87hQ//mQHBMrrdloPv0W90R3b/RPt+gjV7S6re\nPIKVNfgu/Tb3ZIJ13vwNIVg2t9ty8H36Te7K/v/2upenkV24HTVvHsHKGnynfvsvyRQ4WEs3\ng2CZ3G7Lwffqt/4KJz5fjquL571evhEVbx7Byhp8t37jLxgQN1grt4FgWdxuy8H36zf997de\nX6HZr1jn06/egno3j2BlDb5jv+U/ZyNYsyNYBrfbcvA9++3+dYjbk/a4B+vBDah2+whW1uC7\n9pt9srXfs4y5FWsbT7DKb7fl4Pv2G33uouPzuPoG67Fe6/ZVD9b09u1xc98TLPgqfptPBfJ8\n4mmvYm38S0rVYL336f2b2+8JFnwlv8XfrPs0QyNYtW5f5WBNB4IFP5jfR7C8irX1E+01g3Ug\nWPA+fv2/Wfcrhi+/Ea5z+7yD9d9pG14NYztn9VdCfkCf/lbW+80zMx5hwXfsV/64r+cHkRxv\nwHa1yu3zfoRFsOCr+XU/jEKwzH7njhGsrMFL+DU/jOL8mZteN2HP0yLWuH0EK2vwGn7F/5OP\nGay05+ITrKLbbTl4Eb/afcb9H/O53Ii07+JXuH18pnvW4FX8WvcZgrXtt1uPf0uYNXgZ3/5O\ns/VzvSvt8vSNb0b628Ubf++LECx4Id/8ThM2WOmW3/YSpiNYWYMX8q3vNVv/cUqluQUr3fFb\nX8ZwBCtr8Eq+8b2mo2C1vCFpht/6QnYjWFmDl/Jt7zYdPZNp98Gq8fB294sQLHgx3/Ru01Ow\n2t2UM0SwCBa8lH9w7ZVTsD4c//fHd78IwYJX8y3vNl0Fq1GxPhWCRbDg6/uG9xvfXrkE6wJx\n/wDi/hchWPB6vt39pq9gNbk5BItgwTf2re44e75eQY05BOuScP8bj/0vQrDgFX2jO068YF0J\nBItgwbfxTe45u77ASo3d8bWLdf36vf/GI+NFCBa8pm9xzwkXrJtXT7AIFnwrv/yus/MLrFTY\nPV+3WOXBsryBBCtr8KJ+8V0nXLBuXznBIljw7fzC+87urwhlvxm+YrHuXrXz55QQrKzBy/pl\n951owbp/zQSLYMG39EvuPPu/hJ39WgZr5hX7fhIcwcobvLBfcOfpNFjVikWwCBa8u59978n5\nmpvmaxisuVfr+1m7BCtv8CH9boNVp1izr5RgESz4xn7mvSfva25ar1mw5l+n6z8zIFiZg9f2\n8+49HQerQrEWXiPBIljwzf2cu0/mFwm2nmiwjG4hwcoavLqfcffpOljmxVp6fQSLYME7+Lvv\nP9lf1dx4bYK1+Oo8/yFnHk+w4Efw995/QgVr+bURLIIF7+LvuwPlPw2D8ZZ4y2KtvK6C01vc\nQoKVNfgR/F13IIK1zpe91s0jWFmDH8LfcQ8qeRoG2y3ybZ5mo+T0BreQYGUNfgx/+z0oUrBW\nXxHBIljwbv7Wu1DZ88aYbpm3epaNTL74VW8Zwcoa/Cj+xrtQoGA9eC0Ei2DBO/qb7kOFT3Rl\nOuVgld9EgpU1+HH8LfchjWAZPStQNm/w2h+OYGUNfiD/8Z2o+Jn5LFc3WA9fReHpS28iwcoa\nfChfJVgmTwpUwJsA5jzBgh/Lf3QnMnhmPsPVDNaGly89feFNJFhZgx/Kf3An0gmWxbOYlfA2\nhDFPsOBH81fvRRZPJWq4isHa8tLFpy+7iQQra/CD+Wv3IqVglT+JWRlvpVjyBAt+PH/5bmTz\n3Md2qxasbS9afvqiYhGsrMEP5y/ejaIEa+NLEiyCBd+Dv3A/uv/Pfb/1C590sZQ3lKx4ggU/\npD9/P4oSrK0vR7AIFnwf/twdaea/df7WL3oKs3LeFDPhCRb8oP7MHSlIsLa/EMEiWPC9+Hf3\npLm7Vu9v/YJnMLPgjb1ynmDBD+vf3pMI1m7e2CvnCRb8uH5a+VkDfsMqBGvPS9icPrtYBCtr\n8MP6aWc7rIsAAAn4SURBVPEnTfjHe8xnP+OiDW9OFvIECz6IP3+36v6tv7MG+3670elzi0Ww\nsgY/rp9mf9iMf7gNfOYTLlrxFdQS3j5YjHW0dPcDte264U6n9GB5hAU/op9uvm/MP9gWPu8Z\nYs34Gm4+T7DgB/fTxbcO/Ppsg7W7G2anzysWwcoa/Nh++puOc+NXt4nPeYJYQ74OnckTLPjh\n/RQjWBnRsDt9VrEIVtbgx/ZTWimWwlufYF28CMGCH9yXD1bGE1qb8pvWiCdY8KP7MYLV6iGO\npU+wsgY/uK/+Maz9z2dtzG9am/dICRb8+P5yr7yPT7D2vgjBgo/si/A7n8/amt8ygtVq8IF9\nEX7f01mb85vW5NPACBZ8aF+EX49By6/vYnkrCFbW4AP7KvyeZ7OuwG9ai38ZRLDgQ/sqPMF6\nfxGCBR/Zl+GXa9D2aWvW1uCLRRAs+NC+DL8Yg8bPZLo2gtVk8IF9HX6hBiW9Mj99/S94SrDg\nQ/s6/HwMinpFsAgWvJYvxM/WoK9g1X8ODIIFH9oX4udiUNYrgkWw4LV8IX4mBoW9qnD62k+L\nSLDgQ/tK/F0MSntFsAgWvJavxN/GoLhXNU6/50YRrKzBB/al+LT60+r8phGs6oMP7EvxaeVn\nDfht23GzCFbW4AP7Wnxa+HEjftMIVu3BB/a1+DT7w2b8tm2/ZQQra/CBfS0+zfyoIb9tBKvy\n4AP7Yny6+0FTfts23zaClTX4wL4Yn26+b8xvG8GqO/jAvhqfLr514Ldt680jWFmDD+yr8enj\nGxd+2whW1cEH9uX4lSeFbcFv28ZbSLCyBh/YV+PXnsW6Ab9xBKvm4AP7YnxKpsWqdvptt5Bg\nZQ0+sC/GEyyCBR/ZF+NVgrWtWAQra/CBfTVe42NYBKvm4AP7crxlr2qefsutJFhZgw/sw1ca\nwao2+MA+fK1tKBbByhp8YB++1ghWrcEH9uGr7XGxCFbW4AP78NVGsCoNPrAPX28Pi0WwsgYf\n2IevN4JVZ/CBffiKe1QsgpU1+MA+fMURrCqDD+zD19yDYhGsrMEH9uFrjmDVGHxgH77q1otF\nsLIGH9iHrzqCVWHwgX34ulstFsHKGnxgH77uCJb94AP78JW3ViyClTX4wD585XUQrOm4ue8J\nFrycD197K8VqE6zp/Zvb7wkWvJ4PX3sEy3rwgX346lsuFsHKGnxgH776+g3Wf6dtfTWMsRCz\n+vu8zNd6atPEIyz4IXz4+lt8iNXubwl5lxB+DB++wZaK1fDzsAgW/BA+fIP5BosPusOP48O3\n2EKxCFbW4AP78C3mGiw+0x1+HB++yeaLxb8lzBp8YB++yQiW4eAD+/BtNlssgpU1+MA+fJsR\nLLvBB/bhG22uWAQra/CBffhGI1hmgw/sw7faTLEIVtbgA/vwrUawrAYf2IdvtvtiEayswQf2\n4ZuNYBkNPrAP3253xSJYWYMP7MO3G8GyGXxgH77hbotFsLIGH9iHbziCZTL4wD58y90Ui2Bl\nDT6wD99yBMti8IF9+Ka7LhbByhp8YB++6QiWweAD+/Btd1UsgpU1+MA+fNsRrPLBB/bhG++y\nWAQra/CBffjGI1jFgw/sw7feRbEIVtbgA/vwrUewSgcf2Idvvs9iEayswQf24ZuPYBUOPrAP\n334fxSJYWYMP7MO3H8EqG3xgH95h52IRrKzBB/bhHUawigYf2If3WMrmCRZ8aB/eYwSrZPCB\nfXiXpVyeYMGH9uFdRrAKBh/Yh/dZyuQJFnxoH95nBCt/8IF9eKelPJ5gwYf24Z1GsLIHH9iH\n91oiWJmDD+zDe41g5Q4+sA/vtkSw8gYf2Id3G8HKHHxgH95vKaXHv+lmBAs+tA/vtpQyikWw\n4EP78F5LKadYBAs+tA/vNYKVO/jAPrzXCFbu4AP78G7jY1iZgw/sw/uNvyXMG3xgH16LJ1jw\noX14LZ5gwYf24bV4ggUf2ofX4gkWfGgfXosnWPChfXgtnmDBh/bhtXj7YDHGWPXxCAs+oA+v\nxRMs+NA+vBZPsOBD+/BaPMGCD+3Da/EECz60D6/FEyz40D68Fk+w4EP78Fo8wYIP7cNr8QQL\nPrQPr8UTLPjQPrwWT7DgQ/vwWjzBgg/tw2vxBAs+tA+vxRMs+NA+vBZPsOBD+/BaPMGCD+3D\na/EECz60D6/FEyz40D68Fm8frIz950d3sNinD358Tp89guW02KcPfnxOnz2C5bTYpw9+fE6f\nPYLltNinD358Tp89nuaLMSYzgsUYkxnBYozJjGAxxmRGsBhjMiNYjDGZOQRrOu3ix+1vgeOm\n29OHOv7bYS9PHektMH/6KMf/PH3Rnd8jWDc/jHLFPjfdfB9l0+f1fj96pD8A96ePcvLT3ttU\nfOcnWO033f0gxqZD5GDNnD7IyU+bDrLBmm5/HOeqvS/eH9fzIgfr8pzT7X8IsLtLLROsy/di\nP74JtJAfwXgbwbr6Uag/AG+XuvjO7/QIK+Kf1/Ou3yMMdXqCdfWDeKcvv/ZOn9YQ8c/redPK\nzwYfwbr+wd1PBt78O8QEq/tNqz8dewTr8vu5n4072WCF/fP6vpsLF+r0BOsQ9g+A7LuE0yHo\nO/Hvu/7zGuvwBOvuL/ajHP7zUqt90P3jE1ynix8H2sVdNdzp3++yQf8AfP492fl+G+705dee\nf0vIGJMZwWKMyYxgMcZkRrAYYzIjWIwxmREsxpjMCBZjTGYEizEmM4LFGJMZwWIuS/N/8n6E\n+cxvljWCxVy2EKyF/8zY2/jzwVxGsFjO+PPBSpfSy9c0PR/OuTl9e/zf1/T18PKUvv67+s2X\nv/Xztx++T+npx+nHrz/79y2lb/9ef+3P9OXjVxkjWKx0KU2n0DxfB+vr8T/9fDp+8+3y9/57\n/a1fb4P1fPqv6cc5WK+/6en1174cX/78q4wRLFa6Y1X+HX6k6TpY3w4/TxH7ef1O3vPxv/8+\nx+ryt78c//P5NXw/xe85vT7iens09v6rLPwIFivdqSdzBTp+8+/2o1JP6d/5hS5/+5S+/fr4\nz8ff9PrDr+dX/fGrLPwIFivd1UeuLn/0+c3N77377b+O7wQ+vXy+yNvOv+PjV1n4ESxWOoNg\nHQ5/ntL0eyFYH7/Kwo9gsdLdZOplLVj37xK+nH/Dj4//9JSuX/XHr7Lw408BK93lh6J+Hv59\nWQvWc3o+/Dn/hs/fPqXfx/98/qD76TcdfqYv5xf++FUWfgSLle7msxO+rwXr5fwZC9e//e0T\nF76/forE+XMf0p/zC3/8Kgs/gsVKd/GhqOfp2JW1YB3+HB9RfXu5/e2nH02nIr1+dsTh5VtK\nX35/vvD5V1n4ESzGmMwIFmNMZgSLVV9Kn5+owFjJ+CPEqo9gMavxR4gxJjOCxRiTGcFijMmM\nYDHGZEawGGMyI1iMMZkRLMaYzAgWY0xm/wO3nXu+qNGQ0QAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 360,
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(99)\n",
    "\n",
    "# Call kmeans function with centers = 3, centers = 4, etc\n",
    "num_clusters = seq(5, 15,1)\n",
    "\n",
    "# within-cluster sum of squares for all clusters\n",
    "sum_withinss = sapply(num_clusters, function(x) sum(kmeans(movies, centers=x, iter.max=2000)$withinss))\n",
    "\n",
    "# visualize\n",
    "ggplot(mapping = aes(x=num_clusters, y=sum_withinss)) +\n",
    "    geom_line() +\n",
    "    geom_point()\n",
    "    \n",
    "# 12 seems like a good pick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Top 10 algorithms in data mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)\n",
    "\n",
    "[R for Statistical Learning](https://daviddalpiaz.github.io/r4sl/)\n",
    "\n",
    "[The caret Package](http://topepo.github.io/caret/)\n",
    "\n",
    "[Linear Regression: r-statistics.co](http://r-statistics.co/Linear-Regression.html)\n",
    "\n",
    "[Tutorial: SVM in R](http://math.stanford.edu/~yuany/course/2015.fall/SVM_in_R.pdf)\n",
    "\n",
    "[Artificial Neural Networks in R](https://rpubs.com/julianhatwell/annr)\n",
    "\n",
    "[Cluster analysis in R: determine the optimal number of clusters](https://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
